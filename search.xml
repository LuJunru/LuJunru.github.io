<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Interview Preparation from Nuo Chen]]></title>
    <url>%2F2019%2F04%2F11%2FInterview-Preparation%2F</url>
    <content type="text"><![CDATA[[TOC] 1.L1 、L2损失函数L2损失：平方损失函数（回归损失函数）： L(Y,f(X))=(Y−f(X))^2， 计算预测值与真实值之间距离的平方和, 如果误差＞1，误差就会被放大很多 L1损失：绝对损失函数： L(Y,f(X))=|Y−f(X)| 鲁棒性更强 常用损失函数1.平方损失函数 2.绝对值损失函数 3.0-1损失函数 4.对数损失函数（逻辑回归） L(y,p(y|x))=−logp(y|x) 在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。 2.L1、L2正则（机器学习中，如果参数过多，模型过于复杂，容易造成过拟合（overfit）。(即模型在训练样本数据上表现的很好，但在实际测试样本上表现的较差，不具备良好的泛化能力)） 概括： L1和L2是正则化项，又叫做惩罚项，是为了限制模型的参数，防止模型过拟合而加在损失函数后面的一项。 区别： L1是模型各个参数的绝对值之和。 L2是模型各个参数的平方和的开方值。 L1会趋向于产生少量的特征，而其他的特征都是0. 因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵 L2会选择更多的特征，这些特征都会接近于0。 最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0 L1范式和L2范式为什么可以防止过拟合： L1范数符合拉普拉斯分布，是不完全可微的。表现在图像上会有很多角出现。这些角和目标函数的接触机会远大于其他部分。就会造成最优值出现在坐标轴上，因此就会导致某一维的权重为0 ，产生稀疏权重矩阵，进而防止过拟合。 L2范数符合高斯分布，是完全可微的。和L1相比，图像上的棱角被圆滑了很多。一般最优值不会在坐标轴上出现。L2正则化使值最小时对应的参数变小。 L1、L2具体过程：首先针对L1范数，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化当时取1，当时取-1. 从而导致的参数减去了学习率与(13)式的乘积，因此当大于0的时候，会减去一个正数，导致减小，而当小于0的时候，会减去一个负数，导致又变大，因此这个正则项会导致参数取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为 Lasso regularization。 然后针对L2范数，同样对它求导，得到梯度变化为(一般会用来把这个系数2给消掉)。同样的更新之后使得的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。 需要注意的是，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而(12)式中的也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，越大，对参数值惩罚越大，泛化能力越好。 3.逻辑回归逻辑回归原理1.找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为h函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。 所以利用了Logistic函数（或称为Sigmoid函数），函数形式为： 对应的函数图像是一个取值在0和1之间的S型曲线（图1）。 2.构造一个Cost函数（损失函数），该函数表示预测的输出（h）与训练数据类别（y）之间的偏差，可以是二者之间的差（h-y）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为J(θ)函数，表示所有训练数据预测值与实际类别的偏差。逻辑回归的损失函数是 对数损失函数 函数来衡量h函数预测的好坏是合理的。 3.显然，J(θ)函数的值越小表示预测函数越准确（即h函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。 逻辑回归和SVM区别相同点: 都是分类算法 都是监督学习算法 都是判别模型 都能通过核函数方法针对非线性情况分类 目标都是找一个分类超平面 都能减少离群点的影响 不同点: 损失函数不同，逻辑回归是对数损失函数，svm是hinge loss，自带L2正则项 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。 逻辑回归对概率建模，svm对分类超平面建模 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响 逻辑回归是统计方法，svm是几何方法 逻辑回归与最大熵最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。此外，最大熵与逻辑回归都称为对数线性模型(log linear model)。 逻辑回归与线性回归sigmoid在逻辑回归中起到了两个作用，一是将线性函数的结果映射到了(0,1)，一是减少了离群点的影响。 它们要解决的问题不一样，前者解决的是regression问题，后者解决的是classification问题，前者的输出是连续值，后者的输出是离散值，而且前者的损失函数是输出y的正态分布，后者损失函数是输出的伯努利分布。 4.GDBT和RF区别 GDBT和Xgboost区别bagging—-RF 旨在降低方差 stacking boosting—GDBT Adaboost xgboost 旨在降低偏差 随机森林RF和GBDT的区别1）随机森林采用的bagging思想，而GBDT采用的boostingboosting思想。2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。4）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。5）随机森林对异常值不敏感；GBDT对异常值非常敏感。6）随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。 7）随机森林是通过减少模型方差提高性能；GBDTGBDT是通过减少模型偏差提高性能。 Xgboost和GBDT的区别Xgboost优点(与GBDT对比) 1.xgboost对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。 3.xgboost在代价函数里增加了正则项，用于控制模型复杂度。正则项里包含了叶子的结点个数，每个叶子结点输出的score的L2的模的平方和。正则项降低了模型的方差，使模型更简单，防止过拟合 4.支持并行化，直接的效果是训练速度快，boosting技术中下一棵树依赖上述树的训练和预测，所以树与树之间应该是只能串行！但是，同层级节点可并行。具体地，对于某个节点，节点内选择最佳分类点，进行枚举的时候可以并行 hard voting classifier 123456789from sklearn.ensemble import VotingClassifiervoting_clf = VotingClassifier(estimators=[ ('log_clf',LogisticRegression()), ('svm_clf',SVC()), ('dt_clf',DecisionTreeClassifier())],voting='hard')voting_clf.fit(x_train,y_train)voting_clf.score(x_test,y_test) soft voting classifer 不仅要看投票数，还要看权值（需要每一个模型都能估计概率 predict_proba） 逻辑回归本身就是基于概率模型的—-可以预测概率 KNN—(例如本题：分给红色的点 2/3)—可以预测概率 决策树—可以预测概率（计算概率同knn） svc—（把probability:boolean,optional(default=false)）,默认是false,把false改成true就可以计算每一个数据样本分给某个类相应的概率 12345678910#soft voting classiferfrom sklearn.ensemble import VotingClassifiervoting_clf = VotingClassifier(estimators=[ ('log_clf',LogisticRegression()), ('svm_clf',SVC(probability=True)), ('dt_clf',DecisionTreeClassifier())],voting='soft')voting_clf2.fit(x_train,y_train)voting_clf2.score(x_test,y_test) 如何创建差异性—-每个子模型只看样本数据的一部分 取样-放回取样、不放回取样 放回取样-bagging–更常用（又名bootstrap） 123456789101112#使用baggingfrom sklearn.tree import DecisionTreeClassifier()from sklearn.ensemble import BaggingClassifier()/***DecisionTreeClassifier()决策树模型**n_estimators 集成500个这样的模型**max_samples每一个子模型相应的看几个样本数据**bootstrap=True 放回取样*/bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True)bagging_clf.fit(x_train,y_train)bagging_clf.score(x_test,y_test) OOB 12345678910111213#使用OObfrom sklearn.tree import DecisionTreeClassifier()from sklearn.ensemble import BaggingClassifier()/***DecisionTreeClassifier()决策树模型**n_estimators 集成500个这样的模型**max_samples每一个子模型相应的看几个样本数据**bootstrap=True 放回取样**oob_score=True 不使用测试数据集，而使用这部分没有取到的样本做测试/验证*/bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,oob_score=True)bagging_clf.fit(x,y)//所有数据传入进去bagging_clf.oob_score_ 1234567891011121314#使用n_jobsfrom sklearn.tree import DecisionTreeClassifier()from sklearn.ensemble import BaggingClassifier()/***DecisionTreeClassifier()决策树模型**n_estimators 集成500个这样的模型**max_samples每一个子模型相应的看几个样本数据**bootstrap=True 放回取样**oob_score=True 不使用测试数据集，而使用这部分没有取到的样本做测试/验证**n_jobs=-1 并行处理*/bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,oob_score=True,n_jobs=-1)bagging_clf.fit(x,y)//所有数据传入进去bagging_clf.oob_score_ 12345678910111213141516#bootstrap_featruesfrom sklearn.tree import DecisionTreeClassifier()from sklearn.ensemble import BaggingClassifier()/***DecisionTreeClassifier()决策树模型**n_estimators 集成500个这样的模型**max_samples每一个子模型相应的看几个样本数据**bootstrap=True 放回取样**oob_score=True 不使用测试数据集，而使用这部分没有取到的样本做测试/验证**n_jobs=-1 并行处理**max_features==5每一个子模型相应地取几个特征**bootstrap_features==True 有放回地取特征*/Random Patches = BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,oob_score=True,n_jobs=-1,max_features==5,bootstrap_features=True)Random Patches.fit(x,y)//所有数据传入进去Random Patches.oob_score_ 随机森林RF 12345678from sklearn.ensemble import RandomForestClassifier/***random_state随机的种子**max_leaf_nodes通过限制最大叶子节点数,可以防止过拟合,默认是"None”*/rf_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,random_state=666,oob_score=True,n_jobs=-1)rf_clf.fit(x,y)rf_clf.oob_score_ 基本思想就是构造多棵相互独立的CART决策树，形成一个森林，利用这些决策树共同决策输出类别。随机森林算法秉承了bagging方法的思想，以构建单一决策树为基础，同时也是单一决策树算法的延伸和改进。 在整个随机森林算法的过程中，有两个随机过程： 1.输入数据是随机的：从全体训练数据中选取一部分来构建一棵决策树，并且是有放回的选取2.每棵决策树的构建所需的特征是从全体特征中随机选取的 随机森林算法具体流程：1.从原始训练数据中随机选取n个数据作为训练数据输入（通常n远小于全体训练数据N，这样就存在一部分“袋外数据”始终不被取到，它们可以直接用于测试误差（而无需单独的测试集或验证集）。2.选取了要输入的训练数据后，开始构建决策树，具体方法是每一个结点从全体特征集M中选取m个特征进行构建（通常m远小于M）。3.在构造每棵决策树时，选取基尼指数最小的特征来分裂节点构建决策树。决策树的其他结点都采取相同的分裂规则进行构建，直到该节点的所有训练样例都属于同一类或者达到树的最大深度。（该步骤与构建单一决策树相同）4.重复第2步和第3步多次，每一次输入数据对应一颗决策树，这样就得到了一个随机森林，用于对预测数据进行决策。 随机森林算法的注意点：1.在构建决策树的过程中不需要剪枝2.整个森林中树的数量和每棵树的特征需要人为设定3.构建决策树的时候分裂节点的选择依据最小基尼系数 （基尼系数表示被分错的概率）4.在训练时，对于预选变量个数和随机森林中树的个数这两个参数的调优很重要。 随机森林的优点：1.两个随机性的引入，使得随机森林抗噪声能力强，方差小泛化能力强，不易陷入过拟合（因此不需要剪枝）2.易于高度并行训练，且训练速度快，适合处理大数据3.由于随机森林对误差率是无偏估计，因此在算法中不需要再进行交叉验证或者设置单独的测试集来获取测试集上误差的无偏估计 123456from sklearn.ensemble import AdaboostClassifierfrom sklearn.tree import DecisionTreeClassifier//max_depth限制最大深度，防止过拟合。最大深度的增加虽然可以增加对训练集拟合能力的增强,但这也就可能意味着其泛化能力的下降ada_clf = AdaboostClassifier(DecisionTreeClassifier(max_depth=2),n_estimators=500)ada_clf.fit(x_train,y_train)ada_clf.score(x_test,y_test) GBDTGBDT是第一棵树分类后的残差，作为第二棵树的输入;依次类推直到残差为0或者到达树深 以决策树（CART）为基学习器的GB算法 123456#Gradient Boostingfrom sklearn.ensemble import GradientBoostingClassifiergb_clf = GradientBoostingClassifier(max_depth=2,n_estimators=30)gb_clf.fit(x_train,y_train)gb_clf.score(x_test,y_test) Boosting解决回归问题 12from sklearn.ensemble import AdaBoostRegressorfrom sklearn.ensemble import GradientBoostingRegressor GGBDT是GB和DT的结合。要注意的是这里的决策树是回归树，GBDT中的决策树是个弱模型，深度较小一般不会超过5，叶子节点的数量也不会超过10，对于生成的每棵决策树乘上比较小的缩减系数（学习率&lt;0.1），有些GBDT的实现加入了随机抽样（subsample 0.5&lt;=f &lt;=0.8）提高模型的泛化能力。通过交叉验证的方法选择最优的参数。因此GBDT实际的核心问题变成怎么基于使用CART回归树生成 Adaboost每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。 XgboostXgboost优点(与GBDT对比) 1.xgboost对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。 3.xgboost在代价函数里增加了正则项，用于控制模型复杂度。正则项里包含了叶子的结点个数，每个叶子结点输出的score的L2的模的平方和。正则项降低了模型的方差，使模型更简单，防止过拟合 4.支持并行化，直接的效果是训练速度快，boosting技术中下一棵树依赖上述树的训练和预测，所以树与树之间应该是只能串行！但是，同层级节点可并行。具体地，对于某个节点，节点内选择最佳分类点，进行枚举的时候可以并行 Stacking Stacking简单理解就是讲几个简单的模型，一般采用将它们进行K折交叉验证输出预测结果，然后将每个模型输出的预测结果合并为新的特征，并使用新的模型加以训练。 XGB模型，把train分train1~train5,共5份，用其中4份预测剩下的那份,同时预测test数据，这样的过程做5次,生成5份train（原train样本数/5）数据和5份test数据。然后把5份预测的train数据纵向叠起来，把test预测的结果做平均。 RF模型和XGB模型一样，再来一次。这样就生成了2份train数据和2份test数据（XGB重新表达的数据和RF重新表达的数据） 然后用LR模型，进一步做融合，得到最终的预测结果。 5.CNN、DNN卷积运算对应相乘再相加 softmax函数softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类 常用的激活函数及其作用：作用：通过加入非线性因素，提高模型对数据的拟合能力，以此解决线性不可分的问题 在深度学习中，常用的激活函数主要有：sigmod函数，tanh函数，ReLu函数 sigmod函数： sigmod函数将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：$$g(z)=\frac{1}{1+e^{-1}}$$ sigmod函数缺点： 1.当z值非常大或者非常小时，sigmod导数趋于0，这会导致权重w的梯度将接近于0，即出现梯度消失现象 2.函数的输出不是以0为为均值。那么对于一个多层的sigmod神经网络来说，如果你输入的都是正数，在反向传播中w的梯度传播到网络的某一处时，权值的变化是要么全正要么全负，模型拟合的过程就会 非常慢 所以， sigmoid函数可用在网络最后一层，作为输出层进行二分类，尽量不要使用在隐藏层。 tanh函数： tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1) 之间，其公式与图形为：$$g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}$$ tanh函数在 00 附近很短一段区域内可看做线性的。由于tanh函数均值为 0 ，因此弥补了sigmoid函数均值为 0.5的缺点。 tanh函数的缺点同sigmoid函数的第一个缺点一样，当 z 很大或很小时，g′(z)接近于 0，会导致梯度很小，权重更新非常缓慢，即梯度消失问题。 ReLU函数 ReLU函数又称为修正线性单元（Rectified Linear Unit），是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的梯度消失问题。ReLU函数的公式以及图形如下：$$g(z)=\begin{cases} 0，z&lt;0\ z， z&gt;0\end{cases}$$ ReLU函数的优点：在输入为正数的时候,不存在梯度消失问题。计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）ReLU函数的缺点：当输入为负时，梯度为0，会产生梯度消失问题。 神经网络：神经网络的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。增加节点数：增加维度，即增加线性转换能力。增加层数：增加激活函数的次数，即增加非线性转换次数。 对卡在局部极小值的处理方法：1.调节步伐：调节学习速率，使每一次的更新“步伐”不同。常用方法：随机梯度下降、小批量梯度下降、增加动量 BGD(Batch gradient descent)批量梯度下降法：每次迭代使用所有的样本 每次迭代都需要把所有样本都送入，每训练一组样本就把梯度更新一次。。 SGD（Stochastic gradientdescent）随机梯度下降法：每次迭代使用一组样本 针对BGD算法训练速度过慢的缺点，提出了SGD算法，SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了 MBGD（Mini-batch gradient descent）小批量梯度下降：每次迭代使用b组样本 SGD相对来说要快很多，但是也有存在问题，由于单个样本的训练可能会带来很多噪声，使得SGD并不是每次迭代都向着整体最优化方向，因此在刚开始训练时可能收敛得很快，但是训练一段时间后就会变得很慢。在此基础上又提出了小批量梯度下降法，它是每次从样本中随机抽取一小批进行训练，而不是一组。 2.优化起点：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。 常用方法有：高斯（正态）分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution） 初始化权重的意义：决定了loss在loss function中从哪个点开始作为起点训练网络。常用方法介绍： 均匀分布初始化权重：[0,1]范围的均匀分布 可以使用TensorFlow提供的tf.random_uniform()函数。该函数默认在[0，1]范围内取值。 正态分布初始化权重：上面尝试的权重初始化方法都是在权重的取值要靠近0而不能太小的方向上进行着。正态分布正好符合这个方向，其大部分取值靠近0。 浅层VS深层：相比浅层神经网络，深层神经网络可以用更少的数据量来学到更好的拟合。深层的前提是：空间中的元素可以由卷积迭代而来的。 防止过拟合：过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。 防止过拟合的方法： L 2正则化 Dropout 每个epoch之后shuffle训练数据，设置early-stopping 加Batch Normalization（对隐藏层的神经元在激活值获得之后，非线性变换之前进行强制归一化） 名词解释： epoch：当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个 epoch。随着 epoch 数量增加，神经网络中的权重的更新次数也增加，曲线从欠拟合变得过拟合。 batch size：一个 batch 中的样本总数。记住：batch size 和 number of batches 是不同的 batch 是什么？———在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个 batch。 iteration: 迭代数是 batch 需要完成一个 epoch 的次数。记住：在一个 epoch 中，batch 数和迭代数是相等的。 具体解释： L 2正则化：L 2正则表达式是 所有参数平方和的开方值。符合高斯分布，是完全可微的。L 2正则化使值最小时对应的参数变小。 Dropout：在每个训练批次中，让一半的隐层节点值为0。也就是在向前传播时，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强。 将shuffle参数设置为True, 则训练数据将在每个epoch混洗；设置early-stopping（在训练过程中，记录到目前为止最好的验证集精度，当连续10次Epoch（或者更多次）没达到最佳精度时，则可以认为精度不再提高了，则停止训练，将停止之后的权重作为网络的参数），所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。 加Batch Normalization，对隐藏层每个神经元做归一化。即可以想象成在每个隐层上又加了一层BN操作层，它位于x=wu+b激活值获得之后，非线性函数变换之前。对于每个隐层神经元，把输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以收敛地快，能提升效果。 CNN卷积神经网络的层级结构CNN的三个基本层： （1）卷积：对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的范围体现权值共享。一次卷积运算提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算，这也就是多层卷积。 （2）池化：池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。池化层用的方法：Max pooling, average pooling。池化层的具体作用：特征不变性，特征降维，一定程度防止过拟合。（3）全连接：softmax分类 CNN所有层级结构： 数据输入层 （预处理）： [1去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。 [2归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。 [3PCA：用PCA降维 卷积计算层 在这个卷积层，有两个关键操作： • 局部关联。每个神经元看做一个滤波器(filter) • 窗口(receptive field)滑动， filter对局部数据计算 卷积层遇到的几个名词： • 深度/depth（上一层卷积核的个数（滤波器的个数）就是下一层卷积层的深度) • 步长/stride （窗口一次滑动的长度） • 填充值/zero-padding ReLU激励层 把卷积层输出结果做非线性映射。CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱。 激励层的实践经验： ①不要用sigmoid！不要用sigmoid！不要用sigmoid！ ② 首先试RELU，因为快，但要小心点 ③ 如果2失效，请用Leaky ReLU或者Maxout ④ 某些情况下tanh倒是有不错的结果，但是很少 池化层：max pooling 全连接层 通常在CNN的尾部进行重新拟合，减少特征信息的损失。做完Max Pooling后，我们就会把这些数据“拍平”，丢到Flatten层，然后把Flatten层的output放到full connected Layer里，采用softmax对其进行分类。 CNN卷积神经网络的优点局部连接、权值共享 CNN与DNN的区别DNN的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同），CNN的输入可以是tensor，例如二维矩阵，通过filter获得局部特征，较好的保留了平面结构信息。 6.LDA主题模型LDA主题模型，它能够将文档集中每篇文档的主题以概率分布的形式给出。从而通过分析一些文档抽取出它们的主题分布出来后，便能够依据主题（分布）进行主题聚类或文本分类。同一时候，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。此外，一篇文档能够包括多个主题，文档中每一个词都由当中的一个主题生成。 人类是怎么生成文档的呢？LDA的这三位作者在原始论文中给了一个简单的样例。比方假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练。获取每一个主题Topic相应的词语。例如以下图所看到的： 然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的反复这两步，终于生成例如以下图所看到的的一篇文章（当中不同颜色的词语分别相应上图中不同主题下的词）： 而当我们看到一篇文章后。往往喜欢推測这篇文章是怎样生成的，我们可能会觉得作者先确定这篇文章的几个主题，然后环绕这几个主题遣词造句，表达成文。 LDA就是要干这事：依据给定的一篇文档，推測其主题分布（你计算机给我推測分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。） 7.Word2Vec 17.语言模型 || glove词向量1.原始CBOWCbow模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。 这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。 2.改进CBOWword2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。 word2vec对这个模型做了改进，（改进1）首先，对于从输入层到隐藏层的映射，没有采取神经网络的线性变换加激活函数的方法，而是采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。 第二个改进就是(改进2)从隐藏层到输出的softmax层这里的计算量个改进。为了避免要计算所有词的softmax概率，word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的大小。 而内部节点则起到隐藏层神经元的作用。由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一棵二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词w2。 和之前的神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。 如何“沿着霍夫曼树一步步完成”呢？在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数， 这里总结下基于Hierarchical Softmax的CBOW模型算法流程，梯度迭代使用了随机梯度上升法： 输入：基于CBOW的语料训练样本，词向量的维度大小M，CBOW的上下文大小2c,步长η 输出：霍夫曼树的内部节点模型参数θ，所有的词向量w 1. 基于语料训练样本建立霍夫曼树。 2. 随机初始化所有的模型参数θ，所有的词向量w 3. 进行梯度上升迭代过程 3.改进Skip-Gram现在我们先看看基于Skip-Gram模型时，Hierarchical Softmax如何使用。此时输入的只有一个词w,输出的为2c个词向量context(w)。 我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的c个词和后面的c个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。 1.我们需要先将词汇表建立成一颗霍夫曼树。 2.对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即Xw就是词w对应的词向量。 3.通过梯度上升法来更新我们的θ和Xw，注意这里的Xw周围有2c个词向量，Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2c个输出进行迭代更新。 4.glove模型我的理解是skip-gram、CBOW每次都是用一个窗口中的信息更新出词向量，但是Glove则是用了全局的信息（共现矩阵），也就是多个窗口进行更新. 注意，glove模型的代价函数如下，建议记住。具体由来下面会介绍。N是词汇表的大小，Vi,Vj是单词i和j的词向量，bi,bj是两个标量（作者定义的偏差项，）N是词汇表的大小（共现矩阵维度为N*N）$$J=\sum_{i,j}^{N}{f(X_{i,j})(v^T_iv_j+b_i+b_j-log(X_{i,j}))^2}$$具体权重函数f(x)应该是怎么样的呢？首先应该是非减的，其次当词频过高时，权重不应过分增大，作者通过实验确定权重函数f(x)为：$$f(x)=\begin{cases} (\frac{x}{x_{max}})^{0.75}，x&lt;x_{max}\ 1, x&gt;=x_{max}\end{cases}$$具体方法概述： 首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。 开始 -&gt; 统计共现矩阵 -&gt; 训练词向量 -&gt; 结束 1.统计共现矩阵 设共现矩阵为X，其元素为Xij . Xij的意义为：在整个语料库中，单词i和单词j共同出现在一个窗口中的次数。举个例子，设有语料库，i love you but you love him i am sad 这个小小的语料库只有1个句子，涉及到7个单词：i、love、you、but、him、am、sad。如果我们采用一个窗口宽度为5（左右长度都为2）的统计窗口，那么就有以下窗口内容： 窗口标号 中心词 窗口内容 0 i i love you 1 love i love you but 2 you i love you but you 3 but love you but you love 4 you you but you love him 5 love but you love him i 6 him you love him i am 7 i love him i am sad 8 am him i am sad 9 sad i am sad 窗口0、1长度小于5是因为中心词左侧内容少于2个，同理窗口8、9长度也小于5。以窗口5为例说明如何构造共现矩阵：中心词为love，语境词为but、you、him、i；则执行：$$X_{love,but}+=1$$ $$X_{love,I}+=1$$ 使用窗口将整个语料库遍历一遍，即可得到共现矩阵X。 2.使用glove模型训练词向量: 那么作者为什么这么构造模型呢？首先定义几个符号：$$X_i=\sum_{j=1}^{N}{X_{i,j}}$$其实就是矩阵单词i那一行的和；$$P_{i,k}=\frac{X_{i,k}}{X_i}$$条件概率，表示单词k出现在单词i语境中的概率；$$ratio_{i,j,k}=\frac{P_{i,k}}{p_{j,k}}$$两个条件概率的比率。作者的灵感是这样的：作者发现，ratio{i,j,k}这个指标是有规律的，规律统计在下表： ratio**i,j,k的值 单词j,k相关 单词j,k不相关 单词i,k相关 趋近1 很大 单词i,k不相关 很小 趋近1 思想：假设我们已经得到了词向量，如果我们用词向量Vi,Vj,Vk通过某种函数计算ratioi,jk 能够同样得到这样的规律的话，就意味着我们词向量与共现矩阵具有很好的一致性，也就说明我们的词向量中蕴含了共现矩阵中所蕴含的信息。设用词向量Vi,Vj,Vk计算ratioi,j,k的函数为g(vi,vj,vk)（我们先不去管具体的函数形式），那么应该有：$$\frac{P_{i,k}}{p_{j,k}}=ratio_{i,j,k}=g(v_i,v_j,v_k)$$即：$$\frac{P_{i,k}}{p_{j,k}}=g(v_i,v_j,v_k)$$即二者应该尽可能地接近；很容易想到用二者的差方来作为代价函数：$$J=\sum{{i,j,k}^{N}(\frac{P{i,k}}{p_{j,k}}-g(v_i,v_j,v_k))^2}$$但是仔细一看，模型中包含3个单词，这就意味着要在NNN的复杂度上进行计算，太复杂了，最好能再简单点。 作者的脑洞是这样的： 要考虑单词i和单词j之间的关系，那g(vi,vj,vk)中大概要有这么一项吧：Vi-Vj；嗯，合理，在线性空间中考察两个向量的相似性，不失线性地考察，那么vi−vj大概是个合理的选择； ratioi,j,k是个标量，那么g(vi,vj,vk)最后应该是个标量啊，虽然其输入都是向量，那內积应该是合理的选择，于是应该有这么一项吧：(Vi-Vj)^T(Vk) (行向量*列向量) 然后作者又往(vi−vj)^T * vk的外面套了一层指数运算exp(). 套上之后，我们的目标是让以下公式尽可能地成立： $$\frac{P_{i,k}}{p_{j,k}}=g(v_i,v_j,v_k)$$ 即：$$\frac{P_{i,k}}{p_{j,k}}=exp((V_i-Vj)^TV_k)$$即：$$\frac{P_{i,k}}{p_{j,k}}=exp((V_i^TV_k-V_j^TV_k)$$即：$$\frac{P_{i,k}}{p_{j,k}}=\frac{exp(V_i^TV_k)}{exp(V_j^TV_k)}$$然后就发现找到简化方法了：只需要让上式分子对应相等，分母对应相等，即：$$P_{i,j}=exp(V_i^TV_k)$$两边取个对数：$$log(P_{i,j})=V_i^TV_k$$那么代价函数就可以化简为：$$J=\sum_{i,j}^{N}{log(P_{i,j}-V^T_iV_j)^2}$$现在只需要在NN的复杂度上进行计算，而不是NNN，现在关于为什么第3步中，外面套一层exp()就清楚了，正是因为套了一层exp()，才使得差形式变成商形式，进而等式两边分子分母对应相等，进而简化模型。现将代价函数中的条件概率展开：$$log(P_{i,j})=V^T_iV_j$$即为：$$log(X_{i,j})-log(X_{i})=V^T_iV_j$$将其变为：$$log(X_{i,j})=V^T_iV_j+b_i+b_j$$于是代价函数就变成了：$$J=\sum_{i,j}^{N}{(v^T_iv_j+b_i+b_j-log(X_{i,j}))^2}$$然后基于出现频率越高的词对儿权重应该越大的原则，在代价函数中添加权重项，于是代价函数进一步完善：$$J=\sum_{i,j}^{N}{f(X_{i,j})(v^T_iv_j+b_i+b_j-log(X_{i,j}))^2}$$ 8.WordEmbedding如果将word看作文本的最小单元，可以将Word Embedding理解为一种映射，其过程是：将文本空间中的某个word，通过一定的方法，映射或者说嵌入（embedding）到另一个数值向量空间（之所以称之为embedding，是因为这种表示方法往往伴随着一种降维的意思。 WordEmbedding的输入： Word Embedding的输入是原始文本中的一组不重叠的词汇，假设有句子：apple on a apple tree。那么为了便于处理，我们可以将这些词汇放置到一个dictionary里，例如：[“apple”, “on”, “a”, “tree”]，这个dictionary就可以看作是Word Embedding的一个输入。 WordEmbedding的输出： Word Embedding的输出就是每个word的向量表示。 WordEmbedding的类型： 基于频率的Word Embedding（Frequency based embedding） 基于预测的Word Embedding（Prediction based embedding） 基于频率的Word Embedding8.1TF-IDF词频（TF） = 某个词在文章中的出现次数 逆文档频率（IDF） = log（语料库的文档总数/包含该词的文档总数+1） 8.2共现矩阵 &lt;&gt; 7.4 glove模型glove模型，定义一个比如大小为5的共现窗口 基于预测的Word Embedding词的表示中如果蕴含了上下文信息，那么将会更加接近自然语言的本质；并且，由于相似的词有相似的表示方法，甚至可以进行一些运算，例如：人类-男人=女人。但是，上述讨论中，有一个很大的缺陷，那就是词的向量表示维度过大，一个词要用大量其余的词来表示，为后续运算带来了很大的麻烦。因此，我们需要找到一种更好的表示方法，这种方法需要满足如下两点要求： 1.携带上下文信息 2.词的表示是稠密的 方法：CBOW、Skip-Gram 9.tf-idftf:词频（TF） = 某个词在文章中的出现次数 文章有长短之分，为了便于不同文章的比较,做”词频”标准化。 词频（TF） = 某个词在文章中的出现次数 / 文章总词数 或者 词频（TF） = 某个词在文章中的出现次数 / 拥有最高词频的词的次数 Idf:逆文档频率（IDF） = log（语料库的文档总数/包含该词的文档总数+1） TF-IDF = 词频（TF) * 逆文档频率（IDF） 优点：TF-IDF算法的优点是简单快速，结果比较符合实际情况。 缺点：单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。 （一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重） 10.SVMSVM原理基于训练样本D 在二维空间中找到一个符合这样条件的超平面来分开二类样本。这个超平面离两类样本都足够远，也就是使得“间隔”最大。即最终确定的参数w和b,使得r最大。即要： min (1/2)||w||^2 实际中，对某个实际问题函数来寻找一个合适的空间进行映射是非常困难的，幸运的是，在计算中发现，我们需要的只是两个向量在新的映射空间中的内积结果，而映射函数到底是怎么样的其实并不需要知道。这就需要引入了核函数的概念。核函数是这样的一种函数：仍然以二维空间为例，假设对于变量x和y，将其映射到新空间的映射函数为φ，则在新空间中，二者分别对应φ(x)和φ(y)，他们的内积则为&lt;φ(x),φ(y)&gt;。我们令函数Kernel(x,y)=&lt;φ(x),φ(y)&gt;=k(x,y)，可以看出，函数Kernel(x,y)是一个关于x和y的函数！而与φ无关！这是一个多么好的性质！我们再也不用管φ具体是什么映射关系了，只需要最后计算Kernel(x,y)就可以得到他们在高维空间中的内积，这样就可以直接带入之前的支持向量机中计算！ RBF是最常用的核函数，RBF核函数可以将维度扩展到无穷维的空间，RBF对应的是泰勒级数展开，在泰勒级数中，一个函数可以分解为无穷多个项的加和，其中，每一个项可以看做是对应的一个维度，这样，原函数就可以看做是映射到了无穷维的空间中. 通过计算间隔和松弛变量等的最大化，可以对问题进行求解。 逻辑回归和SVM区别相同点: 都是分类算法 都是监督学习算法 都是判别模型 都能通过核函数方法针对非线性情况分类 目标都是找一个分类超平面 都能减少离群点的影响 不同点: 损失函数不同，逻辑回归是对数损失函数，svm是hinge loss，自带L2正则项 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。 逻辑回归对概率建模，svm对分类超平面建模 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响 逻辑回归是统计方法，svm是几何方法 SVM的参数SVM模型有两个非常重要的参数C与gamma。 1.C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差 2.gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，σ越小（σ^2是正太分布的方差，描述数据的离散程度），数据分布越集中，曲线越瘦高，那么会造成只作用于支持向量样本附近，对于未知样本分类效果很差，就会存在训练准确率很高而测试准确率不高的可能，也就是过训练。 而若gamma太小，会造成平滑效应太大，无法在训练集上得到比较好的结果，更不用说测试集准确率。 11.LexRank 、TextRankPageRank:对于某个互联网网页A来说，该网页PageRank的计算基于下面两个基本如果： 数量如果：在Web图模型中，如果一个页面节点接收到的其它网页指向的入链数量越多，那么这个页面越重要。 质量如果：指向页面A的入链质量不同，质量高的页面会通过链接向其它页面传递很多其它的权重。所以越是质量高的页面指向页面A，则页面A越重要。 PageRank的计算充分利用了两个如果：数量如果和质量如果。过程例如以下： 1）在初始阶段：网页通过链接关系构建起Web图，每一个页面设置同样的PageRank值，通过若干轮的计算，会得到每一个页面所获得的终于PageRank值。随着每一轮的计算进行，网页当前的PageRank值会不断得到更新。 ​ 2）在一轮中更新页面PageRank得分的计算方法：在一轮更新页面PageRank得分的计算中，每一个页面将其当前的PageRank值平均分配到本页面包括的出链上，这样每一个链接即获得了对应的权值。而每一个页面将全部指向本页面的入链所传入的权值求和，就可以得到新的PageRank得分。当每一个页面都获得了更新后的PageRank值，就完毕了一轮PageRank计算。 Case1.网页都有出入链 此种情况下的网页A的PR值计算公式为： Case2存在没有出链的网页: 网页C是没有出链。因为C没有出链，所以对A,B,D网页没有PR值的贡献。PageRank算法的策略：从数学上考虑，为了满足Markov链，设定C对A,B,C,D都有出链（也对他自己也出链~）。你也可以理解为：没有出链的网页，我们强制让他对所有的网页都有出链，即让他对所有网页都有PR值贡献。 此种情况PR(A)的计算公式： Case3存在只对自己有出链的网页： C是只对自己出链的网页。 此时访问C时，不会傻乎乎的停留在C页面，一直点击C-Link循环进入C，即C网页只对自己的网页PR值有贡献。正常的做法是，进入C后，存在这种情况：在地址输入栏输入A/B/C/D的URL地址，然后跳转到A/B/C/D进行浏览，这就是PageRank算法解决这种情况的策略：设定存在一定概率为α，用户在地址栏输入A/B/C/D地址，然后从C跳转到A/B/C/D进行浏览。 此时PR(A)的计算公式为： ，一般取值α=0.85 一般情况下，一个网页的PR值计算公式为： 注：Mpi是有出链到pi的所有网页集合，L(pj)是有网页pj的出链总数，N是网页总数，α一般取值为0.85（阻尼系数） 所有网页PR值同时计算需要迭代计算：一直迭代计算，停止直到下面2情况之一发生：每个网页的PR值前后误差dleta_pr小于自定义误差阈值，或者迭代次数超过了自定义的迭代次数阈值 TextRank:基于TextRank的自动文摘属于自动摘录，通过选取文本中重要度较高的句子形成文摘，其主要步骤如下： （1）预处理：将输入的文本或文本集的内容分割成句子得，构建图G =（V,E），其中V为句子集，对句子进行分词、去除停止词，得，其中是保留后的候选关键词。 （2）句子相似度计算：构建图G中的边集E，基于句子间的内容覆盖率，给定两个句子，采用如下公式进行计算： 若两个句子之间的相似度大于给定的阈值0.01，就认为这两个句子语义相关并将它们连接起来，即边的权值； （3）句子权重计算：根据公式，迭代传播权重计算各句子的得分； 和pageRank的公式相比，基本上就是把原来对应边的部分添加了权重，边的数量和改成了权重和，很好理解。 （4）抽取文摘句：将（3）得到的句子得分进行倒序排序，抽取重要度最高的T个句子作为候选文摘句。 （5）形成文摘：根据字数或句子数要求，从候选文摘句中抽取句子组成文摘。 使用 TextRank 算法计算图中各节点的得分时，同样需要给图中的节点指定任意的初值，通常都设为1。然后递归计算直到收敛，即图中任意一点的误差率小于给定的极限值时就可以达到收敛，一般该极限值取 0.0001。 其它计算相似度的方法有：基于编辑距离，基于语义词典，余弦相似度等。 12.VSM向量空间模型 我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。Document = {term1, term2, …… ,term N}Document Vector = {weight1, weight2, …… ,weight N}同样我们把查询语句看作一个简单的文档，也用向量来表示。Query = {term1, term 2, …… , term N}Query Vector = {weight1, weight2, …… , weight N} 我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。 我们认为两个向量之间的夹角越小，相关性越大。所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。 我们只要比较下图中的α,θ的余弦值的大小，余弦值越大，相似度越高。公式如下： 这里的 分别代表向量A和B的各分量。 14.关联规则如何来度量一个规则是否够好？有两个量，置信度(Confidence)和支持度(Support)。假设有如下表的购买记录。 置信度表示了这条规则有多大程度上值得可信。设条件的项的集合为A,结果的集合为B。置信度计算在A中，同时也含有B的概率。即Confidence(A==&gt;B)=P(B|A)。例 如计算”如果Orange则Coke”的置信度。由于在含有Orange的4条交易中，仅有2条交易含有Coke.其置信度为0.5。 支持度计算在所有的交易集中，既有A又有B的概率。例如在5条记录中，既有Orange又有Coke的记录有2条。则此条规则的支持度为2/5=0.4。现在这条规则可表述为，如果一个顾客购买了Orange,则有50%的可能购买Coke。而这样的情况（即买了Orange会再买Coke）会有40%的可能发生。 算法原理和过程1.找出所有的频繁项目集，并计算其支持度。这些项集出现的频繁性至少和预定义的最小支持度一 样。 2.然后由频集产生强关联规则，这些规则必须满足最小支持度和最小可信度。 3.然后使用第1步找到的频繁项目集产生期望的规则，产生只包含集合的项的所有规则，其中 每一条规则的右部只有一项。 15.决策树首先，在了解树模型之前，自然想到树模型和线性模型有什么区别呢？其中最重要的是，树形模型是一个一个特征进行处理，之前线性模型是所有特征给予权重相加得到一个新的值。决策树与逻辑回归的分类区别也在于此，逻辑回归是将所有特征变换为概率后，通过大于某一概率阈值的划分为一类，小于某一概率阈值的为另一类；而决策树是对每一个特征做一个划分。另外逻辑回归只能找到线性分割（输入特征x与logit之间是线性的，除非对x进行多维映射），而决策树可以找到非线性分割。 而树形模型更加接近人的思维方式，可以产生可视化的分类规则，产生的模型具有可解释性（可以抽取规则）。树模型拟合出来的函数其实是分区间的阶梯函数。 决策树学习：采用自顶向下的递归的方法，基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处熵值为0（叶节点中的实例都属于一类）。 决策树的生成： 决策树思想，实际上就是寻找最纯净的划分方法，这个最纯净在数学上叫纯度，纯度通俗点理解就是目标变量要分得足够开（y=1的和y=0的混到一起就会不纯）。另一种理解是分类误差率的一种衡量。实际决策树算法往往用到的是，纯度的另一面也即不纯度，下面是不纯度的公式。不纯度的选取有多种方法，每种方法也就形成了不同的决策树方法，比如ID3算法使用信息增益作为不纯度；C4.5算法使用信息增益率作为不纯度；CART算法使用基尼系数作为不纯度。 决策树要达到寻找最纯净划分的目标要干两件事，建树和剪枝 建树： （1）如何按次序选择属性（最小熵，最大增益，最大增益率，最小基尼系数） 也就是首先树根上以及树节点是哪个变量呢？这些变量是从最重要到次重要依次排序的，那怎么衡量这些变量的重要性呢？ ID3算法用的是信息增益，C4.5**算法用信息增益率；CART**算法使用基尼系数。决策树方法是会把每个特征都试一遍，然后选取那个，能够使分类分的最好的特征，也就是说将A属性作为父节点，产生的纯度增益（GainA）要大于B属性作为父节点，则A作为优先选取的属性。 2） 如何分裂训练数据（对每个属性选择最优的分割点） 如何分裂数据也即分裂准则是什么？依然是通过不纯度来分裂数据的，通过比较划分前后的不纯度值，来确定如何分裂。 信息熵表示不确定性 信息熵越小，表示不确定性越低，即稳定性越高，因为右侧有一类占了70%，这个类别影响力更大 CART分类树在很多书籍和资料中介绍比较多，但是再次强调GDBT中使用的是回归树。作为对比，先说分类树，我们知道CART是二叉树，CART分类树在每次分枝时，是穷举每一个feature的每一个阈值，根据GINI系数找到使不纯性降低最大的的feature以及其阀值，然后按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝，每个分支包含符合分支条件的样本。用同样方法继续分枝直到该分支下的所有样本都属于统一类别，或达到预设的终止条件，若最终叶子节点中的类别不唯一，则以多数人的类别作为该叶子节点的性别。回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是GINI系数，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。 16.K-Means核心思想：由用户指定k个初始质心（initial centroids)，以作为聚类的类别（cluster），重复迭代直至算法收敛。 算法流程： 选取k个初始质心（作为初始cluster）；repeat：对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster；重新计算k个cluser对应的质心；until 质心不再发生变化 缺点： k-Means是局部最优的，容易受到初始质心的影响。 17.语言模型 &lt;&gt; 7.Word2Vec(CBOW) &lt;&gt; N-gram &lt;&gt; 朴素贝叶斯简单的说，语言模型 （Language Model） 是用来计算一个句子出现概率的模型，假设句子 ,其中 代表句子中的第 个词语，则语句 W 以该顺序出现的概率可以表示为：p(S)=p(w1,w2,w3,w4,w5,…,wn) = p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)//链规则。 那么，如何计算p(wi|w1,w2,…,wi-1)呢？最简单、直接的方法是直接计数做除法，如下： p(wi|w1,w2,…,wi-1) = p(w1,w2,…,wi-1,wi) / p(w1,w2,…,wi-1) 但是，这里面临两个重要的问题：数据稀疏严重；参数空间过大，无法实用。 1.朴素贝叶斯贝叶斯公式： 判断一条微信朋友圈是不是广告。前置条件是：我们已经拥有了一个平日广大用户的朋友圈内容库，这些朋友圈当中，如果真的是在做广告的，会被“热心网友”打上“广告”的标签，我们要做的是把所有内容分成一个一个词，每个词对应一个维度，构建一个高维度空间 (别担心，这里未出现向量计算)。当出现一条新的朋友圈new_post，我们也将其分词，然后投放到朋友圈词库空间里。 这里的X表示多个特征（词）x1,x2,x3…组成的特征向量。 P(ad|x)表示：已知朋友圈内容而这条朋友圈是广告的概率。 利用贝叶斯公式，进行转换： P(ad|X) = p(X|ad) p(ad) / p(X) P(not-ad | X) = p(X|not-ad)p(not-ad) / p(X) 比较上面两个概率的大小，如果p(ad|X) &gt; p(not-ad|X),则这条朋友圈被划分为广告，反之则不是广告。 朴素贝叶斯假设：如果认为每个词都是独立的特征，那么朋友圈内容向量可以展开为分词(x1,x2,x3…xn)，因此有了下面的公式推导： P(ad|X)*p(x) = p(X|ad)p(ad) = p(x1, x2, x3, x4…xn | ad) p(ad), p(x)对于所有类别为常数 假设所有词相互条件独立，则进一步拆分： P(ad|X)*p(x) = p(x1|ad)p(x2|ad)p(x3|ad)…p(xn|ad) p(ad) 至此，P(xi|ad)很容易求解，P(ad)为词库中广告朋友圈占所有朋友圈（训练集）的概率。虽然现实中，一条朋友圈内容中，相互之间的词不会是相对独立的，因为我们的自然语言是讲究上下文的,所以由朴素贝叶斯引出n-garm 2.n-gramN-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 N-gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有 Bi-gram (N=2) 和 Tri-gram (N=3)，一般已经够用了。例如在上面这句话里，我可以分解的 Bi-gram 和 Tri-gram ： Bi-gram : {I, love}, {love, deep}, {deep, learning} Tri-gram : {I, love, deep}, {love, deep, learning} N-gram中概率计算: 假设我们有一个由n个词组成的句子S=(w1,w2,⋯,wn) 如何衡量它的概率呢？让我们假设，每一个单词wi都要依赖于从第一个单词w1到它之前一个单词wi−1的影响： p(S)=p(w1w2⋯wn)=p(w1)p(w2|w1)⋯p(wn|wn−1⋯w2w1)不过这个衡量方法有两个缺陷： 参数空间过大，概率 p(wn|wn−1⋯w2w1)的参数有O(n) 个。 数据稀疏严重，词同时出现的情况可能没有，组合阶数高时尤其明显。 为了解决第一个问题(参数空间过大)，我们引入马尔科夫假设（Markov Assumption）：一个词的出现仅与它之前的若干个词有关。 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 Bi-gram： p(S)=p(w1w2⋯wn)=p(w1)p(w2|w1)⋯p(wn|wn−1)如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 Tri-gram： p(S)=p(w1w2⋯wn)=p(w1)p(w2|w1)⋯p(wn|wn−1wn−2) N-gram的 N可以取很高，然而现实中一般 bi-gram 和 tri-gram 就够用了。 那么，如何计算其中的每一项条件概率 p(wn|wn−1⋯w2w1)呢？答案是极大似然估计（Maximum Likelihood Estimation，MLE），说人话就是数频数：p(Wn|Wn−1)=C(Wn−1Wn) / C(Wn−1) p(Wn|Wn−1Wn−2)=C(Wn−2Wn−1Wn)C(Wn−1Wn)p(Wn|Wn−1⋯W2W1)=C(W1W2⋯Wn)C(W1W2⋯Wn−1) 为了解决第二个问题（数据稀疏严重），因此，我们要进行数据平滑（data Smoothing），数据平滑的目的有两个：一个是使所有的N-gram概率之和为1，使所有的n-gram概率都不为0。它的本质，是重新分配整个概率空间，使已经出现过的n-gram的概率降低，补充给未曾出现过的n-gram。方法有拉普拉斯平滑，即强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可。 3.神经网络语言模型—CBOW是在此基础上进行改进的 来看 NNLM（神经网络语言模型） 构造 ,首先，引入了词向量的概念，即将词表中的词语 表示为一个固定长度为 M向量的形式 ，（m 为人工定义的词向量的长度），这样整个词表可以用 一个 的矩阵表示。现在找到词 wt的上下文 context(wt)，这里 Bengio 设定的上下文 context(wt)是词wt 的前 个词语。 神经网络的输入层：把这 n-1个词语的词向量首尾相接的拼起来，形成维度为 (n−1)m**的向量来当做神经网络的输入，所以 NNLM 输入层的大小已经确定为 (n−1)m 隐层：隐层的规模就是人工指定了 神经网络的输出层：输出层的大小为 |V|(词典的大小)，因为共有 |V|个词语，所以输出层维度为 |V| ， wt在词表 V中的下标对应的维度就是映射 wt的得分,而 softmax 正好可以把该得分归一化为概率。 4.CBOW (Word2Vec)对比原始神经网络语言模型使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。输入是上下两个窗口。 18.马尔可夫链19.向量空间模型20.RNN1.为什么具有记忆功能？因为此神经元结点上一时刻隐层的状态参与到了这个时刻的计算过程中。 在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出！表示成图就是这样的： 我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构 （引出LSTM的背景知识） RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，“梯度消失”现象又要出现了，只不过这次发生在时间轴上。对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。为了解决时间上的梯度消失，机器学习领域发展出了长短时记忆单元。LSTM 2.为什么LSTM记的时间长因为在rnn原始隐藏层的一个h状态上增加增加了一个状态c（单元状态），让它来保存长期的状态，并通过三个门控制长期c。这种结构使得误差向上一个状态传递时几乎没有衰减，所以权值调整的时候，对于很长时间之前的状态带来的影响和结尾状态带来的影响可以同时发挥作用，最后训练出来的模型就具有较长时间范围内的记忆功能。 具体解释： 其实，长短时记忆网络的思路比较简单。原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么，假如我们再增加一个状态，即c，让它来保存长期的状态，那么问题不就解决了么？如下图所示： 新增加的状态c，称为单元状态(cell state)。我们把上图按照时间维度展开： LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。三个开关的作用如下图所示： 接下来，我们要描述一下，输出h和单元状态c的具体计算方法。 前面描述的开关是怎样在算法中实现的呢？这就用到了门（gate）的概念。门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，是偏置项，那么门可以表示为： 门 可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层 和一个逐点相乘的操作来实现的。sigmoid 层输出（是一个向量）的每个元素都是一个在 0 和 1 之间的实数，表示让对应信息通过的权重（或者占比）。比如， 0 表示“不让任何信息通过”， 1 表示“让所有信息通过”。 LSTM通过三个这样的本结构来实现信息的保护和控制。这三个门分别输入门、遗忘门和输出门。 遗忘门： 最左边的遗忘门，遗忘们决定有多少重Ct-1 传播到Ct。 在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取ht−1 和xt ,输出一个在 0到 1之间的数值给每个在细胞状态ct-1中的数字。1 表示“完全保留”，0 表示“完全舍弃”。其中ht−1表示的是上一个cell的输出，xt 表示的是当前细胞的输入。σ 表示sigmod函数。 让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语 输入门： 输入门决定当前输入有多少保存到Ct 下一步是决定让多少新的信息加入到 cell 状态 中来。实现这个需要包括两个 步骤：首先，一个叫做“input gate layer ”的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容，即Ct(~) 在下一步，我们把这两部分联合起来，对 cell 的状态进行一个更新。 新的保存信息： 所以最终的保存信息为：符号*表示按元素乘 现在是更新旧细胞状态的时间了，Ct−1 更新为Ct 。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。我们把旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上it∗C˜t 。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。 输出： 最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。 在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。 3.RNN特点RNN（Recurrent Neural Network）是一类用于处理序列数据的神经网络【1】。首先我们要明确什么是序列数据，摘取百度百科词条：时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。这是时间序列数据的定义，当然这里也可以不是时间，比如文字序列，但总归序列数据有一个特点——后面的数据跟前面的数据有关系。 我们从基础的神经网络中知道，神经网络包含输入层、隐层、输出层，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学“到的东西就蕴含在“权值“中。 基础的神经网络只在层与层之间建立了权连接，RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。【2】只要知道上一时刻的隐藏状态ht−1与当前时刻的输入xt，就可以计算当前时刻的隐藏状态ht。 权值共享【3】在RNN中U、V、W的参数都是共享的，也就是只需要关注每一步都在做相同的事情，只是输入不同，这样来降低参数个数和计算量。 LSTM在RNN基础上解决了其时间序列上梯度消失的问题【4】 21.HMM22.CRF]]></content>
      <tags>
        <tag>Internship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-4-dp]]></title>
    <url>%2F2019%2F04%2F09%2FLeetcode-4-dp%2F</url>
    <content type="text"><![CDATA[这篇文章不仅是总结Leetcode上关于DP的题，也恰好是总结一下算法课中关于DP的内容。 Longest Common Subsequence Problem: 最长公共子序列。给定两个字符串，判定公共子序列的最大长度，这里的子序列可以是不连续的。Leetcode上有一道近似题：Given two words word1 and word2, find the minimum number of steps required to make word1 and word2 the same, where in each step you can delete one character in either string. Example: Input: “sea”, “eat”, output: 2. Explanation: You need one step to make “sea” to “ea” and another step to make “eat” to “ea”. 其实就是找出LCS的长度，然后减一下。 Link: https://leetcode.com/problems/delete-operation-for-two-strings/ Answer:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125# 1: 递归，O(2^m)，超时class Solution: def minDistance(self, word1: str, word2: str) -&gt; int: def LCS(X, Y, i, j): if i == -1 or j == -1: return 0 if X[i] == Y[j]: return LCS(X, Y, i - 1, j - 1) + 1 else: return max(LCS(X, Y, i - 1, j), LCS(X, Y, i, j - 1)) word1 = list(word1) word2 = list(word2) return len(word1) + len(word2) - 2 * LCS(word1, word2, len(word1) - 1, len(word2) - 1)# 2: 带有memo的递归，DP，ACclass Solution: def minDistance(self, word1: str, word2: str) -&gt; int: seen = &#123;&#125; def LCS(X, Y, i, j): if (i, j) in seen: return seen[(i, j)] else: if i == -1 or j == -1: seen[(i, j)] = 0 return seen[(i, j)] if X[i] == Y[j]: seen[(i, j)] = LCS(X, Y, i - 1, j - 1) + 1 return seen[(i, j)] else: seen[(i, j)] = max(LCS(X, Y, i - 1, j), LCS(X, Y, i, j - 1)) return seen[(i, j)] word1 = list(word1) word2 = list(word2) return len(word1) + len(word2) - 2 * LCS(word1, word2, len(word1) - 1, len(word2) - 1)# 3: 优化memo的DP，二维表class Solution: def minDistance(self, word1: str, word2: str) -&gt; int: m = len(word1) n = len(word2) memo = [[0] * (n + 1) for i in range(m + 1)] for i in range(1, m + 1): for j in range(1, n + 1): if word1[i - 1] == word2[j - 1]: memo[i][j] = memo[i - 1][j - 1] + 1 else: memo[i][j] = max(memo[i - 1][j], memo[i][j - 1]) return m + n - 2 * memo[-1][-1]# 4: 进一步优化DP，两个数组，bestclass Solution: def minDistance(self, word1: str, word2: str) -&gt; int: if len(word1) &lt; len(word2): pass else: word1, word2 = word2, word1 s, l = len(word1), len(word2) memo = [[0] * (l + 1), [0] * (l + 1)] for i in range(s): for j in range(1, l + 1): if word1[i] == word2[j - 1]: memo[1][j] = memo[0][j - 1] + 1 else: memo[1][j] = max(memo[0][j], memo[1][j - 1]) memo[0], memo[1] = memo[1], memo[0] return s + l - 2 * memo[0][-1]# 5: 若要追溯子序列，在3的基础上增加一个标记memo. 本代码来自网上def find_lcseque(X, Y): m = len(X) n = len(Y) L = [[0] * (n + 1) for i in range(m + 1)] B = [[&quot;D&quot;] * (n + 1) for i in range(m + 1)] for i in range(m + 1): for j in range(n + 1): if i == 0 or j == 0: L[i][j] = 0 if i == 0: B[i][j] = &quot;L&quot; else: B[i][j] = &quot;U&quot; elif X[i - 1] == Y[j - 1]: L[i][j] = L[i - 1][j - 1] + 1 else: if L[i - 1][j] &lt; L[i][j - 1]: L[i][j] = L[i][j - 1] B[i][j] = &quot;L&quot; else: L[i][j] = L[i - 1][j] B[i][j] = &quot;U&quot; return L, Bdef track_lcseque(X, Y, Track): result = [] start = (len(X) - 1, len(Y) - 1) if Track[-1][-1] == &quot;D&quot;: result.append(X[-1]) while start[0] or start[1]: next = Track[start[0]][start[1]] if next == &quot;D&quot;: start = (start[0] - 1, start[1] - 1) result.append(X[start[0]]) elif next == &quot;L&quot;: start = (start[0], start[1] - 1) else: start = (start[0] - 1, start[1]) return result[::-1]x = &apos;abdfg&apos;y = &apos;abcdfg&apos;l, b = find_lcseque(x, y)print(&quot;LCS：&quot;, end=&quot;&quot;)print(track_lcseque(x, y, b))print(&quot;LCS Len：&quot; + str(l[-1][-1])) Longest Common Subarray Problem: 最长公共子串，要求连续。Leetcode上原题：Given two integer arrays A and B, return the maximum length of an subarray that appears in both arrays. Example: Input: A: [1,2,3,2,1], B: [3,2,1,4,7], output: 3. Link: https://leetcode.com/problems/maximum-length-of-repeated-subarray/ Answer:1234567891011121314151617181920212223242526272829303132333435363738394041# Comment: 这一题和上一题最大的区别就是找出的序列要求连续。因此两者的区别也就集中在递归方程的变化上：本题仅仅要求继承对角线上的累积和即可class Solution: def findLength(self, word1, word2): if len(word1) &lt; len(word2): pass else: word1, word2 = word2, word1 s, l = len(word1), len(word2) memo = [[0] * (l + 1), [0] * (l + 1)] max_len = 0 for i in range(s): for j in range(1, l + 1): if word1[i] == word2[j - 1]: memo[1][j] = memo[0][j - 1] + 1 max_len = max(max_len, memo[1][j]) else: memo[1][j] = 0 memo[0], memo[1] = memo[1], memo[0] return max_len# 若要定位出位置，则需要加一个track变量。下面代码来自网上，不过区别不大def find_lcsubstr(X, Y): m = len(X) n = len(Y) L = [[0] * (n + 1) for i in range(m + 1)] max_length = 0 # 最长匹配的长度 p = 0 # 最长匹配对应在s1中的最后一位 for i in range(m): for j in range(n): if X[i] == Y[j]: L[i + 1][j + 1] = L[i][j] + 1 if L[i + 1][j + 1] &gt; max_length: max_length = L[i + 1][j + 1] p = i + 1 return X[p - max_length:p], max_lengthprint(find_lcsubstr(&apos;abcdfg&apos;, &apos;abdfg&apos;)) Longest Palindromic Subsequence Problem: 最长回文子序列。Leetcode原题：Given a string s, find the longest palindromic subsequence’s length in s. You may assume that the maximum length of s is 1000. Example: Input “XABCBBACXA”, output 7 (“XABCBAX”) Link: https://leetcode.com/problems/longest-palindromic-subsequence/ Answer:12345678910111213141516171819202122# Comment: 与LCSeq一致，求自身和倒序的LCSeq即可；Track方式与LCSeq一致class Solution: def longestPalindromeSubseq(self, s): word1 = s word2 = s[::-1] if word1 == word2: return len(word1) s, l = len(word1), len(word2) memo = [[0] * (l + 1), [0] * (l + 1)] for i in range(s): for j in range(1, l + 1): if word1[i] == word2[j - 1]: memo[1][j] = memo[0][j - 1] + 1 else: memo[1][j] = max(memo[0][j], memo[1][j - 1]) memo[0], memo[1] = memo[1], memo[0] return memo[0][-1] Longest Palindromic Subarray Problem: 最长回文子串。Leetcode原题：Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example: Input: “cbbd”, output: “bb”. Link: https://leetcode.com/problems/longest-palindromic-substring/ Answers:12345678910111213141516171819202122232425262728293031323334353637383940414243# Comemnt: 这里无法像上一题一样，用LCSub来解决。因为回文串不一定是最大子串。比如说“aacbdcaa”，镜像的最大子串是“aac”，但是回文只有“aa”# 用区间型动态规划来修改最大子串的查找方法class Solution: def longestPalindrome(self, s): l = len(s) memo = [[0] * l for i in range(l)] max_len = 0 start = end = 0 for ll in range(l): # 长度从0到l for i in range(l - ll): # i从头到l - 长度 j = i + ll if j == i: memo[i][j] = True elif j == i + 1: memo[i][j] = (s[i] == s[j]) else: memo[i][j] = (memo[i + 1][j - 1] and (s[i] == s[j])) if memo[i][j] and j - i + 1 &gt; max_len: max_len = j - i + 1 start, end = i, j return s[start:end + 1]# 另有一种Manacher算法，只需要O(n)：https://zh.wikipedia.org/wiki/%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2def manacher(s0 : str) -&gt; list: T = &apos;$#&apos; + &apos;#&apos;.join(s0) + &apos;#@&apos; l = len(T) P = [0] * l R, C = 0, 0 for i in range(1,l-1): if i &lt; R: P[i] = min(P[2 * C - i], R - i) while T[i+(P[i]+1)] == T[i-(P[i]+1)]: P[i] += 1 if P[i] + i &gt; R: R = P[i] + i C = i return P Longest Increasing Subsequence1# Leetcode Longest Increasing Subarray1# Leetcode 取数游戏：尼姆博弈1# Comment: 与最大回文子串、矩阵链问题一样，是区间型动态规划]]></content>
      <tags>
        <tag>Leetcode</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode-3-stock]]></title>
    <url>%2F2019%2F04%2F02%2FLeetcode-3-stock%2F</url>
    <content type="text"><![CDATA[整理Leetcode上关于股票买卖的一组问题，本质是用贪心/动态求解全局最优 Buy&amp;Sell 1 time Problem: Say you have an array for which the ith element is the price of a given stock on day i. If you were only permitted to complete at most one transaction (i.e., buy one and sell one share of the stock), design an algorithm to find the maximum profit. Note that you cannot sell a stock before you buy one. Example: Input [7,1,5,3,6,4], output 5. Buy at 1 and sell at 6. Link: https://leetcode.com/problems/best-time-to-buy-and-sell-stock/ Answer:123456789101112# Comment: 用一个变量记录已经查看过的价格中最低的点，称为买入价格；用另一个变量记录局部最优解，它的构成要么是当前价格 - 买入价格，要么就是之前的某个价格 - 买入价格。由此我们只需要遍历一遍数组即可。class Solution: def maxProfit(self, prices): if not prices: return 0 minprice = prices[0] maxprofit = 0 for price in prices[1:]: minprice = min(minprice, price) maxprofit = max(maxprofit, price - minprice) return maxprofit Buy&amp;Sell unlimited time Problem: Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (i.e., buy one and sell one share of the stock multiple times). Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example: Input [7,1,5,3,6,4], output: 7. 1st buy at 1 and sell at 5, then 2nd buy at 3 and sell at 6. Total we get 7. Link: https://leetcode.com/problems/best-time-to-buy-and-sell-stock-ii/ Answer:1234567891011# Comment: 本题与上一题的唯一区别就是可以买卖多次，所以局部最优解就从单次买卖的最优解变成累积和的最优解。我们不再需要记录买入价格，而是一旦当有盈利空间时，即当前价格 - 上一个价格 &gt; 0，就实行买卖。class Solution: def maxProfit(self, prices): if not prices: return 0 maxprofit = 0 for i in range(1, len(prices)): if prices[i] &gt; prices[i - 1]: maxprofit += prices[i] - prices[i - 1] return maxprofit Buy&amp;Sell at most 2 time Problem: Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete at most two transactions. Note: You may not engage in multiple transactions at the same time (i.e., you must sell the stock before you buy again). Example: Input: [3,3,5,0,0,3,1,4], output: 6. 1st buy on first 0 and sell at 3, 2nd buy on 1 and sell on 4. If we can buy as many as we could, we would add one buy-sell at 3 and 5, then the output will be 8. Link: https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iii/ Answer:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Comment1: 因为买卖不能重叠（一定是买卖买卖），所以我们可以找到一个适当的点将数组分为两部分，在左右子数组各求出一次交易的最大值，即可得到本题的解。至于一次交易的最大值，就是和第一题的解法一致。# Comment2: 基于Comment1，最先想到的办法是扫描整个数组，然后分别计算扫描左右数组计算最大利润。这样做整个算法的复杂度是O(n^2)。# Comment3: 改进的办法是扫描两遍数组，并将每个切分点的利润保存下来。第一遍是从前往后扫描，这样就能把从第一天到第i天的最大利润记录下来。第二遍是从后往前扫描，这样用来记录第i天到最后一天的最大利润。最后取出两个组在同一个分界点时的最大利润和即可。class Solution: def maxProfit(self, prices): if len(prices) &lt; 2: return 0 pre_profit = [0] minprice = prices[0] maxprofit = 0 for price in prices[1:]: minprice = min(minprice, price) maxprofit = max(maxprofit, price - minprice) pre_profit.append(maxprofit) pos_profit = [0] minprice = prices[-1] maxprofit = 0 for price in prices[::-1][:-1]: minprice = max(minprice, price) maxprofit = max(maxprofit, minprice - price) pos_profit.insert(0, maxprofit) for i, profit in enumerate(pre_profit): maxprofit = max(maxprofit, profit + pos_profit[i]) return maxprofit# Comment4: 思路是我们在第一次交易的基础上，考虑第二次交易。买卖买卖是不重叠的，因此第二次交易面临的世界必然是第一次交易决策完成后的世界。如果第一次交易没有进行，那么第二次交易就变为第一次交易；否则，我们可以将第一次交易的情况映射到股价上，这表示我们可以动用第一次交易的收益进行第二次交易，即在第二次交易前，股价会全面下跌第一次交易的收益。完成映射后，我们再进行第二次交易，此时的情况已经与第一次交易完全一致。class Solution: def maxProfit(self, prices): if len(prices) &lt; 2: return 0 minPrice1, minPrice2 = prices[0], prices[0] maxProfit1, maxProfit2 = 0, 0 for price in prices[1:]: minPrice1 = min(price, minPrice1) maxProfit1 = max(maxProfit1, price - minPrice1) minPrice2 = min(minPrice2, price - maxProfit1) maxProfit2 = max(maxProfit2, price - minPrice2) return maxProfit2 Buy&amp;Sell at most k time Problem: Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete at most k transactions. Note: You may not engage in multiple transactions at the same time (ie, you must sell the stock before you buy again). Example: Input: [3,2,6,5,0,3], k &gt;= 2, output: 7. 1st buy at 2 and sell at 6, 2nd buy at 0 and sell at 3. Link: https://leetcode.com/problems/best-time-to-buy-and-sell-stock-iv/ Answer:12345678910111213141516171819202122232425262728# Comment: 本题的关键是划定k的边界。对于一个长度为n的数组，买卖至多n // 2次。因此当k &gt;= (n // 2)时，本题退化为k可为无限次的Case II，只需要用不断贪心求累积和即可；否则，与给定上限次数的CaseI、III一致。class Solution: def maxProfit(self, k, prices): if k == 0: return 0 if len(prices) &lt; 2: return 0 if k &gt;= len(prices) // 2: maxprofit = 0 for i in range(1, len(prices)): if prices[i] &gt; prices[i - 1]: maxprofit += prices[i] - prices[i - 1] return maxprofit else: minPrices = [prices[0]] * k maxProfits = [0] * k for price in prices[1:]: minPrices[0] = min(price, minPrices[0]) maxProfits[0] = max(maxProfits[0], price - minPrices[0]) for i in range(1, k): minPrices[i] = min(minPrices[i], price - maxProfits[i - 1]) maxProfits[i] = max(maxProfits[i], price - minPrices[i]) return maxProfits[-1] Buy&amp;Sell unlimited time with cooldown limitation Problem: Say you have an array for which the ith element is the price of a given stock on day i. Design an algorithm to find the maximum profit. You may complete as many transactions as you like (ie, buy one and sell one share of the stock multiple times) with the following restrictions: You may not engage in multiple transactions at the same time (ie, you must sell the stock before you buy again). After you sell your stock, you cannot buy stock on next day. (ie, cooldown 1 day) Example: Input: [1,2,3,0,2], output: 3. 1st buy at 1 and sell at first 2, 2nd buy at 0 and sell at second 2. You cannot 1st buy&amp;sell at (1, 3) and 2nd buy&amp;sell at (0, 2), as we need cooldown Link: https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/ Answer:12345678910111213141516171819202122232425262728293031323334# Comment1: 本题与Case II类似，但实质上有着重大区别。因为我们无法连续买卖，所以不能继续用贪心求累积和的办法。这里改用动态规划。# Comment2: 首先我们明确在第i天，可做的操作有2种：买和卖。对于买，buy[i] = max(buy[i - 1], sell[i - 2] - prices[i])，即要不就是不买，要么就是用两天前卖出后的资金买入新的股票；对于卖，sell[i] = max(sell[i - 1], buy[i - 1] + prices[i])，要么就是不卖，要么就是把一天前买入的再卖出去。第一个式子已经包含了cooldown的逻辑，可见如果没有cooldown，那么直接用sell[i - 1]即可。class Solution: def maxProfit(self, prices): if len(prices) &lt; 2 : return 0 buy, sell, pre_buy, pre_sell = -prices[0], 0, 0, 0 for price in prices: pre_buy = buy buy = max(pre_sell - price, pre_buy) pre_sell = sell sell = max(pre_buy + price, pre_sell) return sell# 下面这一段是用同样的思路解决Case IIclass Solution: def maxProfit(self, prices): if len(prices) &lt; 2 : return 0 buy, sell, pre_buy = -prices[0], 0, 0 for price in prices: pre_buy = buy buy = max(sell - price, pre_buy) sell = max(pre_buy + price, sell) return sell Buy&amp;Sell unlimited time with fees Problem: Your are given an array of integers prices, for which the i-th element is the price of a given stock on day i; and a non-negative integer fee representing a transaction fee. You may complete as many transactions as you like, but you need to pay the transaction fee for each transaction. You may not buy more than 1 share of a stock at a time (ie. you must sell the stock share before you buy again.). Return the maximum profit you can make. Example: Input: prices = [1, 3, 2, 8, 4, 9], fee = 2, output: 8. ((8 - 1) - 2) + ((9 - 4) - 2) = 8. Link: https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-transaction-fee/ Answer:123456789101112131415# Comment: 我们采用与上面一样的思路，只不过每次买入时需要扣除费用。class Solution: def maxProfit(self, prices, fee): if len(prices) &lt; 2 : return 0 buy, sell, pre_buy = -prices[0], 0, 0 for price in prices: pre_buy = buy buy = max(sell - price, pre_buy) sell = max(pre_buy + price - fee, sell) return sell]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 2: path]]></title>
    <url>%2F2019%2F02%2F20%2FLeetcode-2-path%2F</url>
    <content type="text"><![CDATA[根据我个人的总结，Leetcode上Array类问题中，easy主要是解决一维数组的搜索和排序问题，技巧要点是用pointer和hash表来减少loop成本；medium则是上升到了二维数组、矩阵和图的层次，至于技巧还需要慢慢摸索。在这里总结一些典型的路径搜索问题。 Unique Paths Problem: A robot is located at the top-left corner of a m x n grid. The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid. How many possible unique paths are there? Example: m = 3, n = 2, return 3. From the top-left corner, there are a total of 3 ways to reach the bottom-right corner: 1. Right -&gt; Right -&gt; Down; 2. Right -&gt; Down -&gt; Right; 3. Down -&gt; Right -&gt; Right Link: https://leetcode.com/problems/unique-paths/ Answer:12345# Comment: Easiest way is to use mathclass Solution: def uniquePaths(self, m, n): return int(math.factorial(m + n - 2) / math.factorial(m - 1) / math.factorial(n - 1)) Unique Paths II Problem: A robot is located at the top-left corner of a m x n grid. The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid. Now consider if some obstacles are added to the grids. How many unique paths would there be? Example: Input [[0,0,0], [0,1,0], [0,0,0]], then output should be 2. The Explanation is: there is one obstacle in the middle of the 3x3 grid above. There are two ways to reach the bottom-right corner: 1. Right -&gt; Right -&gt; Down -&gt; Down; 2. Down -&gt; Down -&gt; Right -&gt; Right Link: https://leetcode.com/problems/unique-paths-ii/ Answer:123456789101112131415161718192021222324252627282930# Comment: pure math cannot be use here, as the location of obstacles are random.# Fastest idea is recurrence, which is executable but with explosive complexity.# A better solution is dynamic programming.class Solution: def uniquePathsWithObstacles(self, obstacleGrid): m = len(obstacleGrid) n = len(obstacleGrid[0]) obstacleGrid[0][0] = 1 - obstacleGrid[0][0] for i in range(1, n): if not obstacleGrid[0][i]: obstacleGrid[0][i] = obstacleGrid[0][i-1] else: obstacleGrid[0][i] = 0 for i in range(1, m): if not obstacleGrid[i][0]: obstacleGrid[i][0] = obstacleGrid[i-1][0] else: obstacleGrid[i][0] = 0 for i in range(1, m): for j in range(1, n): if not obstacleGrid[i][j]: obstacleGrid[i][j] = obstacleGrid[i][j-1]+obstacleGrid[i-1][j] else: obstacleGrid[i][j] = 0 return obstacleGrid[-1][-1] Minimum Path Sum Problem: Given a m x n grid filled with non-negative numbers, find a path from top left to bottom right which minimizes the sum of all numbers along its path. Note: You can only move either down or right at any point in time. Example: Input [[1,3,1], [1,5,1], [4,2,1]] (In fact, this is an weighted version of Probelm Unique Path), output 7 (1-&gt;3-&gt;1-&gt;1-&gt;1). Link: https://leetcode.com/problems/minimum-path-sum/ Answer:12345678910111213141516171819# Recurrence: possible but low-effiency# Dynamic Programmingclass Solution: def minPathSum(self, grid): m = len(grid) n = len(grid[0]) for i in range(1, m): grid[i][0] += grid[i - 1][0] for i in range(1, n): grid[0][i] += grid[0][i - 1] for i in range(1, m): for j in range(1, n): grid[i][j] += min(grid[i][j - 1], grid[i - 1][j]) return grid[-1][-1] Word Search Problem: Given a 2D board and a word, find if the word exists in the grid. The word can be constructed from letters of sequentially adjacent cell, where “adjacent” cells are those horizontally or vertically neighboring. The same letter cell may not be used more than once. Example: board = [[‘A’,’B’,’C’,’E’], [‘S’,’F’,’C’,’S’], [‘A’,’D’,’E’,’E’]], Given word = “ABCCED”, return true. Given word = “SEE”, return true. Given word = “ABCB”, return false. Link: https://leetcode.com/problems/word-search/ Answer:12345678910111213141516171819202122232425262728293031323334# 优化回溯算法，来自Submission中的最强者，真的很厉害from collections import Counterdirs = [(0, 1), (1, 0), (0, -1), (-1, 0)] # 移动方向class Solution: def exist(self, board, word): m, n = len(board), len(board[0]) bcnts = Counter(c for r in board for c in r) for w, w_cnt in Counter(word).items(): # 先通过个数检测是否不可能存在 if w not in bcnts or w_cnt &gt; bcnts[w]: return False def backtrack(i, j, index): if index == len(word) - 1: # 如果只有一个字母，即为True return True # 标记为已访问 board[i][j] = &apos;*&apos; for dx, dy in dirs: next_i, next_j = i + dx, j + dy # 先判断再进入，减少递归次数 if 0 &lt;= next_i &lt; m and 0 &lt;= next_j &lt; n and word[index + 1] == board[next_i][next_j] and backtrack( next_i, next_j, index + 1): return True board[i][j] = word[index] # 再把标记过的还原 return False for i in range(m): for j in range(n): if board[i][j] == word[0] and backtrack(i, j, 0): return True return False]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Algorithm]]></title>
    <url>%2F2019%2F02%2F02%2FAlgorithm%2F</url>
    <content type="text"><![CDATA[本文是为了2019年春季的Design &amp; Analysis of Algotirhm而撰写，目的是总结课程中学到的算法。 Sorting123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304import timeclass Sort: &quot;&quot;&quot; Class for all kinds of sorting algorithms implemented in Python3 Notes: if maxone - minone in the list &lt; nlogn, we can use counting sort else range k of the nums is &gt;&gt; nlong, we can use radix sort. Both Algos are O(n) if we need to sort for numbers in other number system, like bin or hex, we need to use Radix Sort if nums are very huge and cannot load at a time, we need to use heap sort with forward-method for building heaps &quot;&quot;&quot; def InsertSort(self, nums): &quot;&quot;&quot; :intro: insertion sort :param nums: unsorted nums (list) :return: sorted nums (list) &quot;&quot;&quot; sorted_nums = nums[:1] for num in nums[1:]: for sorted_num_index in range(len(sorted_nums) - 1, -1, -1): if sorted_nums[sorted_num_index] &gt; num: if sorted_num_index == 0: sorted_nums.insert(sorted_num_index, num) else: continue else: sorted_nums.insert(sorted_num_index + 1, num) break return sorted_nums def MergeSort(self, nums): &quot;&quot;&quot; :intro: merge sort :param nums: unsorted nums (list) :return: sorted nums (list) &quot;&quot;&quot; def divide(given_nums): &quot;&quot;&quot; :intro: dimidiate given array :param given_nums: given array (list) :return: two parts of dimidiated array (list) &quot;&quot;&quot; return given_nums[:(len(given_nums) // 2)], given_nums[(len(given_nums) // 2):] def merge(left_nums, right_nums): &quot;&quot;&quot; :intro: merge sorted arrays :param left_nums: one sorted array (list) :param right_nums: another sorted array (list) :return: merged sorted array (list) &quot;&quot;&quot; merged_nums = [] l = r = 0 for i in range(len(left_nums) + len(right_nums)): if left_nums[l] &lt; right_nums[r]: merged_nums.append(left_nums[l]) l += 1 else: merged_nums.append(right_nums[r]) r += 1 if l == len(left_nums): merged_nums += right_nums[r:] break if r == len(right_nums): merged_nums += left_nums[l:] break return merged_nums if len(nums) &gt;= 2: nums_l, nums_r = divide(nums) nums_l = self.MergeSort(nums_l) nums_r = self.MergeSort(nums_r) return merge(nums_l, nums_r) return nums def HeapSort(self, nums): &quot;&quot;&quot; :intro: heap sort # 从小到大排需建立大根堆，从大到小需建立小根堆，或者reverse一下 :param nums: unsorted array :return: sorted array &quot;&quot;&quot; def adjust(nums, start, end): &quot;&quot;&quot; :intro: adjust given heap, which contained in a given nums :param nums: given nums :param start: the startpoint of heap :param end: the endpoint of heap :return: adjusted heap, which contained in the previous nums &quot;&quot;&quot; root = start left = 2 * start + 1 right = 2 * start + 2 if left &lt; end and nums[left] &lt; nums[root]: root = left if right &lt; end and nums[right] &lt; nums[root]: root = right if root != start: nums[start], nums[root] = nums[root], nums[start] adjust(nums, root, end) return nums length = len(nums) for i in range(length // 2 - 1, -1, -1): adjust(nums, i, length) for j in range(length - 1, -1, -1): nums[0], nums[j] = nums[j], nums[0] adjust(nums, 0, j) return nums[::-1] def CountingSort(self, nums): &quot;&quot;&quot; :intro: Counting sort for n integers, where value range k of n &lt; nlogn :param nums: unsorted array :return: sorted array &quot;&quot;&quot; minone = min(nums) gap = 1 - minone # 把负数填充成大于等于1的正数 maxone = max(nums) sorted_nums = [0] * len(nums) countings = [0] * (maxone - minone + 1) for num in nums: countings[num + gap - 1] += 1 summings = [sum(countings[:c + 1]) for c in range(len(countings))] for num in nums[::-1]: sorted_nums[summings[num + gap - 1] - 1] = num summings[num + gap - 1] -= 1 return sorted_nums def RadixSort(self, nums): &quot;&quot;&quot; :intro: Radix sort for n integers, where value range k of n can be far bigger than nlogn :param nums: unsorted array :return: sorted array &quot;&quot;&quot; # Radix Sort必须使用stable的算法来进行每一个数位上的排序，同时数位上数的范围最多是0～9，远小于n，所以最好用Counting Sort # 这里建立一个特殊的counting sort，是为了返回原list中元素的index，方便定位 def CountingSort_withindex(nums): &quot;&quot;&quot; :intro: Counting sort and kept original indexes :param nums: unsorted array :return: sorted array &quot;&quot;&quot; minone = min(nums, key=lambda x: x[1])[1] gap = 1 - minone # 把负数填充成大于等于1的正数 maxone = max(nums, key=lambda x: x[1])[1] sorted_nums = [0] * len(nums) countings = [0] * (maxone - minone + 1) for num in nums: countings[num[1] + gap - 1] += 1 summings = [sum(countings[:c + 1]) for c in range(len(countings))] for num in nums[::-1]: sorted_nums[summings[num[1] + gap - 1] - 1] = (num[0], num[1]) summings[num[1] + gap - 1] -= 1 return sorted_nums minone = min(nums) gap = -minone # 把负数填充成大于等于0的数 nums = [num + gap for num in nums] length = len(str(max(nums))) l = 0 digits = [(n, nums[n] // (10 ** l) % 10) for n in range(len(nums))] while l &lt; length: c_digits = CountingSort_withindex(digits) l += 1 digits = [(i, nums[i] // (10 ** l) % 10) for i, d in c_digits] return [nums[i] - gap for i, d in digits] def QuickSort(self, nums): &quot;&quot;&quot; :intro: Quick Sort for n numbers :param nums: unsorted array :return: sorted array &quot;&quot;&quot; if len(nums) &lt;= 1: return nums else: pivot = nums.pop() s1, s2 = [], [] for num in nums: if num &lt; pivot: s1.append(num) else: s2.append(num) return self.QuickSort(s1) + [pivot] + self.QuickSort(s2) def QuickSort_inplace_1(self, nums, p, r): &quot;&quot;&quot; :intro: Quick Sort for n numbers (in place version 1: keep move smaller elements front of the pivot) :param nums: unsorted array :param p: start position of sorting any part of the whole array :param r: end position of sorting any part of the whole array :return: sorted array &quot;&quot;&quot; def Partition(nums, p, r): &quot;&quot;&quot; :intro: Partition for given array, by keeping move smaller elements front of the pivot :param nums: any array :param p: start position of targeted array waiting to be partitioned :param r: end position of targeted array waiting to be partitioned :return: elements in range(p, r) will be partitioned by pivot with smaller before the pivot and larger after &quot;&quot;&quot; pivot = nums[p] pivot_index = p for i in range(p + 1, r + 1): current = nums[i] if current &lt; pivot: nums.pop(i) nums.insert(pivot_index, current) pivot_index += 1 return pivot_index if p &lt; r: q = Partition(nums, p, r) self.QuickSort_inplace_1(nums, p, q - 1) self.QuickSort_inplace_1(nums, q + 1, r) def QuickSort_inplace_2(self, nums, p, r): &quot;&quot;&quot; :intro: Quick Sort for n numbers (in place version 2: two pointers searching) :param nums: unsorted array :param p: start position of sorting any part of the whole array :param r: end position of sorting any part of the whole array :return: sorted array &quot;&quot;&quot; def Partition(nums, p, r): &quot;&quot;&quot; :intro: Partition for given array, by using two pointers to track adjusting pairs and swap :param nums: any array :param p: start position of targeted array waiting to be partitioned :param r: end position of targeted array waiting to be partitioned :return: elements in range(p, r) will be partitioned by pivot with smaller before the pivot and larger after &quot;&quot;&quot; pivot = nums[p] while p &lt; r: while p &lt; r and nums[r] &gt;= pivot: r -= 1 nums[p] = nums[r] while p &lt; r and nums[p] &lt;= pivot: p += 1 nums[r] = nums[p] nums[p] = pivot return p if p &lt; r: q = Partition(nums, p, r) self.QuickSort_inplace_2(nums, p, q - 1) self.QuickSort_inplace_2(nums, q + 1, r)sorting = Sort()unsorted_nums = [1, 22, 1, 1, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99, -3, -10, 0]start = time.time()print(sorting.InsertSort(unsorted_nums))stop1 = time.time()print((stop1 - start) * 1000)print(sorting.MergeSort(unsorted_nums))stop2 = time.time()print((stop2 - stop1) * 1000)# Heap sort is an in-place operation, so we need to rebuild the unsorted array belowprint(sorting.HeapSort(unsorted_nums))stop3 = time.time()print((stop3 - stop2) * 1000)unsorted_nums = [1, 22, 1, 1, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99, -3, -10, 0]print(sorting.CountingSort(unsorted_nums))stop4 = time.time()print((stop4 - stop3) * 1000)print(sorting.RadixSort(unsorted_nums))stop5 = time.time()print((stop5 - stop4) * 1000)# The extra space quick sort will delete the tail of original array, so we need to rebuild the unsorted array belowprint(sorting.QuickSort(unsorted_nums))stop6 = time.time()print((stop6 - stop5) * 1000)# The in-place quick sort do in-place operation, so we need to rebuild the unsorted array belowunsorted_nums = [1, 22, 1, 1, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99, -3, -10, 0]sorting.QuickSort_inplace_1(unsorted_nums, 0, len(unsorted_nums) - 1)print(unsorted_nums)stop7 = time.time()print((stop7 - stop6) * 1000)# The in-place quick sort do in-place operation, so we need to rebuild the unsorted array belowunsorted_nums = [1, 22, 1, 1, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99, -3, -10, 0]sorting.QuickSort_inplace_2(unsorted_nums, 0, len(unsorted_nums) - 1)print(unsorted_nums)stop8 = time.time()print((stop8 - stop7) * 1000)unsorted_nums = [1, 22, 1, 1, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99, -3, -10, 0]# Wait for next sort... Search123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import timeclass Search: &quot;&quot;&quot; Class for all searching algorithms implemented in Python 3 &quot;&quot;&quot; def BinarySearch(self, sorted_nums, start, end, target): &quot;&quot;&quot; :intro: binary search for a given target in a sorted array :param sorted_nums: sorted array :param start: start index :param end: end index :param target: given number :return: index of given number &quot;&quot;&quot; if start &gt; end: return &quot;No such target&quot; mid = (start + end) // 2 if target == sorted_nums[mid]: return mid elif target &lt; sorted_nums[mid]: return self.BinarySearch(sorted_nums, start, mid - 1, target) elif target &gt; sorted_nums[mid]: return self.BinarySearch(sorted_nums, mid + 1, end, target) def Search2nd(self, unsorted_nums): &quot;&quot;&quot; :intro: find 2nd biggest number in an unsorted array :param unsorted_nums: unsorted array :return: 2nd biggest number &quot;&quot;&quot; mmmone = unsorted_nums[-1] maxone = ma2one = 0 n = len(unsorted_nums) kdict = &#123;&#125; while n &gt; 1: next_nums = [] for i in range(n // 2): maxone = max(unsorted_nums[2 * i], unsorted_nums[2 * i + 1]) minone = min(unsorted_nums[2 * i], unsorted_nums[2 * i + 1]) next_nums.append(maxone) if maxone in kdict: kdict[maxone].append(minone) else: kdict[maxone] = [minone] unsorted_nums = next_nums n = len(unsorted_nums) for c in kdict[maxone]: ma2one = max(ma2one, c) if len(unsorted_nums) % 2 == 0: pass else: if mmmone &gt; maxone: return maxone elif mmmone &gt; ma2one: return mmmone else: pass return ma2one def SearchK_deterministic(self, unsorted_nums, k): &quot;&quot;&quot; :intro: find k-th smallest number in an unsorted and distinct array via deterministic pivot :param unsorted_nums: unsorted array :param k: k-th smallest index :return: k-th smallest number &quot;&quot;&quot; group = 5 length = len(unsorted_nums) // group ms = [sorted(unsorted_nums[group * i: group * i + group], reverse=True) for i in range(length)] # 按五个分为一组 ms.append(sorted(unsorted_nums[group * length:], reverse=True)) # 最后一组可以小于5 mm = [s[len(s) // 2] for s in ms if s != []] # 每组的中位数 if len(mm) &gt; 1: # 中位数的中位数作为m，递归实现 m = self.SearchK_deterministic(mm, len(mm) // 2) else: m = mm[0] s1 = [] s2 = [] for num in unsorted_nums: if num &lt; m: s1.append(num) elif num &gt; m: s2.append(num) length = len(s1) if length + 1 == k: return m elif length &gt;= k: return self.SearchK_deterministic(s1, k) else: return self.SearchK_deterministic(s2, k - length - 1) def SearchK_randomized(self, unsorted_nums, k): &quot;&quot;&quot; :intro: find k-th smallest number in an unsorted and distinct array via randomly pivot :param unsorted_nums: unsorted array :param k: k-th smallest index :return: k-th smallest number &quot;&quot;&quot; from random import sample r = sample(range(len(unsorted_nums)), 1)[0] # 随机选一个数 s1 = [] s2 = [] for num in unsorted_nums: if num &lt; unsorted_nums[r]: s1.append(num) elif num &gt; unsorted_nums[r]: s2.append(num) length = len(s1) if length + 1 == k: return unsorted_nums[r] elif length &gt;= k: return self.SearchK_randomized(s1, k) else: return self.SearchK_randomized(s2, k - length - 1)search = Search()sorted_nums = [-10, -10, -3, -3, -2, 0, 0, 1, 1, 1, 2, 4, 5, 7, 19, 22, 99]unsorted_nums = [1, 22, 1, 1, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99, -3, -10, 0]unsorted_distinct_nums = [1, 22, 2, -3, 5, -10, -2, 0, 7, 19, 4, 99]start = time.time()print(search.BinarySearch(sorted_nums, 0, len(sorted_nums) - 1, 5))stop1 = time.time()print((stop1 - start) * 1000)print(search.Search2nd(unsorted_nums))stop2 = time.time()print((stop2 - stop1) * 1000)print(search.SearchK_deterministic(unsorted_distinct_nums, 10))stop3 = time.time()print((stop3 - stop2) * 1000)print(search.SearchK_randomized(unsorted_distinct_nums, 10))stop4 = time.time()print((stop4 - stop3) * 1000) Binary Search Tree12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394# 代码来自：https://blog.csdn.net/u010089444/article/details/70854510class Node: def __init__(self, data): self.data = data self.lchild = None self.rchild = Noneclass BST: def __init__(self, node_list): self.root = Node(node_list[0]) for data in node_list[1:]: self.insert(data) def search(self, node, parent, data): if node is None: return False, node, parent if node.data == data: return True, node, parent if node.data &gt; data: return self.search(node.lchild, node, data) else: return self.search(node.rchild, node, data) def insert(self, data): flag, n, p = self.search(self.root, self.root, data) if not flag: new_node = Node(data) if data &gt; p.data: p.rchild = new_node else: p.lchild = new_node def delete(self, data): flag, n, p = self.search(self.root, self.root, data) if flag is False: print(&quot;无该关键字，删除失败&quot;) else: if n.lchild is None: # 如果右子树为空，直接往上继承 if n == p.lchild: p.lchild = n.rchild else: p.rchild = n.rchild del n elif n.rchild is None: # 如果左子树为空，直接往上继承 if n == p.lchild: p.lchild = n.lchild else: p.rchild = n.lchild del n else: # 左右子树均不为空 pre = n.rchild if pre.lchild is None: n.data = pre.data n.rchild = pre.rchild del pre else: next = pre.lchild # 往右一个节点，然后一直找到最左下角 while next.lchild is not None: pre = next next = next.lchild n.data = next.data pre.lchild = next.rchild # 把next拿走后，因为next必定没有左孩子，故直接让上一层的左孩子继承next的右子树 del next def preOrderTraverse(self, node): if node is not None: print(node.data, end=&quot;, &quot;) self.preOrderTraverse(node.lchild) self.preOrderTraverse(node.rchild) return &quot;End&quot; def inOrderTraverse(self, node): if node is not None: self.inOrderTraverse(node.lchild) print(node.data, end=&quot;, &quot;) self.inOrderTraverse(node.rchild) return &quot;End&quot; def postOrderTraverse(self, node): if node is not None: self.postOrderTraverse(node.lchild) self.postOrderTraverse(node.rchild) print(node.data, end=&quot;, &quot;) return &quot;End&quot;a = [3, 1, 8, 4, 6, 5, 7]bst = BST(a) # 创建二叉查找树print(bst.inOrderTraverse(bst.root)) # 中序遍历，等于排序bst.delete(3)print(bst.inOrderTraverse(bst.root)) # 中序遍历，等于排序]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 1: sum]]></title>
    <url>%2F2019%2F01%2F15%2FLeetcode-1-sum%2F</url>
    <content type="text"><![CDATA[整理了一部分Leetcode上关于两个及两个以上数求和的问题 Two Sum Problem: Given an array of integers, return indices of the two numbers such that they add up to a specific target. Example: Given nums = [2, 7, 11, 15], target = 9. Return [0, 1] Link: https://leetcode.com/problems/two-sum/ Answer: 123456789101112# Comments: time complexity of searching in dict is O(1), since dict in python is organized by hashclass Solution(object): def twoSum(self, nums, target): if len(nums) &lt;= 1: return False buff_dict = &#123;&#125; for i in range(len(nums)): if nums[i] in buff_dict: return [buff_dict[nums[i]], i] else: buff_dict[target - nums[i]] = i Two Sum, Input array is sorted Problem: Given an array of integers that is already sorted in ascending order, find two numbers such that they add up to a specific target number. The function twoSum should return indices of the two numbers such that they add up to the target, where index1 must be less than index2. Example: Given numbers = [2,7,11,15], target = 9. Return [1,2] Link: https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/ Answer 1: 12345678910111213141516class Solution: # 两个指针往中间移动，较快 def twoSum(self, numbers, target): &quot;&quot;&quot; :type numbers: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; l, r = 0, len(numbers) - 1 while l &lt; r: s = numbers[l] + numbers[r] if s == target: # 如果等于，直接返回 return [l + 1, r + 1] elif s &lt; target: # 如果小于，左边指针右移 l += 1 else: # 如果大于，右边指针左移 r -= 1 Answer 2: 123456789101112class Solution: # 哈希表，最快 def twoSum(self, numbers, target): &quot;&quot;&quot; :type numbers: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; dic = &#123;&#125; for i, num in enumerate(numbers): # = for i in range(len(numbers)) if target - num in dic: return [dic[target - num] + 1, i + 1] dic[num] = i Three Sum Problem: Given an array nums of n integers, are there elements a, b, c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. The solution set must not contain duplicate triplets. Example: Given array nums = [-1, 0, 1, 2, -1, -4], return [[-1, 0, 1], [-1, -1, 2]] Link: https://leetcode.com/problems/3sum/ Answer, copied from here:12345678910111213141516171819202122# Comment 1: 最容易想到的是基于Two Sum的两重循环加hash，但要想到先排序是比较难的class Solution(object): def threeSum(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; if len(nums) &lt; 3: return [] nums.sort() res = set() for i, v in enumerate(nums[:-2]): if i &gt;= 1 and v == nums[i-1]: continue d = &#123;&#125; for x in nums[i+1:]: if x not in d: d[-v-x] = 1 else: res.add((v, -v-x, x)) return list(map(list, res)) Three Sum Closest Problem: Given an array nums of n integers and an integer target, find three integers in nums such that the sum is closest to target. Return the sum of the three integers. Example: Given array nums = [-1, 2, 1, -4], and target = 1. The sum that is closest to the target is 2. (-1 + 2 + 1 = 2). Link: https://leetcode.com/problems/3sum-closest/ Answer:12345678910111213141516171819202122232425262728293031323334353637# Comment 1: can&apos;t find exact source# Comment 2: based on Three Sum, with two pointersclass Solution: def threeSumClosest(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: int &quot;&quot;&quot; if len(nums) &lt; 3: return [] nums.sort() closest = None closest_distance = float(&apos;inf&apos;) for i in range(len(nums[:-2])): if i &gt; 0 and nums[i] == nums[i-1]: continue l, r = i + 1, len(nums) - 1 while l &lt; r: current_sum = nums[i] + nums[l] + nums[r] if current_sum &lt; target: l += 1 elif current_sum &gt; target: r -= 1 elif current_sum == target: return target current_distance = abs(current_sum - target) if current_distance &lt; closest_distance: closest = current_sum closest_distance = current_distance return closest Four Sum Problem: Given an array nums of n integers and an integer target, are there elements a, b, c, and d in nums such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target. The solution set must not contain duplicate quadruplets. Example: Given array nums = [1, 0, -1, 0, -2, 2], and target = 0. A solution set is: [[-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]] Link: https://leetcode.com/problems/4sum/ Answer:12345678910111213141516171819202122232425262728293031# Comment 1: can&apos;t find exact source# Comment 2: 我写了一个基于Three Sum + 一重循环的，能提交，但毕竟还是多了一个量级# Comment 3: 另一种思路是把Four Sum当成两个Two Sum，但并不是迭代。可以先两两计算所有可能的值，然后匹配成4元组，即如下class Solution: def fourSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[List[int]] &quot;&quot;&quot; if not nums or len(nums) &lt; 4: return [] nums.sort() sum_to_ix = &#123;&#125; ans = set() m = len(nums) for i in range(m): for j in range(i+1,m): two_sum = nums[i] + nums[j] key_list = sum_to_ix.keys() if two_sum not in key_list: sum_to_ix[two_sum] = [[i,j]] else: sum_to_ix[two_sum].append([i,j]) if target-two_sum in key_list: for idx in sum_to_ix[target-two_sum]: if i not in idx and j not in idx: ans.add(tuple(sorted([nums[idx[0]],nums[idx[1]],nums[i],nums[j]]))) return list(ans) Combination Sum Problem: Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target. The same repeated number may be chosen from candidates unlimited number of times. The solution set must not contain duplicate combinations. Example: Given candidates = [2,3,6,7], target = 7. A solution set is: [[7], [2,2,3]]. Link: https://leetcode.com/problems/combination-sum/ Answer, copied from here 123456789101112131415161718class Solution(object): def combinationSum(self, candidates, target): solution = [] def dfs(running, running_sum, ind): for i in range(ind, len(candidates)): next_sum = running_sum + candidates[i] if next_sum &lt; target: dfs(running + [candidates[i]], next_sum, i) elif next_sum == target: solution.append(running + [candidates[i]]) else: continue dfs([], 0, 0) return solution Appendix: 补充一段Python实现BFS和DFS的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748graph = &#123; &quot;A&quot;: [&quot;B&quot;, &quot;C&quot;], &quot;B&quot;: [&quot;A&quot;, &quot;C&quot;, &quot;D&quot;], &quot;C&quot;: [&quot;A&quot;, &quot;B&quot;, &quot;D&quot;, &quot;E&quot;], &quot;D&quot;: [&quot;B&quot;, &quot;C&quot;, &quot;E&quot;, &quot;F&quot;], &quot;E&quot;: [&quot;C&quot;, &quot;D&quot;], &quot;F&quot;: [&quot;D&quot;]&#125;def BFS(graph, s): &quot;&quot;&quot; :param graph: graph :param s: startpoint :return: BFS path &quot;&quot;&quot; queue = [] queue.append(s) seen = set() seen.add(s) while len(queue) &gt; 0: vertex = queue.pop(0) nodes = graph[vertex] for w in nodes: if w not in seen: queue.append(w) seen.add(w) print(vertex)def DFS(graph, s): &quot;&quot;&quot; :param graph: graph :param s: startpoint :return: DFS path &quot;&quot;&quot; stack = [] stack.append(s) seen = set() seen.add(s) while len(stack) &gt; 0: vertex = stack.pop() nodes = graph[vertex] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(vertex) Subarray Sum Equals K Problem: Given an array of integers and an integer k, you need to find the total number of continuous subarrays whose sum equals to k. Example: Input nums = [1,1,1] and k = 2, output 2. Link: https://leetcode.com/problems/subarray-sum-equals-k/ Answer:123456789101112131415# Comment: 从0到j的累积和-从0到i的累积和=k，则i到j的和为k --》转化为Two Sumclass Solution: def subarraySum(self, nums, k): count = 0 if not nums: return count mapping = &#123;0: 1&#125; cur_sum = 0 for i in range(len(nums)): cur_sum += nums[i] if cur_sum - k in mapping: count += mapping[cur_sum - k] mapping[cur_sum] = mapping.get(cur_sum, 0) + 1 return count Valid Triangle Number Problem: Given an array consists of non-negative integers, your task is to count the number of triplets chosen from the array that can make triangles if we take them as side lengths of a triangle. Example: Input nums = [2, 2, 3, 4], output 3 (223, 234, 234). Link: https://leetcode.com/problems/valid-triangle-number/ Answer12345678910111213141516# Comment: 3指针法。先排序，然后一个指针定位长边，两个指针（称为左右指针）在该指针的左侧搜寻可能的组合。左指针从index=0处出发，右指针从index=i-1处出发。当两短边相加大于长边时，计数right - left（中间所有元素与right的和势必都可以大于长边）同时右指针左移一位来使短边和下降；反之，左指针右移一位使短边和上升class Solution: def triangleNumber(self, nums: List[int]) -&gt; int: nums.sort() count = 0 for i in range(2, len(nums)): left = 0 right = i - 1 while left &lt; right: if nums[left] + nums[right] &gt; nums[i]: count += right - left right -= 1 else: left += 1 return count Subarray Product Less Than K Problem: Your are given an array of positive integers nums. Count and print the number of (contiguous) subarrays where the product of all the elements in the subarray is less than k. Example: Input: nums = [10, 5, 2, 6], k = 100; output 8. Link: https://leetcode.com/problems/subarray-product-less-than-k/ Answer:1234567891011121314151617# Comment1: 这类找子串的问题，最方便的就是用指针设置滑动窗口。部分子序列问题，也可以先排序，然后再转化为子串问题# Comment2: 2指针法。用O(1)空间记录当前的积，右指针从index=0处不断右移，积不断变大。当积大于目标值时，左指针从index=0处右移，积不断变小。计数就等于左右指针间距离之和。需要注意，由于nums全是正数，所以不存在任何组合积&lt;=1class Solution: def numSubarrayProductLessThanK(self, nums, k): if k &lt;= 1: return 0 curr_prod = 1 l = 0 count = 0 for r, num in enumerate(nums): curr_prod *= num while curr_prod &gt;= k: curr_prod //= nums[l] l += 1 count += r - l + 1 return count]]></content>
      <tags>
        <tag>Leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Does money make people happy?]]></title>
    <url>%2F2019%2F01%2F11%2FDoes-money-make-people-happy%2F</url>
    <content type="text"><![CDATA[秋季学期的Applied Data Science课程要求做一个和城市数据有关的期末项目。我们小组设计了一个非常有哲学含义的项目：Does money make people happy? 思考 这是一个立意比较高、实践难度比较大的项目 为什么这么说？首先我们把这个命题翻译成机器学习用语就是：以能够描述金钱的变量作为特征，以能够描述人们心里高兴程度的变量为目标值，通过相关性分析或机器学习模型，能否发现这些特征与目标值之间的一定关系？ 那么问题来了，什么变量可以描述金钱？什么变量可以描述人们心里的高兴程度呢？ 思路 特征值：我们认为包含在人口普查数据中的一系列经济社会数据可以描述金钱，或者说可支配收入水平。此外，我们也思考是否可以扩大“金钱”这个词的内涵，比如如果一个人住在地段比较好、房价比较贵的地方，周边服务设施多，那么可能ta的心情就会比较好，又比如一个人住的房子很大、很新，那么可能ta的心情就会更好。因此结合数据可获得的难易程度，我们选取了两组数据作为特征。一是全部540+的美国人口普查经济指标，其中包含每个统计区的平均收入、平均寿命预期等等可能的因变量；二是每个统计区的中位数收入、平均房价和地价。这里的统计区是以邮编划分的。 目标值：自然语言处理里有一门技术叫情感分析，主要思想是利用基于大规模统计的情感词典来量化文本所体现的情绪。比如对于“今天天气真好！”这样一句话，可能量化为(正面：0.9，中性：0.2，负面：-0.1)的情感得分。至此，我们只需要寻找能够反映人们情绪的载体–大规模文本即可。那么什么是最接近的呢？自然是每天产生大量内容的社交媒体。 数据 ACS：基于邮编的纽约市人口普查经济指标 PLUTO：纽约市房价地价数据。值得感慨的是美国人把这个数据开源到了每栋房屋的级别，坐标、用地类型和价格信息都很详细，数据建设是真的好。考虑到居住人口比移动人口有更好的参考价值，我们根据用地类型筛选出了居住用地，计算了每个邮编区域的平均地价和房价。此外，从ACS数据中选取了最有代表性的中位数收入数据补充到这一组特征中 情绪：我们使用Twitter API收集了纽约市内从2018年10月24日20点到2018年11月26日19点的带有打卡信息的42万tweets，其中打卡信息就是发推地点的准确经纬度坐标。之后，我们筛选了发送时间在早8点之前和晚7点之后的数据–因为它们更可能来自居住地。紧接着，我们对全部文本进行自然语言处理，包括分词、停用词处理、词形还原和情感分量化等过程。量化时借鉴了两部词典，分别是英文词情感词典和emoji情感词典。至于我们为什么要特别处理emojis，是因为在某些情况下表情符号可以传达文字无法传达的情绪，请看下图： 实验 基于以上数据，我们做了相关性检测、回归和分类算法的测试 试验结果、代码和非常有趣的诸多GIS图像，请查看本项目 感想 第一次自己设计一个有意思的社会学命题，并尝试用数据科学和机器学习的方法去解决问题 存在很多问题，特别是人口普查的样本和推特样本严重不一致 但还是很自豪，并希望继续努力 放一张纽约市积极情绪图做一纪念：]]></content>
      <tags>
        <tag>Data mining</tag>
        <tag>Sentiment analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SMP CUP 2018]]></title>
    <url>%2F2018%2F07%2F12%2FSMP-CUP-2018%2F</url>
    <content type="text"><![CDATA[时间太快啦，转眼就一年了，又到了SMP CUP 2018的时候。今年比赛和去年略有不同，去年是一份数据三个任务，今年则是一个任务独享一份数据。任务一文本分类，任务三文本溯源。 比赛简介 任务一文本分类：给定一篇短文，判断是人类作者、自动摘要、机器翻译和机器作者中的哪一种，详见官网 任务三文本溯源：给定一个句子，判断是否复制或改编(删减/摘要/替换/重排等)于其它的句子，详见官网 数据样例 任务一：ID12345 ||| 方法通过文献研究、药物实测、炮制方法、方药配伍、煎服方法、安全性及临床用药特点等方面考证《伤寒论》药物剂量。 任务三：自动摘要 ||| 自动摘要生成的文章，从新闻网站爬取后调用各种自动摘要工具生成，共60,000篇。 思路总结 任务一：基础文本分类模型如TextCNN、TextRNN和Fasttext等分别训练，然后模型融合。 任务三：句子相似度计算，然后对召回句子进行优化排序。这里要注意的是，任务三中没有任何的标签，因此是一个无监督的候选句子排序任务。考虑到句子间循环匹配的数量级，我搭建了ElasticSearch全文检索引擎来帮助初选优化，大大降低了检索成本。 模型实现 我在github上的开源代码 比赛心得 任务一： 1.基础模型没能做到极致，仅仅尝试了TextCNN和Fasttext； 2.计算资源要充足，没能把TextRNN做起来的原因有一部分就是实验室的服务器没有GPU…； 3.时间要安排好，工作要分配好，临近毕业实在是太忙了… 任务三： 1.想到用ES来加速查询真的是个非常正确的决定，因为测试集计算量是验证集的200倍; 2.最后实现的还是句子匹配，而非文本理解和溯源。没想到很好的方法。 总的来说，写代码比去年顺手多了，也规范很多。比赛套路也熟悉了很多，从容了一些。但是真的要到进一步提升效果，冲刺排名的时候，就会因为平时积累不够而有心无力。比如说模型融合应该是要基于差异性大的底层模型，这一点我也是在整个比赛结束时才深有体会。再比如Attention和LSTM相关的网络都没能写完、运算完，师哥说的Fasttext能有0.976的结果也没有实现，这些都是因为平时没有去实现过。所以未来还是要好好加油啊！]]></content>
      <tags>
        <tag>Data Mining Contest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP Internship in Ideepwise Infotech]]></title>
    <url>%2F2018%2F06%2F13%2FNLP-Internship-in-Ideepwise-Infotech%2F</url>
    <content type="text"><![CDATA[2018年3月-2018年6月，我在深思考自然语言处理组担任算法实习工程师 求职面试 是人生中的第二份面试啦，一定程度上算是“老司机”了。 面试分成笔试、上机、技术一面、技术二面、技术三面和HR面。 笔试 有一道概率题，贝叶斯公式带入算一下就可以。 有一道算法题，考的是排序优化，我做出来了，但是方法并不是最好的。 比较奇怪的是，其它几乎都是概念和问答题，机器学习与自然语言处理相关，都不是特别难。 上机 这个算是比较有特色的环节了。 现场要求做一个上机题，关于字符串匹配的，就是要求比较复杂，比如去掉含A的、保留含A但含B的、保留含A但含C且长度小于5的等等，难度不大。不过上机环境是台式机，我不太习惯，还好我习惯去面试带着自己的电脑(首要目的是问到以前工作可以展示代码或界面)。 技术面 一面基本上围绕笔试题，这一次由于有第一次实习的基础，加上我寒假刷了一些leetcode，所以做得还行(其实主要是这次笔试简单了)。 二面主要是白板写算法。印象最深的是问了一个位运算的题：把一个十进制数转换成二进制以后，计算这个二进制数中有多少个1。我用的是最简单的短除法，实现的时候每次减去2的幂就可以了。相比之前的面试，我的进步在于可以白板直接写出来程序了，而缺点还是没有给出最优的方案。算法永无止境，我还需要努力。 三面问了一些实际开发的内容，主要就介绍了SMP CUP 2017(主要是这家公司也参加过这个比赛，虽然不是同一个Track的任务)，以及上一份实习构建问答系统的工作。这里可以插一句，我带电脑真的有用，直接展示了问答系统的前端demo，说服力就很强了(不过很尴尬的是，调用在线搜索后，百度居然直接给了我一个带有色情信息的回答…当时我的脸和面试官一起绿了…还好我机智地说这是即将改进的部分…)。 工作 2018.3：自动问答机器人，主要是套用之前实习开发的系统框架+新公司数据+针对性调整。说实话我觉得这个新公司招我进来重要的原因就是偷学技术…所以在这个过程中我特别注意了数据和参数脱敏的问题，保证保密性(不能违背保密合同)。 2018.4：百度机器阅读理解比赛。尽管我在比赛中实现的实际成果不多，但这份工作经历对我来说比较重要，因为我第一次比较系统地去深入调研和理解了自然语言处理中的某一任务专题，以至于我最终的毕业设计也聚焦在机器阅读理解这项任务上。相关代码和工程还在完善中，之后应该也会开源到我的GitHub上。 2018.5：孪生网络句对相似度计算模型，这份工作来的有点突然，毕竟我本来还是在专心研究机器阅读理解的…目前成果一般，后续可能还会有深入研究，不过它也促进了我对目前自然语言处理领域工作的理解。 感想自然语言处理的核心工作是什么？ 在应用研究层面，应当是文本建模。文本建模意味着什么？意味着通过文本数字化，机器“读懂”了文本。只有理解了文本，机器才能开展文本分类、生成、问答、翻译和阅读理解等工作；只有数字化后的文本能更全面、充分和尽可能少损失地被表示，机器的“智能”才能提升，才更可能拟人。当然，除此以外，能对自然语言处理的研究产生巨大影响的还有文本向量化、模型网络类脑化等工作。只不过它们都不太好上手，文本建模应当是普及最广、被研究最多和进展最快的一类研究。 就我个人观察，文本建模已经从加权tfidf、双向lstm和cnn等方法逐渐往自注意力机制和优化卷积等方法变迁，其中引领潮流的当然是Google著名的《Attention is all your need》一文。在机器翻译和机器阅读理解领域，代表性的新新模型当属Transformer和QANet。 工程和算法的区别？ 在百分点，我更多的是从基础做起，做工程、做产品。五个月里，我锻炼了写代码的能力、熟悉了团队工作环境，知道怎么用wiki、git、filezilla和linux等等一系列东西，可以说是从0到1。 在深思考，我则尝试着去思考自己想做什么，在一定程度突破了代码障碍的基础上，开始学会读论文、追前沿和做模型。算法相比工程最大的区别，就是不必再因为客户的具体需求而去调试参数和设定人工规则，而可以专注于模型本身的设计，去思考学术研究上的优化。我觉得我被这种开创性的研究工作深深吸引着。 换实习要不要“裸辞”？ 原则上不要，因为裸辞会产生空窗期。在空窗期，公司一般不在意我们在学校实验室里做了什么，甚至会认为只是在休息(也就意味着代码实践能力在下滑)。 不足 实习时间太短，没能做更多研究。 公司加班太多(虽然我几乎都没有参与)，我认为被动加班太多只会引起效率下降，劳逸结合、平衡工作和休息才是正道。]]></content>
      <tags>
        <tag>Internship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Postgraduate Application]]></title>
    <url>%2F2018%2F05%2F31%2FPostgraduate-Application%2F</url>
    <content type="text"><![CDATA[2017年10月-2018年6月，伴随着长达8个月的实习，我度过了漫长而揪心的研究生申请季。我的背景是双非一本信管，GPA90，T101，G160+170+3.5。我申请的方向是CS/DS，最后结果是申11中3，一所平调，一所降调带奖，一所带奖。我是全程自己DIY，没有找中介。 准备学历背景 如果还有的选，一定去一所综合性211/985，尽可能不要去那种“分数并不低于985，所以学校也不差于985”的院校(比如我…)，或者那种“小规模精英办学”的院校(又比如我…)。原因在于：1.外国录取委员会更相信中国政府给出的大学档次排名，而不相信我们自己在文书中对学校品质的努力解释；2.小平台=小资源=小校友圈子=少出国人数=少在录取委员会眼前曝光，同时小资源=少有分量推荐信，小资源=少机会提升自己的软实力。 学历背景是很硬的基础。英国规定的最明确，211/985的同学录取GPA要求比双非一本低5分；美国则是隐性要求，不过这一要求没有那么5分那么多，但3-4分左右是要的。 在这一点，对于我个人，我的办法是去读了北大经双(北京高校同学都可以考)。虽然因为我申请的方向和经济/金融关系很小，帮助不大，但是在我找实习、认识更多学校的优秀同龄人、找推荐信等方面都起过或多或少的作用。 语言考试 语言考试指的是托福、雅思、GRE和GMAT等。我考的是托福和GRE。 托福：我从大二开始断断续续考托福，直到大三暑假才考完，前前后后加起来大半年。因为自己的英语基础确实不好，再加上没有很大块的时间可以专心备考，所以战线只能拉的很长。托福我的总结就是多练，刷阅读刷听力刷口语刷写作。以我的经验，从85到101分大概需要100小时全神贯注的学习(一般来说自习一整天=2小时全神贯注)。 GRE：GRE我确实很占便宜，因为我的强项恰好是阅读。所以尽管GRE我也是断断续续准备，但是因为只需要准备单词部分，就比较轻松。我的GRE是在大三下及八月准备的(中间穿插考了一次托福)，阅读从155进步到160。我对GRE的认识是，这门考试考的是短时间大量练习的消化能力，考前反复背10天单词做50篇阅读和10篇作文，考的时候效果很显著。 GPA 我的GPA其实还不错，虽然用我们学校的绩点计算只有3.65。 网上有很多计算绩点的方法，众说纷纭。其实最好的办法就是直接报百分制GPA，这种制度全世界认可，并且还能用99/98来挽救一下非线性下降的绩点。 还有一种办法是把成绩送去WES认证，这个机构出具的成绩报告是比较有效力的，并且对于中国学生一般会使绩点提高一些。但它认证时间较长且贵(几千RMB)，请自行慎重决定。 推荐信 推荐信不太重要，因为你很难让它很重要。如果能找到蜚声国际的人写推荐信，那么推荐信几乎等于保送。比如我校曾有师哥拿到海牙国际法庭大法官推荐信，直接去了哈佛法学院。 对于一般人来说，推荐信都是学校教授和实习公司领导写。建议学校教授找国际知名度高一些的和对自己非常了解的(上过很多门课的)，实习公司领导则要尽可能去大名气的公司找高级别领导。 中国人的推荐信可以说是国际默认自己写的，这里提示一些推荐信要点：1.扫描一张带有机构title的纸作为pdf文件的背景；2.要一个推荐人的电子签名；3.推荐信不宜过长，要有详有略，末尾写上推荐日期和电子签名；4.和推荐人协商好，把含有所有学校的推荐信提交链接的邮件及时转发给自己；5.提交推荐信的时候可以开VPN来避免ip查重。 这里可以延伸说一下实习，我犯的错误是太忽视公司名气。确实，在公司做了什么项目、学到了什么最重要，但是公司名气同样重要，因为很多录取委员会的人只听说过国际大公司，一个小公司的核心成员的名头很难说服他们。 确定学校及项目 在整个申请中，最重要的是确定学校和想学的项目。 粗选：先确定底线，比如QA排名Top50，然后挨个打开官网搜自己想学方向的关键词(比如传媒=communication、media和advertising等)，把所有搜到的项目页面打开，浏览一遍项目培养目标、课程设置和录取要求。再根据自己的实力(上述四点)，筛选一遍候选项目。 精选：看候选项目的录取细节，要不要准备特殊材料、是不是只给有工作经验的人读，以及学生毕业去向、往年录取比例等等(练T&amp;G阅读终于有了用武之地233)。最终建议锁定大概10-15个项目。 文书 首先根据学校要求，确定有哪些：Personal Statement、Statement of Purpose、Research Proposal、Essay、Essay Video、Resume等等。 根据每个文书下面的具体要求，按学校要求写作(划重点)。这里特别要注意去看项目页面上的FAQs，很多都会有文书材料内必须包含内容的要求，一定要把这些基本要求达到。 文书的写作我建议先按照中文思路，总分总(为啥申请/你的优势123/未来计划)。把每一块儿写清楚了以后，再按照西式思路调整，比如开头用一个引人入胜的小故事、结尾用个自己对这个行业和社会的思考等等，使之文学化、情感化。另外要注意文书重点是体现个人的潜力，不是现有的成就，毕竟录取委员会的人很可能不是特别懂专业上的事。 实在把握不准，可以找文书中介。 网申提前办Visa或Master卡 网申是需要用信用卡的，需要提早办理，以便网申的时候不慌乱。 注意各种deadlines 看项目主页的时候，里面会有申请项目的开放申请和具体截止时间。有的申请是滚动录取的，会明确写上一批二批三批等申请和录取时间点，需要额外注意。其次是要分清国际生和本地生、研究生和博士生的时间界线，一般来说国际生截止日期早于本地生，博士生早于研究生。 准备各种材料 一是明确的邮寄地址、不会失效的电子邮件地址、推荐信作者的联系信息等电子材料。 二是护照扫描件、本科中英文成绩单和在读证明扫描件、获奖证书扫描件、TG成绩单扫描件等实体材料转电子版。 三是在ETS官网购买TG送分服务、WES成绩认证送分服务等电子事务，这些都需要用信用卡支付。 提交及等待 提交完网申以后，注意看看哪些信息还可以继续更新，有的学校还允许继续更新语言成绩(意味着还可以再刷)。 漫长的等待，以及随时注意邮箱(和垃圾邮箱)中学校发来的邮件，看是否有材料需要补充。 如果被调剂了(被该项目拒了，但是推荐了别的项目)，想申请的话就要立刻赶快再填写网申，抢人头。 入学确认Offer(无奖录取=admission，有奖录取=offer，这里用offer统称) 正常的offer发放日期应该是大四寒假及大四下。在比较多家offer后，给最心仪的学校交订金，其它的写信礼貌拒绝。 对确认好的offer尽早开始研究细节，特别重要的是申请I-20表格，这一点对于后面申请签证、体检疫苗和租房等都有影响。一切信息以官网为主，一般会有一个admitted student页面来说明所有事务。 项目细节 很多项目会为被录取者开一个网上研讨会Webinar，来介绍今年项目的各种细节，包括课程设置、教学日历、就业升学、校友资源和学校资源等。这是一个很好的去了解该不该接offer的机会。 签证 务必重视材料的准备！！！ 需要的材料有：1.学校I-20表格；2.个人resume；3.学术型项目需要提供的导师简历；4.DS-160打印页、预约确认单和美签尺寸的照片等使馆要求的必备物品；5.学习计划 面签时要尽早去排队。我是在北京约的7:30到使馆，然而7:10到的时候已经排起了长龙；不过整个流程不算慢，约1小时。 其它细节：1.照片要求比较复杂，深色衬衣和白背景，建议找天真蓝这类机构进行拍摄；2.北京使馆对面是可以存东西的（面签时不能带手机），然而我当时交给街头小贩保管了…损失了100大洋；3.最好找白人男签证官。我分到了全北京拒签率最高的那位亚裔女签证官，然后就GG了…补了很多材料，这几天才通过。 补签：如果被发到221绿表，也不用太紧张，多半是要以邮件形式补充材料。行政审理期大概1周到4周不等，如果当场不留护照的话，最后还得通过中信银行寄送护照回使馆去盖签证（免费寄送）。 体检/疫苗 体检是为了拿到健康证明小红本。疫苗是为了拿到疫苗证明小黄本，同时可能需要完成学校规定的疫苗注射，并请医生填写学校的外文单。 体检/疫苗攻略：https://mp.weixin.qq.com/s/S8FOFCJINn-5v7E-qq1TXg 温馨提示：大城市如北京，一般疫苗种类齐全，但排队人数众多，还需要提前预约；小城市可能疫苗种类不足，需要提取打电话问一下，但是胜在完全没人排队，半小时即可全部搞定。我就是疫苗在北京打，但体检回自家小城市做的。 机票/租房/家具 机票：1.至少提早一个月买，便宜点；2.在第三方平台看好机票后，去该航空的官网买可能会更便宜一点；3.去美国的话飞行时间特别长，建议能买评分高的航空就买高的，舒服一些。比如ANA、卡塔尔航空、南航等等；4.不买直航买转机的话，建议两段旅程都不要飞太久，比如…从香港转就有一段16～17个小时，会累死的= =。 租房：讲道理可以不找中介自己租，但是纽约租房可能是全美最复杂的租房了，所以还是找了在做中介的NYU学长帮忙。有几点tips：1.尽早租房，晚了就没好房源；2.尽早找好合租的人，然后大家按实际使用面积分摊房租，最好在敲定房子前就分配好比例；3.合同签署后48小时内就得付押金等，所以要先准备好一些钱；4.纽约还有一点特殊的就是部分新房需要有当地第三方机构担保，所以还要准备更多的押金… 家具：一般来说新生老生们会拉二手家具交易群，可以随时关注里面的信息。如果不想买二手的，就去亚马逊上买，但是记得地址一定要填到公寓的详细单元号，否则会寄丢(譬如我…)！！！ 感想 最大问题就是学历背景。师哥曾和我说，双非申学校，如果没有比同样申这个学校的985同学各方面都好上一截儿，就别考虑了。现实很残酷，我的申请结果也证明了这一点。 其次问题是太轻视托福。当时老看网上说托福过百即可，所以就没有继续认真刷。但事实上，105+的托福能让一个人从一群100+的申请者中脱颖而出。此外，GRE的重要性远低于托福，G320+T110&gt;&gt;G330+T100。 有好几个项目是我心有不甘的，当时看了地里被录取的人的背景，基本和我一致，唯一区别就是学历和一些软经历。所以说如果当时自己把托福考得更高一点、实习找的名气(比如几乎保送北美top10的msra)再大一些，也许结果就又完全不同了。 第一次有一段很长时间的半夜惊醒去查看邮箱，第一次因为失去信心匆匆忙忙追加保底院校，但也是第一次自己认认真真做完人生的一件大事。不能后悔，不必后悔，也更想感谢一路帮我的朋友们！]]></content>
      <tags>
        <tag>Postgraduate Application</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Siamese-LSTM (孪生网络)]]></title>
    <url>%2F2018%2F05%2F03%2FSiamese-LSTM-%E5%AD%AA%E7%94%9F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[最近因为公司leader的要求，简单研究了一下孪生网络(Siamese LSTM，一个用来计算句对相似度的模型)。 背景 孪生网络的思想比较简单，是分别利用LSTM对待比较的句对中句子进行建模，然后计算两个隐层向量的曼哈顿距离(Manhattan distance)来评价句子相似度。由于LSTM建模过程一致，因此可以用全部句子训练LSTM的参数，然后参数共享给左右两个LSTM网络。 要点 1.将句子建模网络从LSTM改造为Bi-LSTM+Attention 2.中文训练数据为蚂蚁金服句对数据，约4万组，正负样本比例1:3.6；英文训练数据来自Kaggle上的Quora句对数据，约40万组，比例1:1.7。翻译数据指使用Google Translator将Quora数据翻译成中文(机翻，质量一般)。 资料 参考文献 Siamese Recurrent Architectures for Learning Sentence Similarity How to predict Quora Question Pairs using Siamese Manhattan LSTM 其它数据 英文词向量：GoogleNews-vectors-negative300.bin.gz 英文词向量：GoogleNews-vectors-negative300.bin.gz的百度网盘地址 中文词向量：基于120G中文语料训练的64维、128维词向量 工程参考 likejazz/Siamese-LSTM Original author’s GitHub 做个聊天机器人/智能客服 一些网络设计思路 代码 我的实现 结果123456789$ 根据数据比例来看，中文训练集的基准准确率应为0.783，英文与翻译数据为0.630$ =================================================================================================$ 中文 数据实际训练 5 轮时的效果：使用随机词向量时，训练集十折交叉0.778；使用CN120G词向量时，训练集十折交叉0.789$ 英文 数据实际训练 5 轮时的效果：使用随机词向量时，训练集十折交叉0.774；使用Google词向量时，训练集十折交叉0.771$ 翻译 数据实际训练 5 轮时的效果：使用随机词向量时，训练集十折交叉0.755；使用CN120G词向量时，训练集十折交叉0.756$ =================================================================================================$ 中文 数据实际训练 8 轮时的效果：使用随机词向量时，训练集十折交叉0.777；使用CN120G词向量时，训练集十折交叉0.787$ 英文 数据实际训练 8 轮时的效果：使用随机词向量时，训练集十折交叉0.774；使用Google词向量时，训练集十折交叉0.778$ 翻译 数据实际训练 8 轮时的效果：使用随机词向量时，训练集十折交叉0.786；使用CN120G词向量时，训练集十折交叉0.786 总结 1.有无预训练词向量几乎不影响结果。 2.中文数据上训练几乎没有效果，和英文形成鲜明对比–这是因为蚂蚁金服数据间太相似了或者数据量太小，翻译数据集上的实验证明了这一点。 3.孪生网络的效果没有想象中的那么好，后续还会继续从调参、加停用词等角度进行研究。此外，之前在QA机器人中用CNN来做句子语义匹配时缺少数据，现在这里的数据可以用了！233]]></content>
      <tags>
        <tag>SentenceMatching</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My QA Robot]]></title>
    <url>%2F2018%2F04%2F01%2FMy-QA-Robot%2F</url>
    <content type="text"><![CDATA[从2017年10月-2018年3月，出于实习公司的要求，我尝试搭建了一个简单的单轮检索式问答系统。 背景 目前，自动问答技术的实现主要有阅读式、检索式和生成式三大类。 阅读式：指机器阅读理解，通过给定文章和针对文章提出的具体问答对训练机器的阅读理解能力，并对针对文章的新问题作出回答。机器阅读理解包括passage-based、assertion-based、sentence-based和answer-span-based等不同粒度。阅读式的问题是知识边界狭窄，训练语料构筑困难，较适合于专业领域的机器人。 生成式：指使用Seq2Seq搭配海量语料训练具有自主对话能力的系统。生成式理论上可实现不受知识局限的通用机器人，但目前生成式最大的问题是答非所问、胡言乱语，无法表现出人类在解决问题时的逻辑自洽、思维发散和自然表达等能力。这个实现技术比较多用在闲聊机器人方面。 检索式：检索式是一种取长补短的中间无监督技术。检索式要求搜集众多在线问答网站海量的历史问答对语料作为知识库，通过检索与新问题最接近的历史相似问题和最匹配这一新问题的历史最佳问答，变相完成自动问答任务。检索技术既避免了知识的局限，又利用了人类的先验经验积累，是当前处理通用问答任务较合适的方法之一。 思路 结合实际，我的设计为：1.通过爬虫和用户历史数据建立QA式本地知识库；2.当给定用户新问题时，系统先通过ES（ElasticSearch，ES原理）进行词义级的本地检索得到参考问答初选集；3.使用语义深度匹配和问答质量评估从初选集中精选出参考问答精选集，按得分降序返回最终参考问答；4.若没有任何问答通过精选，则启动在线搜索获取标识有“最佳问答”的参考问答返回给用户，并将其中的优质问答缓存到本地ES索引中。 变量或参数名 说明 StopWords 停用词表 KeyWords 核心主题词表 Text_Main_Content(TMC) 用户新问题经分词、去除停用词后的分词结果 Keywords_in_Content(KIC) 用户新问题命中的核心主题词 Text_NER_List(TNL) 用户新问题中的分词后的NER Question_Main_Content(QMC) ES本地检索返回的候选问题经分词、去除停用词后的分词结果 QuestionScore(QS) 语义匹配模块返回的候选问题与新问题的相似度得分 AnswerScore(AS) 通过答案排序模块计算出的，候选问题对应的候选答案与用户新问题组成“好问答对”的概率得分 FinalScore(FS) 候选问答在精选规则下的最终得分 THRESHOLD 控制在精选集中记录ES初选结果的阈值。 ZHIDAOMAX 控制在线搜索时从百度知道返回的问答数量。 SAVESCORE 控制增量存储模块中的百度知道问答对的阈值 ESMAX 控制精选集中的精选问答数量 子模块 根据上文项目思路的介绍，除主函数外，共调用了语义深度匹配、问答质量评估和在线搜索缓存三大子模块 语义深度匹配子模块：参考项目 句对预处理及建模: 将输入的句子1与句子2进行分词，然后将两组切分后的结果两两计算基于Word Embedding的余弦相似度，并填充成上图所示的2D Feature Map形式。当句对的分词list长度不同时，使用0填充缺失值。 模型训练与调用：训练时，将训练集句对的Feature Map输入到CNN模型中，获取语义匹配模型并保存；预测时，调用预训练好的语义匹配模型，输入新句对的Feature Map，将返回这组句对的相似概率得分。 问答质量评估子模块：使用若干文本和非文本特征训练二分类器，参考论文 在线搜索缓存子模块： 百度主页搜索: 向百度主页请求输入用户新问题后的检索结果，若检索结果的第一条中包含预设结果链接，则获取内容并向用户返回。 百度知道搜索：当百度主页搜索没有结果时，在百度知道页面请求输入用户新问题后的检索结果，若Top K条结果中包含带有“最佳回答”标识的回答，则获取对应问答内容并返回。 本地缓存优质问答：当百度知道搜索有结果返回时，调用问答质量评价子模块对该问答对进行质量判断，将高于设定阈值(SAVESCORE)的问答对缓存到本地知识库。缓存机制有助于后续提升系统整体的响应速度及性能。 代码 My QA Robot 总结 项目内：1.项目执行时，语义深度匹配子模块缺少合适的负样本(词义上相似但语义不相似的句对)；2.各种参数阈值很难调试。 项目外：工程性太重，并且以检索为主，在语义理解、匹配和推理方面展开太少。]]></content>
      <tags>
        <tag>QuestionAnswering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Assertion-based QA with Question-Aware Open Information Extraction]]></title>
    <url>%2F2018%2F03%2F26%2FAssertion-based-QA-with-Question-Aware-Open-Information-Extraction%2F</url>
    <content type="text"><![CDATA[实验室组会分到一篇断言式QA的论文，阅读后总结如下 背景AAAI 2018 要点 1.定义了基于断言的QA任务(Assertion-based QA) 2.标注了一个Assertion-based QA数据集 3.提出了能够完成Assertion-based QA的两个算法，抽取法和生成法 4.设计了一些实验证明以上方法的效力 内容定义 文章首先介绍了从document_based QA和Assertion_based QA的区别与联系。 文章中的断言指的是具有主谓宾、主谓宾补等结构的句子，该句子可能是给定材料的一部分，也可能是新生成的。 数据集 由于Assertion-based QA是第一次被提出，所以作者团队标注了一个数据集来做具体的研究(666)。 标注方法是先用OIE工具+is-a规则从背景材料中抽取候选断言，然后人工标注断言是否可以作为答案。 算法 第一种算法是生成法。首先用bi-lstm分别对问题和材料编码，拼接尾部隐状态作为特征表示。其次先用一个元组级解码器(Tuple-level decoder)生成结构，然后再用词级别解码器(Word-level Decoder)在结构中逐个生成需要填充的词。下图来自PPT，但是缺了不少信息，我做了批注。 第二种算法是抽取法。抽取法的本质就是对所有候选断言进行机器排序，过程和人工标注很相似。这里的机器排序算法是开源的LambdaMART算法。需要指出的是，这个方法思路很简单，但是做的比较精致的是特征工程，分别从word、phrase和sentence level抽取了特征进行组合。前两类特征比较简单，sentence level使用了CNN和GRU组合来抽取隐藏层的语义交互特征。下图同样来自PPT，其中包含一些和论文对不上的内容，最后是通过邮件和作者沟通后补充了部分，详情见批注。 实验 实验部分文章主要做了4个实验：分别测试生成法和抽取法的效果，以及很创新地分别把生成与抽取的结果再编码成特征放到Passage_based QA中去。后面两个实验是为了证明Assertion_based QA具有更多研究价值，可以辅助别的QA任务。 不过我感觉PPT有点“王婆卖瓜，自卖自夸”。我从文章中摘了一张针对后两个实验和其它算法一起比较效果的图放在下面，数据显示：1.工程性更重的抽取法效果好于生成法；2.CNN等方法得到的特征效果好于抽取法。 资料 译文和PPT 总结 总的来说，这篇论文确实做了非常多且较完整的开创性工作，特别是Assertion-based QA的定义和数据集，为QA领域开辟了一些新的思路。尽管提出的算法效果比较一般，但是也可以视作该领域的baseline之一，以鼓励更多更优秀的算法和模型出现。]]></content>
      <tags>
        <tag>QuestionAnswering</tag>
        <tag>Paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Mining & NLP Internship in BaiFenDian Infotech]]></title>
    <url>%2F2018%2F03%2F20%2FData-Mining-NLP-Internship-in-BaiFenDian-Infotech%2F</url>
    <content type="text"><![CDATA[2017年10月-2018年3月，我在百分点自然语言处理组担任数据挖掘实习工程师 求职面试 其实面试真的很糟糕… 因为是第一份实习，而且当时真的是很弱的小白一只(现在也是= =)，所以面试几乎都没有答上来。 面试分成笔试、技术一面、技术二面和HR面。 笔试印象最深的是第一题，问有一个信号发射器，每次以不均等的概率发射0或1信号，如何组合出一个以均等概率发射0或1的信号发射器？如何组合出一个以均等概率发射0…N的信号发射器？然后…我很想当然地以为是像N次独立二项试验一样组合概率就可以，其实是要模拟二进制数。后面记不太清楚了，基本都是NLP编程题，读取文本之类的。还记得有一个算法题是要优化排序，然而我直接写用list.sort()不就好了…然后技术面就被问了list.sort()内部用的什么排序算法= =…最后就是一些机器学习的概念题，KNN和Kmeans概念之类的，比较简单。 技术面 一面基本上围绕笔试题，被疯狂吐槽了一番各种错误，基本上每道题都有错…后来坚定了我刷leetcode的决心… 二面问了一些实际开发的内容，先是介绍自己做过的SMP CUP 2017，再是问了诸如神经网络怎么防止过拟合等问题。说实话，这种沟通型面试我还是挺擅长的…各种嘴炮护体。 工作 工程方面：论坛帖子分类、检索式社区问答系统搭建 算法方面：利用N-gram和CNN实现相似句对判断、问答对质量评估，都是在问答系统搭建里面做的子模块 客户方面：一个远程电话支持，一个客户现场技术支持(我一个小小的实习生，怎么就跟着去做了售前呢…可能是我的嘴炮能力被发现了= =) 感想这个行业挣钱多不多？多。2017年可以说是中国的人工智能和机器学习元年。我们每天的数据中有80%是文本数据，所以以NLP为基础的数据处理业务增长迅速。大量的公司在分蛋糕，并且分得很轻松，俗称“降维打击”：大量的公司只是觉得人工智能高大上，实际上弱AI和机器学习很简单，各种工程用的都是github上的开源代码… 工作具体干什么？两类工作：业务线与产品线。业务指的是接销售、产品给的项目，然后做定制化开发；当然有的时候也包括售前支持，比如说去客户现场做技术支援。产品线指的是开发通用产品，比如说我一直负责开发的自动问答系统，甚至还包含了一点点算法研究的意味在里面。 能力要求怎么样？三个能力最重要：自己网上找资料的能力，熟练的代码能力和持续加班的身体能力。 要怎么培养能力？“偷看”公司以前的积累(来自前辈的教导~)多和同事聊天，勤问勤练；好记性不如烂笔头，把每天的事情都总结下来，调研+开发等多写代码，养成优秀的代码习惯：加注释+readme！！！想清楚自己的未来发展方向：工作？升学？但不管哪种，建议多读书..很多时候有啥不理解的，一看相关论文就有了思路。所以说我感觉这个领域读深一点真的没坏处，需要知识积累。 不足百分点以前一直是一个大数据工程公司，现在开始转型AI。正因如此，并且由于企业需要生存，它很多时候经营思路还是做工程，很少有沉下心来做算法研究的机会。我认为自己在工程上已经学到一点皮毛了，尽管很轻浮，但我还是希望能去一个研究型的AI公司感受一下算法研究工程师的工作∠( ᐛ 」∠)＿～]]></content>
      <tags>
        <tag>Internship</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SMP CUP 2017]]></title>
    <url>%2F2017%2F08%2F31%2FSMP-CUP-2017%2F</url>
    <content type="text"><![CDATA[SMP CUP 2017是我参加的第一个数据挖掘类比赛。 比赛简介给定CSDN用户的博客和行为数据（浏览、评论、收藏、转发、点赞/踩、关注、私信），进行用户画像生成。赛事官网地址：https://biendata.com/competition/smpcup2017/画像生成总共分为三项任务： 从给定博文中抽取3个关键词标签 从给定标签空间中选出3个来给给定用户打标签，依据是用户发表的博文以及一系列行为数据 通过给定用户过去一段时间的行为数据，预测其在未来一段时间内的成长值。用户成长值是根据用户的综合表现打分所得，但不知道具体打分准则。成长值将会归一化到[0, 1]区间，其中值为0表示用户流失。 数据样例 项目 内容 用户ID U00296783 博文ID D00034623 博文内容 Title:[转]使用TextRank算法为文本生成关键字和摘要; Content: TextRank算法基于PageRank… 博文标签 Keyword1: TextRank; Keyword2: PageRank; Keyword3: 摘要 用户标签 Tag1: 大数据; Tag2: 数据挖掘; Tag3: 机器学习 发布记录 U00296783／D00034623／20160408 12:35:49 浏览记录 D09983742／20160410 08:30:40 评论记录 D09983742／20160410 08:49:02 点赞记录 D00234899／20160410 09:40:24 点踩记录 D00098183／20160501 15:11:00 私信记录 U00296783／U02748273／20160501 15:30:36 收藏记录 D00234899／20160410 09:40:44 关注记录 U00296783／U02666623／20161119 10:30:44 成长打分 0.0367 (注：浏览、评论、点赞、点踩和收藏指的是该用户对其它博文的行为) 思路总结 抽取关键词其实方法也不太多，一开始想到的办法就是利用tfidf和textrank。后来查阅到了一些2016年搜狗CCF比赛的资料，就尝试把LDA抽取出来的主题关键词作为规则融合进来。简而言之，就是不断调整tfidf和textrank赋予给每个词的权重，尽可能地去提升比赛评分标准下的得分。 任务二大概的做法是先把用户行为分类转化成对应的文章，发表行为下的文章是一类，浏览的、评论的、点赞点踩的、收藏的文章是一类，私信和关注的用户所写的文章是一类。这个假设非常的主观，我们当时的依据就是用户做这些行为的成本不一样，比如说发表文章比较累…所以这些文章可以被分成3类，刚好对应3个标签(其实简单对训练集做了统计发现这个规律也确实还有一点点道理)。之后就是让每一类文章投票选出一个最可能的标签，分类方法是tfidf加权生成句向量+SVC。 成长值这个任务其实一看就是和时间序列有关的，但是局限于代码水平…当时就直接当作一个简单的回归问题来处理。先手动抽取了一些特征，比如每个用户过去每个月的评论数及增长率等，然后直接丢到PassiveAggressive和GrandientBoosting分类器里面，加一个Stacking做了融合预测。 模型实现 我在github上的开源代码 论文链接 比赛心得 任务1：现在回过头来看这个方法真的非常稚嫩，但是倒也相当符合“规则派”的做法，毕竟我后来的实习工作中也证明了规则的有效性。不过第一名中科院团队的做法是纯正的神经网络，用来多个Softmax来投票，效果比第二名好了一大截，我怀疑除了他们大家都是tfidf+规则… 任务2：最开始采用了TextCNN的方法，但是效果比较一般，主要是不知道怎么把单标签分类的模型改成多标签…现在想想其实就是一个SoftMax…任务2我们的效果是最差的，我猜测就是因为主观假设太强了。 任务3：说实话这个模型的效果竟然意外的好，我觉得应该是因为纯数字数据的原因，毕竟对于数字，机器还是很容易去“拟合”一个模型出来的。 不过总的来说，当时真的是年轻头铁硬扛…开始比赛前，我连sklearn和numpy这些必备的包都不会用，更别说tensorflow和keras了。当时也是运气好，比赛因故延期了很长时间，我在比赛中期发现了kaggle和泰坦尼克数据集，并且由此自学了一堆机器学习的东西，才最后踉踉跄跄搞出了任务3的模型。像时间序列预测等等都没法做出来，很重要的原因是花了很长时间自学这些底层的东西，虽然按理来说应该在赛前就先打好基础的。不过回头看，此前整个实验组都没有经验，因为我们的辛苦帮老师和实验组做了一些开拓性的事情，还是非常让人喜悦的。]]></content>
      <tags>
        <tag>Data Mining Contest</tag>
      </tags>
  </entry>
</search>
