<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Internship,">










<meta name="description" content="[TOC] 1.L1 、L2损失函数L2损失：平方损失函数（回归损失函数）：   L(Y,f(X))=(Y−f(X))^2， 计算预测值与真实值之间距离的平方和, 如果误差＞1，误差就会被放大很多 L1损失：绝对损失函数： L(Y,f(X))=|Y−f(X)| 鲁棒性更强 常用损失函数1.平方损失函数 2.绝对值损失函数 3.0-1损失函数 4.对数损失函数（逻辑回归） L(y,p(y|x))=−">
<meta name="keywords" content="Internship">
<meta property="og:type" content="article">
<meta property="og:title" content="Interview Preparation from Nuo Chen">
<meta property="og:url" content="http://yoursite.com/2019/04/11/Interview-Preparation/index.html">
<meta property="og:site_name" content="Junru">
<meta property="og:description" content="[TOC] 1.L1 、L2损失函数L2损失：平方损失函数（回归损失函数）：   L(Y,f(X))=(Y−f(X))^2， 计算预测值与真实值之间距离的平方和, 如果误差＞1，误差就会被放大很多 L1损失：绝对损失函数： L(Y,f(X))=|Y−f(X)| 鲁棒性更强 常用损失函数1.平方损失函数 2.绝对值损失函数 3.0-1损失函数 4.对数损失函数（逻辑回归） L(y,p(y|x))=−">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121209345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121156701?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121147551?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121313817?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121318220?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121322436?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121325555?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121329128?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121332645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121335232?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121338803?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121354566?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121414285?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121421970?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121434228?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180816121451605?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20131113203307078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20131113202319171?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20131113203457937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20131113203517875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554355746126.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554355797245.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554355890343.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554361201436.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554362054737.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554362410969.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554366967554.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554391876514.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554373460186.png">
<meta property="og:image" content="https://upload.wikimedia.org/math/2/b/b/2bbe4b0725baed85eae8dbbb20360ea6.png">
<meta property="og:image" content="https://upload.wikimedia.org/math/7/f/d/7fd32efb7a21cc484be23d1015ee074e.png">
<meta property="og:image" content="https://img-blog.csdn.net/20171117094740886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTg5MTYzMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/1238724/201808/1238724-20180816153629046-1282691739.jpg">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/1238724/201808/1238724-20180816153708598-1386881939.jpg">
<meta property="og:image" content="https://images2018.cnblogs.com/blog/1238724/201808/1238724-20180816155333172-1812216944.jpg">
<meta property="og:image" content="http://img.blog.csdn.net/20141117153816148">
<meta property="og:image" content="http://img.blog.csdn.net/20141117154035285">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713152436931-1817493891.png">
<meta property="og:image" content="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png">
<meta property="og:image" content="https://img-blog.csdn.net/20180517202555969?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180517203053419?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180517204233919?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180517204803765?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/2018051721101642?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180517211424276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180517211736459?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img.mukewang.com/5b3f329300018c2001690021.jpg">
<meta property="og:image" content="https://img1.mukewang.com/5b3f32a00001d9ab01800021.jpg">
<meta property="og:image" content="https://img1.mukewang.com/5b3f32a90001906800570021.jpg">
<meta property="og:image" content="https://img4.mukewang.com/5b3f32b3000101fc00550021.jpg">
<meta property="og:image" content="https://img1.mukewang.com/5b3f32db00019d8302930055.jpg">
<meta property="og:image" content="https://img2.mukewang.com/5b3f32ee00012ce201720022.jpg">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2018110410374392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpYW45OQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Vector_space_model.jpg/250px-Vector_space_model.jpg">
<meta property="og:image" content="https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D394/sign=20b5db49b7a1cd1101b674298d13c8b0/ac4bd11373f0820282c6ae4646fbfbedab641b76.jpg">
<meta property="og:image" content="https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D38/sign=5ba753a7306d55fbc1c6702e6d22952c/f2deb48f8c5494ee8d2f306020f5e0fe99257ebd.jpg">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554384143952.png">
<meta property="og:image" content="c:/Users/norkey/AppData/Roaming/Typora/typora-user-images/1554384301056.png">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?W%3D%28w_1%2Cw_2%2C...%2Cw_n%29">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?w_i">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?i">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/716934/201509/716934-20150901131811606-1633545801.jpg">
<meta property="og:image" content="https://img-blog.csdn.net/20180507155519645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NvbmdiaW54dQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/743682/201606/743682-20160604170443024-2120174326.png">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?w">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?C%28w%29">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?m%20%5Ctimes%20%7CV%7C">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?n-1">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1108812/201702/1108812-20170223164229007-1606932794.png">
<meta property="og:image" content="https://images2015.cnblogs.com/blog/1108812/201702/1108812-20170223164433570-1697102042.png">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215807453">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215819060">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215826539">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215838680?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbmZlbmdnYW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215849396">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/005BVyzmly1fotnxtqjlmj305i06q3yd.jpg">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215859449">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215908067">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215918744">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215926971">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215934392">
<meta property="og:image" content="https://img-blog.csdn.net/20170904215943994">
<meta property="og:image" content="https://img-blog.csdn.net/20170724164552739?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHdkMTgyODA4MjAwNTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:updated_time" content="2019-04-11T17:14:30.156Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Interview Preparation from Nuo Chen">
<meta name="twitter:description" content="[TOC] 1.L1 、L2损失函数L2损失：平方损失函数（回归损失函数）：   L(Y,f(X))=(Y−f(X))^2， 计算预测值与真实值之间距离的平方和, 如果误差＞1，误差就会被放大很多 L1损失：绝对损失函数： L(Y,f(X))=|Y−f(X)| 鲁棒性更强 常用损失函数1.平方损失函数 2.绝对值损失函数 3.0-1损失函数 4.对数损失函数（逻辑回归） L(y,p(y|x))=−">
<meta name="twitter:image" content="https://img-blog.csdn.net/20180816121209345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/04/11/Interview-Preparation/">





  <title>Interview Preparation from Nuo Chen | Junru</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Junru</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-resume">
          <a href="/resume/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-file"></i> <br>
            
            Resume
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            Commonweal 404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/11/Interview-Preparation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Junru Lu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/29519546.png">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Junru">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Interview Preparation from Nuo Chen</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-11T00:00:00-04:00">
                2019-04-11
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/11/Interview-Preparation/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/04/11/Interview-Preparation/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/04/11/Interview-Preparation/" class="leancloud_visitors" data-flag-title="Interview Preparation from Nuo Chen">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>[TOC]</p>
<h4 id="1-L1-、L2损失函数"><a href="#1-L1-、L2损失函数" class="headerlink" title="1.L1 、L2损失函数"></a>1.L1 、L2损失函数</h4><p>L2损失：平方损失函数（回归损失函数）：   L(Y,f(X))=(Y−f(X))^2， 计算预测值与真实值之间距离的平方和, 如果误差＞1，误差就会被放大很多</p>
<p>L1损失：绝对损失函数： L(Y,f(X))=|Y−f(X)| 鲁棒性更强</p>
<h5 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h5><p>1.平方损失函数</p>
<p>2.绝对值损失函数</p>
<p>3.0-1损失函数</p>
<p>4.对数损失函数（逻辑回归）</p>
<p>L(y,p(y|x))=−logp(y|x)</p>
<p>在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。</p>
<h4 id="2-L1、L2正则"><a href="#2-L1、L2正则" class="headerlink" title="2.L1、L2正则"></a>2.L1、L2正则</h4><p>（机器学习中，如果参数过多，模型过于复杂，容易造成过拟合（overfit）。(即模型在训练样本数据上表现的很好，但在实际测试样本上表现的较差，不具备良好的泛化能力)）</p>
<p><strong>概括：</strong></p>
<p>L1和L2是正则化项，又叫做惩罚项，是为了限制模型的参数，防止模型过拟合而加在损失函数后面的一项。</p>
<p><strong>区别：</strong></p>
<p>L1是模型各个参数的绝对值之和。</p>
<p>L2是模型各个参数的平方和的开方值。</p>
<p>L1会趋向于产生少量的特征，而其他的特征都是0. 因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵</p>
<p>L2会选择更多的特征，这些特征都会接近于0。  最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0</p>
<p><strong>L1范式和L2范式为什么可以防止过拟合</strong>：</p>
<p>L1范数符合拉普拉斯分布，是不完全可微的。表现在图像上会有很多角出现。这些角和目标函数的接触机会远大于其他部分。就会造成最优值出现在坐标轴上，因此就会导致某一维的权重为0 ，产生稀疏权重矩阵，进而防止过拟合。</p>
<p>L2范数符合高斯分布，是完全可微的。和L1相比，图像上的棱角被圆滑了很多。一般最优值不会在坐标轴上出现。L2正则化使值最小时对应的参数变小。</p>
<h6 id="L1、L2具体过程："><a href="#L1、L2具体过程：" class="headerlink" title="L1、L2具体过程："></a>L1、L2具体过程：</h6><p>首先针对L1范数<img src="https://img-blog.csdn.net/20180816121209345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化当<img src="https://img-blog.csdn.net/20180816121156701?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">时取1，当<img src="https://img-blog.csdn.net/20180816121147551?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">时取-1.</p>
<p>从而导致的参数<img src="https://img-blog.csdn.net/20180816121313817?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">减去了学习率与(13)式的乘积，因此当<img src="https://img-blog.csdn.net/20180816121318220?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">大于0的时候，<img src="https://img-blog.csdn.net/20180816121322436?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">会减去一个正数，导致<img src="https://img-blog.csdn.net/20180816121325555?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">减小，而当<img src="https://img-blog.csdn.net/20180816121329128?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">小于0的时候，<img src="https://img-blog.csdn.net/20180816121332645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">会减去一个负数，导致<img src="https://img-blog.csdn.net/20180816121335232?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">又变大，因此这个正则项会导致参数<img src="https://img-blog.csdn.net/20180816121338803?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为 Lasso regularization。</p>
<p>然后针对L2范数<img src="https://img-blog.csdn.net/20180816121354566?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">，同样对它求导，得到梯度变化为<img src="https://img-blog.csdn.net/20180816121414285?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">(一般会用<img src="https://img-blog.csdn.net/20180816121421970?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">来把这个系数2给消掉)。同样的更新之后使得<img src="https://img-blog.csdn.net/20180816121434228?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。</p>
<p>需要注意的是，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而(12)式中的<img src="https://img-blog.csdn.net/20180816121451605?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NjYmxvZ2dlcg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，越大，对参数值惩罚越大，泛化能力越好。</p>
<h4 id="3-逻辑回归"><a href="#3-逻辑回归" class="headerlink" title="3.逻辑回归"></a>3.逻辑回归</h4><h5 id="逻辑回归原理"><a href="#逻辑回归原理" class="headerlink" title="逻辑回归原理"></a>逻辑回归原理</h5><p>1.找一个合适的预测函数（Andrew Ng的公开课中称为hypothesis），一般表示为<strong>h</strong>函数，该函数就是我们需要找的分类函数，它用来预测输入数据的判断结果。这个过程时非常关键的，需要对数据有一定的了解或分析，知道或者猜测预测函数的“大概”形式，比如是线性函数还是非线性函数。</p>
<p>所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：</p>
<p><img src="https://img-blog.csdn.net/20131113203307078?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img">          </p>
<p>对应的函数图像是一个取值在0和1之间的S型曲线（图1）。</p>
<p><img src="https://img-blog.csdn.net/20131113202319171?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>2.构造一个Cost函数（损失函数），该函数表示预测的输出（<strong>h</strong>）与训练数据类别（<strong>y</strong>）之间的偏差，可以是二者之间的差（<strong>h-y</strong>）或者是其他的形式。综合考虑所有训练数据的“损失”，将Cost求和或者求平均，记为<strong>J(θ)</strong>函数，表示所有训练数据预测值与实际类别的偏差。逻辑回归的损失函数是 对数损失函数</p>
<p>函数来衡量<strong>h</strong>函数预测的好坏是合理的。</p>
<p><img src="https://img-blog.csdn.net/20131113203457937?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p><img src="https://img-blog.csdn.net/20131113203517875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvZG9uZ3Rpbmd6aGl6aQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>3.显然，<strong>J(θ)</strong>函数的值越小表示预测函数越准确（即<strong>h</strong>函数越准确），所以这一步需要做的是找到J(θ)函数的最小值。找函数的最小值有不同的方法，Logistic Regression实现时有的是梯度下降法（Gradient Descent）。</p>
<h5 id="逻辑回归和SVM区别"><a href="#逻辑回归和SVM区别" class="headerlink" title="逻辑回归和SVM区别"></a>逻辑回归和SVM区别</h5><p>相同点:</p>
<ol>
<li>都是分类算法</li>
<li>都是监督学习算法</li>
<li>都是判别模型</li>
<li>都能通过核函数方法针对非线性情况分类</li>
<li>目标都是找一个分类超平面</li>
<li>都能减少离群点的影响</li>
</ol>
<p>不同点:</p>
<ol>
<li>损失函数不同，逻辑回归是对数损失函数，svm是hinge loss，自带L2正则项</li>
<li>逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。</li>
<li>逻辑回归对概率建模，svm对分类超平面建模</li>
<li>逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有</li>
<li>逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响</li>
<li>逻辑回归是统计方法，svm是几何方法</li>
</ol>
<h5 id="逻辑回归与最大熵"><a href="#逻辑回归与最大熵" class="headerlink" title="逻辑回归与最大熵"></a>逻辑回归与最大熵</h5><p>最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。此外，最大熵与逻辑回归都称为对数线性模型(log linear model)。</p>
<h5 id="逻辑回归与线性回归"><a href="#逻辑回归与线性回归" class="headerlink" title="逻辑回归与线性回归"></a>逻辑回归与线性回归</h5><p>sigmoid在逻辑回归中起到了两个作用，一是将线性函数的结果映射到了(0,1)，一是减少了离群点的影响。</p>
<p>它们要解决的问题不一样，前者解决的是regression问题，后者解决的是classification问题，前者的输出是连续值，后者的输出是离散值，而且前者的损失函数是输出y的正态分布，后者损失函数是输出的伯努利分布。</p>
<h4 id="4-GDBT和RF区别-GDBT和Xgboost区别"><a href="#4-GDBT和RF区别-GDBT和Xgboost区别" class="headerlink" title="4.GDBT和RF区别   GDBT和Xgboost区别"></a>4.GDBT和RF区别   GDBT和Xgboost区别</h4><p>bagging—-RF  旨在降低方差</p>
<p>stacking</p>
<p>boosting—GDBT Adaboost  xgboost 旨在降低偏差</p>
<h5 id="随机森林RF和GBDT的区别"><a href="#随机森林RF和GBDT的区别" class="headerlink" title="随机森林RF和GBDT的区别"></a>随机森林RF和GBDT的区别</h5><p>1）随机森林采用的bagging思想，而GBDT采用的boostingboosting思想。<br>2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只能由回归树组成。<br>3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。<br>4）对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。<br>5）随机森林对异常值不敏感；GBDT对异常值非常敏感。<br>6）随机森林对训练集一视同仁；GBDT是基于权值的弱分类器的集成。 </p>
<p>7）随机森林是通过减少模型方差提高性能；GBDTGBDT是通过减少模型偏差提高性能。</p>
<h5 id="Xgboost和GBDT的区别"><a href="#Xgboost和GBDT的区别" class="headerlink" title="Xgboost和GBDT的区别"></a>Xgboost和GBDT的区别</h5><p>Xgboost优点(与GBDT对比)</p>
<p>1.xgboost对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</p>
<p>3.xgboost在代价函数里增加了正则项，用于控制模型复杂度。正则项里包含了叶子的结点个数，每个叶子结点输出的score的L2的模的平方和。正则项降低了模型的方差，使模型更简单，防止过拟合</p>
<p>4.<strong>支持并行化</strong>，<strong>直接的效果是训练速度快</strong>，boosting技术中下一棵树依赖上述树的训练和预测，所以树与树之间应该是只能串行！但是，同层级节点可并行。具体地，对于某个节点，节点内选择最佳分类点，进行枚举的时候可以并行</p>
<p><strong>hard voting classifier</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line">voting_clf = VotingClassifier(estimators=[</span><br><span class="line">    (<span class="string">'log_clf'</span>,LogisticRegression()),</span><br><span class="line">    (<span class="string">'svm_clf'</span>,SVC()),</span><br><span class="line">    (<span class="string">'dt_clf'</span>,DecisionTreeClassifier())</span><br><span class="line">],voting=<span class="string">'hard'</span>)</span><br><span class="line"></span><br><span class="line">voting_clf.fit(x_train,y_train)</span><br><span class="line">voting_clf.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p>soft voting classifer</p>
<p>不仅要看投票数，还要看权值（需要每一个模型都能估计概率 predict_proba）</p>
<p>逻辑回归本身就是基于概率模型的—-可以预测概率</p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554355746126.png" alt="1554355746126"></p>
<p>KNN—(例如本题：分给红色的点 2/3)—可以预测概率</p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554355797245.png" alt="1554355797245"></p>
<p>决策树—可以预测概率（计算概率同knn）</p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554355890343.png" alt="1554355890343"></p>
<p>svc—（把probability:boolean,optional(default=false)）,默认是false,把false改成true就可以计算每一个数据样本分给某个类相应的概率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#soft voting classifer</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line">voting_clf = VotingClassifier(estimators=[</span><br><span class="line">    (<span class="string">'log_clf'</span>,LogisticRegression()),</span><br><span class="line">    (<span class="string">'svm_clf'</span>,SVC(probability=<span class="keyword">True</span>)),</span><br><span class="line">    (<span class="string">'dt_clf'</span>,DecisionTreeClassifier())</span><br><span class="line">],voting=<span class="string">'soft'</span>)</span><br><span class="line"></span><br><span class="line">voting_clf2.fit(x_train,y_train)</span><br><span class="line">voting_clf2.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p>如何创建差异性—-每个子模型只看样本数据的一部分</p>
<p>取样-放回取样、不放回取样</p>
<p>放回取样-bagging–更常用（又名bootstrap）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用bagging</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier()</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier()</span><br><span class="line">/*</span><br><span class="line">**DecisionTreeClassifier()决策树模型</span><br><span class="line">**n_estimators 集成<span class="number">500</span>个这样的模型</span><br><span class="line">**max_samples每一个子模型相应的看几个样本数据</span><br><span class="line">**bootstrap=<span class="keyword">True</span> 放回取样</span><br><span class="line">*/</span><br><span class="line">bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=<span class="number">500</span>,max_samples=<span class="number">100</span>,bootstrap=<span class="keyword">True</span>)</span><br><span class="line">bagging_clf.fit(x_train,y_train)</span><br><span class="line">bagging_clf.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p>OOB </p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554361201436.png" alt="1554361201436"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用OOb</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier()</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier()</span><br><span class="line">/*</span><br><span class="line">**DecisionTreeClassifier()决策树模型</span><br><span class="line">**n_estimators 集成<span class="number">500</span>个这样的模型</span><br><span class="line">**max_samples每一个子模型相应的看几个样本数据</span><br><span class="line">**bootstrap=<span class="keyword">True</span> 放回取样</span><br><span class="line">**oob_score=<span class="keyword">True</span> 不使用测试数据集，而使用这部分没有取到的样本做测试/验证</span><br><span class="line">*/</span><br><span class="line">bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=<span class="number">500</span>,max_samples=<span class="number">100</span>,bootstrap=<span class="keyword">True</span>,oob_score=<span class="keyword">True</span>)</span><br><span class="line">bagging_clf.fit(x,y)//所有数据传入进去</span><br><span class="line">bagging_clf.oob_score_</span><br></pre></td></tr></table></figure>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554362054737.png" alt="1554362054737"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用n_jobs</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier()</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier()</span><br><span class="line">/*</span><br><span class="line">**DecisionTreeClassifier()决策树模型</span><br><span class="line">**n_estimators 集成<span class="number">500</span>个这样的模型</span><br><span class="line">**max_samples每一个子模型相应的看几个样本数据</span><br><span class="line">**bootstrap=<span class="keyword">True</span> 放回取样</span><br><span class="line">**oob_score=<span class="keyword">True</span> 不使用测试数据集，而使用这部分没有取到的样本做测试/验证</span><br><span class="line">**n_jobs=<span class="number">-1</span> 并行处理</span><br><span class="line">*/</span><br><span class="line">bagging_clf = BaggingClassifier(DecisionTreeClassifier(),n_estimators=<span class="number">500</span>,max_samples=<span class="number">100</span>,bootstrap=<span class="keyword">True</span>,oob_score=<span class="keyword">True</span>,n_jobs=<span class="number">-1</span>)</span><br><span class="line">bagging_clf.fit(x,y)//所有数据传入进去</span><br><span class="line">bagging_clf.oob_score_</span><br></pre></td></tr></table></figure>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554362410969.png" alt="1554362410969"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#bootstrap_featrues</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier()</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier()</span><br><span class="line">/*</span><br><span class="line">**DecisionTreeClassifier()决策树模型</span><br><span class="line">**n_estimators 集成<span class="number">500</span>个这样的模型</span><br><span class="line">**max_samples每一个子模型相应的看几个样本数据</span><br><span class="line">**bootstrap=<span class="keyword">True</span> 放回取样</span><br><span class="line">**oob_score=<span class="keyword">True</span> 不使用测试数据集，而使用这部分没有取到的样本做测试/验证</span><br><span class="line">**n_jobs=<span class="number">-1</span> 并行处理</span><br><span class="line">**max_features==<span class="number">5</span>每一个子模型相应地取几个特征</span><br><span class="line">**bootstrap_features==<span class="keyword">True</span> 有放回地取特征</span><br><span class="line">*/</span><br><span class="line">Random Patches = BaggingClassifier(DecisionTreeClassifier(),n_estimators=<span class="number">500</span>,max_samples=<span class="number">100</span>,bootstrap=<span class="keyword">True</span>,oob_score=<span class="keyword">True</span>,n_jobs=<span class="number">-1</span>,max_features==<span class="number">5</span>,bootstrap_features=<span class="keyword">True</span>)</span><br><span class="line">Random Patches.fit(x,y)//所有数据传入进去</span><br><span class="line">Random Patches.oob_score_</span><br></pre></td></tr></table></figure>
<h5 id="随机森林RF"><a href="#随机森林RF" class="headerlink" title="随机森林RF"></a>随机森林RF</h5><p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554366967554.png" alt="1554366967554"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">/*</span><br><span class="line">**random_state随机的种子</span><br><span class="line">**max_leaf_nodes通过限制最大叶子节点数,可以防止过拟合,默认是<span class="string">"None”</span></span><br><span class="line"><span class="string">*/</span></span><br><span class="line"><span class="string">rf_clf = RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,random_state=666,oob_score=True,n_jobs=-1)</span></span><br><span class="line"><span class="string">rf_clf.fit(x,y)</span></span><br><span class="line"><span class="string">rf_clf.oob_score_</span></span><br></pre></td></tr></table></figure>
<p>基本思想就是构造多棵相互独立的CART决策树，形成一个森林，利用这些决策树共同决策输出类别。随机森林算法秉承了bagging方法的思想，以构建单一决策树为基础，同时也是单一决策树算法的延伸和改进。</p>
<p>在整个随机森林算法的过程中，有<strong>两个随机过程</strong>：</p>
<p>1.输入数据是随机的：从全体训练数据中选取一部分来构建一棵决策树，并且是有放回的选取<br>2.每棵决策树的构建所需的特征是从全体特征中随机选取的</p>
<p>随机森林算法具体流程：<br>1.从原始训练数据中随机选取n个数据作为训练数据输入（通常n远小于全体训练数据N，这样就存在一部分“袋外数据”始终不被取到，它们可以直接用于测试误差（而无需单独的测试集或验证集）。<br>2.选取了要输入的训练数据后，开始构建决策树，具体方法是每一个结点从全体特征集M中选取m个特征进行构建（通常m远小于M）。<br>3.在构造每棵决策树时，选取基尼指数最小的特征来分裂节点构建决策树。决策树的其他结点都采取相同的分裂规则进行构建，直到该节点的所有训练样例都属于同一类或者达到树的最大深度。（该步骤与构建单一决策树相同）<br>4.重复第2步和第3步多次，每一次输入数据对应一颗决策树，这样就得到了一个随机森林，用于对预测数据进行决策。 </p>
<p>随机森林算法的注意点：<br>1.在构建决策树的过程中不需要剪枝<br>2.整个森林中树的数量和每棵树的特征需要人为设定<br>3.<strong>构建决策树的时候分裂节点的选择依据最小基尼系数</strong> （基尼系数表示被分错的概率）<br>4.在训练时，对于预选变量个数和随机森林中树的个数这两个参数的调优很重要。</p>
<p>随机森林的优点：<br>1.两个随机性的引入，使得随机森林抗噪声能力强，<strong>方差小泛化能力强，不易陷入过拟合</strong>（因此不需要剪枝）<br>2.易于高度<strong>并行训练</strong>，且训练速度快，适合处理大数据<br>3.由于随机森林对误差率是无偏估计，因此在算法中<strong>不需要再进行交叉验证或者设置单独的测试集来获取测试集上误差的无偏估计</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaboostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">//max_depth限制最大深度，防止过拟合。最大深度的增加虽然可以增加对训练集拟合能力的增强,但这也就可能意味着其泛化能力的下降</span><br><span class="line">ada_clf = AdaboostClassifier(DecisionTreeClassifier(max_depth=<span class="number">2</span>),n_estimators=<span class="number">500</span>)</span><br><span class="line">ada_clf.fit(x_train,y_train)</span><br><span class="line">ada_clf.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<h5 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h5><p>GBDT是第一棵树分类后的残差，作为第二棵树的输入;依次类推直到残差为0或者到达树深</p>
<p>以决策树（CART）为基学习器的GB算法</p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554391876514.png" alt="1554391876514"></p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554373460186.png" alt="1554373460186"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">#Gradient Boosting</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line">gb_clf = GradientBoostingClassifier(max_depth=<span class="number">2</span>,n_estimators=<span class="number">30</span>)</span><br><span class="line">gb_clf.fit(x_train,y_train)</span><br><span class="line">gb_clf.score(x_test,y_test)</span><br></pre></td></tr></table></figure>
<p><strong>Boosting解决回归问题</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br></pre></td></tr></table></figure>
<p>GGBDT是GB和DT的结合。要注意的是这里的决策树是回归树，GBDT中的决策树是个弱模型，深度较小一般不会超过5，叶子节点的数量也不会超过10，对于生成的每棵决策树乘上比较小的缩减系数（学习率&lt;0.1），有些GBDT的实现加入了随机抽样（subsample 0.5&lt;=f &lt;=0.8）提高模型的泛化能力。通过交叉验证的方法选择最优的参数。因此GBDT实际的核心问题变成怎么基于<img src="https://upload.wikimedia.org/math/2/b/b/2bbe4b0725baed85eae8dbbb20360ea6.png" alt="\{(x_i, r_{im})\}_{i=1}^n">使用CART回归树生成<img src="https://upload.wikimedia.org/math/7/f/d/7fd32efb7a21cc484be23d1015ee074e.png" alt="\! h_m(x)"></p>
<h5 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h5><p>每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低；相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。</p>
<h5 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h5><p>Xgboost优点(与GBDT对比)</p>
<p>1.xgboost对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</p>
<p>3.xgboost在代价函数里增加了正则项，用于控制模型复杂度。正则项里包含了叶子的结点个数，每个叶子结点输出的score的L2的模的平方和。正则项降低了模型的方差，使模型更简单，防止过拟合</p>
<p>4.<strong>支持并行化</strong>，<strong>直接的效果是训练速度快</strong>，boosting技术中下一棵树依赖上述树的训练和预测，所以树与树之间应该是只能串行！但是，<strong>同层级节点可并行。具体地，对于某个节点，节点内选择最佳分类点，进行枚举的时候可以并行</strong></p>
<h5 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h5><ul>
<li>Stacking简单理解就是讲几个简单的模型，一般采用将它们进行K折交叉验证输出预测结果，然后将每个模型输出的预测结果合并为新的特征，并使用新的模型加以训练。</li>
</ul>
<p><img src="https://img-blog.csdn.net/20171117094740886?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXFfMTg5MTYzMTE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p>XGB模型，把train分train1~train5,共5份，用其中4份预测剩下的那份,同时预测test数据，这样的过程做5次,生成5份train（原train样本数/5）数据和5份test数据。然后把5份预测的train数据纵向叠起来，把test预测的结果做平均。</p>
<p>RF模型和XGB模型一样，再来一次。这样就生成了2份train数据和2份test数据（XGB重新表达的数据和RF重新表达的数据）</p>
<p>然后用LR模型，进一步做融合，得到最终的预测结果。</p>
<h4 id="5-CNN、DNN"><a href="#5-CNN、DNN" class="headerlink" title="5.CNN、DNN"></a>5.CNN、DNN</h4><h5 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h5><p>对应相乘再相加</p>
<h5 id="softmax函数"><a href="#softmax函数" class="headerlink" title="softmax函数"></a>softmax函数</h5><p>softmax用于多分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解，从而来进行多分类</p>
<h5 id="常用的激活函数及其作用："><a href="#常用的激活函数及其作用：" class="headerlink" title="常用的激活函数及其作用："></a>常用的激活函数及其作用：</h5><p>作用：通过加入非线性因素，提高模型对数据的拟合能力，以此解决线性不可分的问题</p>
<p>在深度学习中，常用的激活函数主要有：sigmod函数，tanh函数，ReLu函数</p>
<p><strong>sigmod函数：</strong></p>
<p>sigmod函数将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：<br>$$<br>g(z)=\frac{1}{1+e^{-1}}<br>$$</p>
<p><img src="https://images2018.cnblogs.com/blog/1238724/201808/1238724-20180816153629046-1282691739.jpg" alt="img"></p>
<p>sigmod函数缺点：</p>
<p>1.当z值非常大或者非常小时，sigmod导数趋于0，这会导致权重w的梯度将接近于0，即出现梯度消失现象</p>
<p>2.函数的输出不是以0为为均值。那么对于一个多层的sigmod神经网络来说，如果你输入的都是正数，在反向传播中w的梯度传播到网络的某一处时，权值的变化是要么全正要么全负，模型拟合的过程就会</p>
<p>非常慢</p>
<p>所以，</p>
<p><strong>sigmoid函数可用在网络最后一层，作为输出层进行二分类</strong>，尽量不要使用在隐藏层。</p>
<p><strong>tanh</strong>函数：</p>
<p>tanh函数相较于sigmoid函数要常见一些，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (−1,1)(−1,1) 之间，其公式与图形为：<br>$$<br>g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}<br>$$<br><img src="https://images2018.cnblogs.com/blog/1238724/201808/1238724-20180816153708598-1386881939.jpg" alt="img"></p>
<p>tanh函数在 00 附近很短一段区域内可看做线性的。由于tanh函数<strong>均值</strong>为 0 ，因此弥补了sigmoid函数均值为 0.5的缺点。</p>
<p>tanh函数的<strong>缺点</strong>同sigmoid函数的第一个缺点一样，当 z <strong>很大或很小</strong>时，g′(z)接近于 0，会导致梯度很小，权重更新非常缓慢，即<strong>梯度消失问题</strong>。</p>
<p><strong>ReLU</strong>函数</p>
<p>ReLU函数又称为<strong>修正线性单元（Rectified Linear Unit）</strong>，是一种分段线性函数，其弥补了sigmoid函数以及tanh函数的<strong>梯度消失问题</strong>。ReLU函数的公式以及图形如下：<br>$$<br>g(z)=\begin{cases} 0，z&lt;0\ z， z&gt;0\end{cases}<br>$$<br><img src="https://images2018.cnblogs.com/blog/1238724/201808/1238724-20180816155333172-1812216944.jpg" alt="img"></p>
<p>ReLU函数的<strong>优点</strong>：<br>在输入为正数的时候,不存在梯度消失问题。<br>计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多。（sigmod和tanh要计算指数，计算速度会比较慢）<br>ReLU函数的<strong>缺点</strong>：<br>当输入为负时，梯度为0，会产生梯度消失问题。</p>
<h5 id="神经网络："><a href="#神经网络：" class="headerlink" title="神经网络："></a>神经网络：</h5><p>神经网络的学习就是学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。<strong>增加节点数：增加维度，即增加线性转换能力。增加层数：增加激活函数的次数，即增加非线性转换次数。</strong></p>
<h5 id="对卡在局部极小值的处理方法："><a href="#对卡在局部极小值的处理方法：" class="headerlink" title="对卡在局部极小值的处理方法："></a>对卡在局部极小值的处理方法：</h5><p>1.调节步伐：调节学习速率，使每一次的更新“步伐”不同。常用方法：随机梯度下降、小批量梯度下降、增加动量</p>
<p><strong>BGD</strong>(Batch gradient descent)批量梯度下降法：<strong>每次迭代使用所有的样本</strong>  </p>
<p>每次迭代都需要把所有样本都送入，每训练一组样本就把梯度更新一次。。</p>
<p><strong>SGD</strong>（Stochastic gradientdescent）随机梯度下降法：<strong>每次迭代使用一组样本</strong></p>
<p>针对BGD算法训练速度过慢的缺点，提出了SGD算法，SGD算法是从样本中随机抽出一组，训练后按梯度更新一次，然后再抽取一组，再更新一次，在样本量及其大的情况下，可能不用训练完所有的样本就可以获得一个损失值在可接受范围之内的模型了</p>
<p><strong>MBGD</strong>（Mini-batch gradient descent）小批量梯度下降：<strong>每次迭代使用b组样本</strong></p>
<p>SGD相对来说要快很多，但是也有存在问题，由于单个样本的训练可能会带来很多噪声，使得SGD并不是每次迭代都向着整体最优化方向，因此在刚开始训练时可能收敛得很快，但是训练一段时间后就会变得很慢。<strong>在此基础上又提出了小批量梯度下降法，它是每次从样本中随机抽取一小批进行训练，而不是一组。</strong></p>
<p>2.优化起点：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。</p>
<p>常用方法有：高斯（正态）分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）</p>
<p>初始化权重的意义：决定了loss在loss function中从哪个点开始作为起点训练网络。常用方法介绍：</p>
<p>均匀分布初始化权重：<strong>[0,1]范围的均匀分布</strong>  可以使用TensorFlow提供的tf.random_uniform()函数。该函数默认在[0，1]范围内取值。</p>
<p>正态分布初始化权重：上面尝试的权重初始化方法都是在权重的取值要靠近0而不能太小的方向上进行着。正态分布正好符合这个方向，其大部分取值靠近0。</p>
<h5 id="浅层VS深层："><a href="#浅层VS深层：" class="headerlink" title="浅层VS深层："></a>浅层VS深层：</h5><p>相比浅层神经网络，深层神经网络可以用<strong>更少的数据量</strong>来学到更好的拟合。深层的前提是：空间中的元素可以由卷积迭代而来的。</p>
<h5 id="防止过拟合："><a href="#防止过拟合：" class="headerlink" title="防止过拟合："></a>防止过拟合：</h5><p>过拟合具体表现在：模型在训练数据上损失函数较小，预测准确率较高；但是在测试数据上损失函数比较大，预测准确率较低。</p>
<p>防止过拟合的方法：</p>
<ol>
<li><p>L 2正则化</p>
</li>
<li><p>Dropout</p>
</li>
<li><p>每个epoch之后shuffle训练数据，设置early-stopping</p>
</li>
<li><p>加Batch Normalization（对隐藏层的神经元在激活值获得之后，非线性变换之前进行强制归一化）</p>
</li>
</ol>
<p>名词解释：</p>
<p>epoch：当一个完整的数据集通过了神经网络一次并且返回了一次，这个过程称为一个 epoch。随着 epoch 数量增加，神经网络中的权重的更新次数也增加，曲线从欠拟合变得过拟合。</p>
<p>batch size：一个 batch 中的样本总数。记住：batch size 和 number of batches 是不同的</p>
<p>batch 是什么？———在不能将数据一次性通过神经网络的时候，就需要将数据集分成几个 batch。</p>
<p>iteration: 迭代数是 batch 需要完成一个 epoch 的次数。记住：在一个 epoch 中，batch 数和迭代数是相等的。</p>
<p>具体解释：</p>
<ol>
<li>L 2正则化：L 2正则表达式是 所有参数平方和的开方值。符合高斯分布，是完全可微的。L 2正则化使值最小时对应的参数变小。</li>
<li>Dropout：在每个训练批次中，让一半的隐层节点值为0。也就是在向前传播时，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强。</li>
<li>将shuffle参数设置为True, 则训练数据将在每个epoch混洗；<strong>设置early-stopping</strong>（在训练过程中，记录到目前为止最好的验证集精度，当连续10次Epoch（或者更多次）没达到最佳精度时，则可以认为精度不再提高了，则停止训练，将停止之后的权重作为网络的参数），所以early stopping要做的就是在中间点停止迭代过程。我们将会得到一个中等大小的w参数，会得到与L2正则化相似的结果，选择了w参数较小的神经网络。</li>
<li>加Batch Normalization，对隐藏层每个神经元做归一化。即可以想象成在每个隐层上又加了一层BN操作层，它位于x=wu+b激活值获得之后，非线性函数变换之前。<strong>对于每个隐层神经元，把输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。</strong>因为梯度一直都能保持比较大的状态，所以收敛地快，能提升效果。</li>
</ol>
<h5 id="CNN卷积神经网络的层级结构"><a href="#CNN卷积神经网络的层级结构" class="headerlink" title="CNN卷积神经网络的层级结构"></a>CNN卷积神经网络的层级结构</h5><p><strong>CNN的三个基本层：</strong></p>
<p>（1）卷积：对图像元素的矩阵变换，是提取图像特征的方法，多种卷积核可以提取多种特征。一个卷积核覆盖的范围体现<strong>权值共享</strong>。一次卷积运算提取的特征往往是局部的，难以提取出比较全局的特征，因此需要在一层卷积基础上继续做卷积计算，这也就是多层卷积。</p>
<p>（2）池化：池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。池化层用的方法：Max pooling, average pooling。池化层的具体作用：特征不变性，特征降维，一定程度防止过拟合。<br>（3）全连接：softmax分类</p>
<p><strong>CNN所有层级结构：</strong></p>
<ol>
<li><p>数据输入层 （预处理）：</p>
<p>[1去均值：把输入数据各个维度都中心化为0，其目的就是把样本的中心拉回到坐标系原点上。</p>
<p>[2归一化：幅度归一化到同样的范围，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。</p>
<p>[3PCA：用PCA降维</p>
</li>
<li><p>卷积计算层</p>
<p>在这个卷积层，有两个关键操作：<br>　　•    局部关联。每个神经元看做一个滤波器(filter)<br>　　•    窗口(receptive field)滑动， filter对局部数据计算</p>
<p>卷积层遇到的几个名词：<br>　　•    深度/depth（上一层卷积核的个数（滤波器的个数）就是下一层卷积层的深度)<br>　　•    步长/stride （窗口一次滑动的长度）<br>　　•    填充值/zero-padding</p>
</li>
<li><p>ReLU激励层</p>
<p>把卷积层输出结果做非线性映射。CNN采用的激励函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单，但较脆弱。</p>
<p>激励层的实践经验：<br>　　①不要用sigmoid！不要用sigmoid！不要用sigmoid！<br>　　② 首先试RELU，因为快，但要小心点<br>　　③ 如果2失效，请用Leaky ReLU或者Maxout<br>　　④ 某些情况下tanh倒是有不错的结果，但是很少</p>
</li>
<li><p>池化层：max pooling</p>
</li>
<li><p>全连接层</p>
<p>通常在CNN的尾部进行重新拟合，减少特征信息的损失。做完Max Pooling后，我们就会把这些数据“拍平”，丢到Flatten层，然后把Flatten层的output放到full connected Layer里，采用softmax对其进行分类。</p>
</li>
</ol>
<h5 id="CNN卷积神经网络的优点"><a href="#CNN卷积神经网络的优点" class="headerlink" title="CNN卷积神经网络的优点"></a>CNN卷积神经网络的优点</h5><p>局部连接、权值共享</p>
<h5 id="CNN与DNN的区别"><a href="#CNN与DNN的区别" class="headerlink" title="CNN与DNN的区别"></a>CNN与DNN的区别</h5><p>DNN的输入是向量形式，并未考虑到平面的结构信息，在图像和NLP领域这一结构信息尤为重要，例如识别图像中的数字，同一数字与所在位置无关（换句话说任一位置的权重都应相同），CNN的输入可以是tensor，例如二维矩阵，通过filter获得局部特征，较好的保留了平面结构信息。</p>
<h4 id="6-LDA主题模型"><a href="#6-LDA主题模型" class="headerlink" title="6.LDA主题模型"></a>6.LDA主题模型</h4><p>LDA主题模型，它能够<strong>将文档集中每篇文档的主题以概率分布的形式给出</strong>。从而通过分析一些文档抽取出它们的主题分布出来后，便能够依据主题（分布）进行主题聚类或文本分类。同一时候，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。此外，一篇文档能够包括多个主题，文档中每一个词都由当中的一个主题生成。</p>
<p>人类是怎么生成文档的呢？LDA的这三位作者在原始论文中给了一个简单的样例。比方假设事先给定了这几个主题：Arts、Budgets、Children、Education，然后通过学习训练。获取每一个主题Topic相应的词语。例如以下图所看到的：</p>
<p><img src="http://img.blog.csdn.net/20141117153816148" alt="img"></p>
<p>然后以一定的概率选取上述某个主题，再以一定的概率选取那个主题下的某个单词，不断的反复这两步，终于生成例如以下图所看到的的一篇文章（当中不同颜色的词语分别相应上图中不同主题下的词）：</p>
<p><img src="http://img.blog.csdn.net/20141117154035285" alt="img"></p>
<p>而当我们看到一篇文章后。往往喜欢推測这篇文章是怎样生成的，<strong>我们可能会觉得作者先确定这篇文章的几个主题，然后环绕这几个主题遣词造句，表达成文。</strong></p>
<p>LDA就是要干这事：<strong>依据给定的一篇文档，推測其主题分布（</strong>你计算机给我推測分析网络上各篇文章分别都写了些啥主题，且各篇文章中各个主题出现的概率大小（主题分布）是啥。）</p>
<h4 id="7-Word2Vec-17-语言模型-glove词向量"><a href="#7-Word2Vec-17-语言模型-glove词向量" class="headerlink" title="7.Word2Vec   17.语言模型 || glove词向量"></a>7.Word2Vec  <-> 17.语言模型 || glove词向量</-></h4><h5 id="1-原始CBOW"><a href="#1-原始CBOW" class="headerlink" title="1.原始CBOW"></a>1.原始CBOW</h5><p>Cbow模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。</p>
<p><img src="https://images2015.cnblogs.com/blog/1042406/201707/1042406-20170713152436931-1817493891.png" alt="img"></p>
<p>这样我们这个CBOW的例子里，我们的输入是8个词向量，输出是所有词的softmax概率（训练的目标是期望训练样本特定词对应的softmax概率最大），对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一次DNN前向传播算法并通过softmax激活函数找到概率最大的词对应的神经元即可。</p>
<h5 id="2-改进CBOW"><a href="#2-改进CBOW" class="headerlink" title="2.改进CBOW"></a>2.改进CBOW</h5><p>word2vec也使用了CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。<strong><u>最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。</u></strong></p>
<p>word2vec对这个模型做了改进，<strong>（改进1）首先，对于从输入层到隐藏层的映射</strong>，没有采取神经网络的线性变换加激活函数的方法，<strong><u>而是采用简单的对所有输入词向量求和并取平均的方法。</u></strong>比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12)(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)(5,6,7,8)。<strong><u>由于这里是从多个词向量变成了一个词向量。</u></strong></p>
<p>第二个改进就是<strong><u>(改进2)从隐藏层到输出的softmax层这里的计算量个改进</u></strong>。为了避免要计算所有词的softmax概率，<strong><u>word2vec采样了霍夫曼树来代替从隐藏层到输出softmax层的映射。<u>用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的大小。 而内部节点则起到隐藏层神经元的作用。</u></u></strong>由于我们把之前所有都要计算的从输出softmax层的概率计算变成了一棵二叉霍夫曼树，那么我们的softmax概率计算只需要沿着树形结构进行就可以了。如下图所示，我们可以沿着霍夫曼树从根节点一直走到我们的叶子节点的词w2。</p>
<p><img src="https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png" alt="img"></p>
<p>和之前的神经网络语言模型相比<strong>，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。</strong>在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为”Hierarchical Softmax”。</p>
<p>如何“沿着霍夫曼树一步步完成”呢？<strong><u>在word2vec中，我们采用了二元逻辑回归的方法，即规定沿着左子树走，那么就是负类(霍夫曼树编码1)，沿着右子树走，那么就是正类(霍夫曼树编码0)。判别正类和负类的方法是使用sigmoid函数，</u></strong></p>
<p>这里总结下基于Hierarchical Softmax的CBOW模型算法流程，<strong><u>梯度迭代使用了随机梯度上升法：</u></strong></p>
<p>　　输入：基于CBOW的语料训练样本，词向量的维度大小M，CBOW的上下文大小2c,步长η</p>
<p>　　输出：霍夫曼树的内部节点模型参数θ，所有的词向量w</p>
<p>  　　1. 基于语料训练样本建立霍夫曼树。<br>        　　2. 随机初始化所有的模型参数θ，所有的词向量w<br>              　　3. 进行梯度上升迭代过程</p>
<h5 id="3-改进Skip-Gram"><a href="#3-改进Skip-Gram" class="headerlink" title="3.改进Skip-Gram"></a>3.改进Skip-Gram</h5><p>现在我们先看看基于Skip-Gram模型时，Hierarchical Softmax如何使用。此时输入的只有一个词w,输出的为2c个词向量context(w)。</p>
<p>我们对于训练样本中的每一个词，该词本身作为样本的输入， 其前面的c个词和后面的c个词作为了Skip-Gram模型的输出,，期望这些词的softmax概率比其他的词大。</p>
<p>1.我们需要先将词汇表建立成一颗霍夫曼树。</p>
<p>2.对于从输入层到隐藏层（投影层），这一步比CBOW简单，由于只有一个词，所以，即Xw就是词w对应的词向量。</p>
<p>3.通过梯度上升法来更新我们的θ和Xw，注意这里的Xw周围有2c个词向量，<strong><u>Skip-Gram模型并没有和CBOW模型一样对输入进行迭代更新，而是对2c个输出进行迭代更新。</u></strong></p>
<h5 id="4-glove模型"><a href="#4-glove模型" class="headerlink" title="4.glove模型"></a>4.glove模型</h5><p><strong>我的理解是skip-gram、CBOW每次都是用一个窗口中的信息更新出词向量，但是Glove则是用了全局的信息（共现矩阵），也就是多个窗口进行更新</strong>.</p>
<p><strong>注意，glove模型的代价函数如下，建议记住。具体由来下面会介绍。N</strong>是词汇表的大小，Vi,Vj是单词i和j的词向量，bi,bj是两个标量（作者定义的偏差项，）N是词汇表的大小（共现矩阵维度为N*N）<br>$$<br>J=\sum_{i,j}^{N}{f(X_{i,j})(v^T_iv_j+b_i+b_j-log(X_{i,j}))^2}<br>$$<br>具体权重函数f(x)应该是怎么样的呢？<br>首先应该是非减的，其次当词频过高时，权重不应过分增大，作者通过实验确定权重函数f(x)为：<br>$$<br>f(x)=\begin{cases} (\frac{x}{x_{max}})^{0.75}，x&lt;x_{max}\ 1, x&gt;=x_{max}\end{cases}<br>$$<br>具体方法概述：</p>
<p>首先基于语料库构建词的共现矩阵，然后基于共现矩阵和GloVe模型学习词向量。<strong> 开始 -&gt; 统计共现矩阵 -&gt; 训练词向量 -&gt; 结束</strong></p>
<p>1.统计共现矩阵</p>
<p>设共现矩阵为X，其元素为Xij . Xij的意义为：在整个语料库中，单词i和单词j共同出现在一个窗口中的次数。举个例子，设有语料库，i love you but you love him i am sad</p>
<p>这个小小的语料库只有1个句子，涉及到7个单词：i、love、you、but、him、am、sad。<br>如果我们采用一个窗口宽度为5（左右长度都为2）的统计窗口，那么就有以下窗口内容：</p>
<table>
<thead>
<tr>
<th>窗口标号</th>
<th>中心词</th>
<th>窗口内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>i</td>
<td>i love you</td>
</tr>
<tr>
<td>1</td>
<td>love</td>
<td>i love you but</td>
</tr>
<tr>
<td>2</td>
<td>you</td>
<td>i love you but you</td>
</tr>
<tr>
<td>3</td>
<td>but</td>
<td>love you but you love</td>
</tr>
<tr>
<td>4</td>
<td>you</td>
<td>you but you love him</td>
</tr>
<tr>
<td>5</td>
<td>love</td>
<td>but you love him i</td>
</tr>
<tr>
<td>6</td>
<td>him</td>
<td>you love him i am</td>
</tr>
<tr>
<td>7</td>
<td>i</td>
<td>love him i am sad</td>
</tr>
<tr>
<td>8</td>
<td>am</td>
<td>him i am sad</td>
</tr>
<tr>
<td>9</td>
<td>sad</td>
<td>i am sad</td>
</tr>
</tbody>
</table>
<p>窗口0、1长度小于5是因为中心词左侧内容少于2个，同理窗口8、9长度也小于5。<br>以窗口5为例说明如何构造共现矩阵：中心词为love，语境词为but、you、him、i；则执行：<br>$$<br>X_{love,but}+=1<br>$$</p>
<p>$$<br>X_{love,I}+=1<br>$$</p>
<p>使用窗口将整个语料库遍历一遍，即可得到共现矩阵X。</p>
<p>2.使用glove模型训练词向量:</p>
<p>那么作者为什么这么构造模型呢？首先定义几个符号：<br>$$<br>X_i=\sum_{j=1}^{N}{X_{i,j}}<br>$$<br>其实就是矩阵单词i那一行的和；<br>$$<br>P_{i,k}=\frac{X_{i,k}}{X_i}<br>$$<br>条件概率，表示单词k出现在单词i语境中的概率；<br>$$<br>ratio_{i,j,k}=\frac{P_{i,k}}{p_{j,k}}<br>$$<br>两个条件概率的比率。作者的灵感是这样的：<br>作者发现，ratio{i,j,k}这个指标是有规律的，规律统计在下表：</p>
<table>
<thead>
<tr>
<th><em>r<strong>a</strong>t<strong>i</strong>o**i</em>,<em>j</em>,<em>k</em>的值</th>
<th>单词j,k相关</th>
<th>单词j,k不相关</th>
</tr>
</thead>
<tbody>
<tr>
<td>单词i,k相关</td>
<td>趋近1</td>
<td>很大</td>
</tr>
<tr>
<td>单词i,k不相关</td>
<td>很小</td>
<td>趋近1</td>
</tr>
</tbody>
</table>
<p>思想：假设我们已经得到了词向量，如果我们用词向量Vi,Vj,Vk通过某种函数计算ratioi,jk 能够同样得到这样的规律的话，就意味着我们词向量与共现矩阵具有很好的一致性，也就说明我们的词向量中蕴含了共现矩阵中所蕴含的信息。设用词向量Vi,Vj,Vk计算ratioi,j,k的函数为g(vi,vj,vk)（我们先不去管具体的函数形式），那么应该有：<br>$$<br>\frac{P_{i,k}}{p_{j,k}}=ratio_{i,j,k}=g(v_i,v_j,v_k)<br>$$<br>即：<br>$$<br>\frac{P_{i,k}}{p_{j,k}}=g(v_i,v_j,v_k)<br>$$<br>即二者应该尽可能地接近；<br>很容易想到用二者的差方来作为代价函数：<br>$$<br>J=\sum{<em>{i,j,k}^{N}(\frac{P</em>{i,k}}{p_{j,k}}-g(v_i,v_j,v_k))^2}<br>$$<br>但是仔细一看，模型中包含3个单词，这就意味着要在N<em>N</em>N的复杂度上进行计算，太复杂了，最好能再简单点。</p>
<p>作者的脑洞是这样的：</p>
<ol>
<li>要考虑单词i和单词j之间的关系，那g(vi,vj,vk)中大概要有这么一项吧：Vi-Vj；嗯，合理，在线性空间中考察两个向量的相似性，不失线性地考察，那么vi−vj大概是个合理的选择；</li>
<li>ratioi,j,k是个标量，那么g(vi,vj,vk)最后应该是个标量啊，虽然其输入都是向量，那內积应该是合理的选择，于是应该有这么一项吧：(Vi-Vj)^T(Vk) (行向量*列向量)</li>
<li>然后作者又往(vi−vj)^T * vk的外面套了一层指数运算exp(). 套上之后，我们的目标是让以下公式尽可能地成立：</li>
</ol>
<p>$$<br>\frac{P_{i,k}}{p_{j,k}}=g(v_i,v_j,v_k)<br>$$</p>
<p>即：<br>$$<br>\frac{P_{i,k}}{p_{j,k}}=exp((V_i-Vj)^T<em>V_k)<br>$$<br>即：<br>$$<br>\frac{P_{i,k}}{p_{j,k}}=exp((V_i^T</em>V_k-V_j^T<em>V_k)<br>$$<br>即：<br>$$<br>\frac{P_{i,k}}{p_{j,k}}=\frac{exp(V_i^T</em>V_k)}{exp(V_j^T<em>V_k)}<br>$$<br>然后就发现找到简化方法了：只需要让上式分子对应相等，分母对应相等，即：<br>$$<br>P_{i,j}=exp(V_i^T</em>V_k)<br>$$<br>两边取个对数：<br>$$<br>log(P_{i,j})=V_i^T<em>V_k<br>$$<br>那么代价函数就可以化简为：<br>$$<br>J=\sum_{i,j}^{N}{log(P_{i,j}-V^T_iV_j)^2}<br>$$<br>现在只需要在N</em>N的复杂度上进行计算，而不是N<em>N</em>N，现在关于为什么第3步中，外面套一层exp()就清楚了，正是因为套了一层exp()，才使得差形式变成商形式，进而等式两边分子分母对应相等，进而简化模型。<br>现将代价函数中的条件概率展开：<br>$$<br>log(P_{i,j})=V^T_iV_j<br>$$<br>即为：<br>$$<br>log(X_{i,j})-log(X_{i})=V^T_iV_j<br>$$<br>将其变为：<br>$$<br>log(X_{i,j})=V^T_iV_j+b_i+b_j<br>$$<br>于是代价函数就变成了：<br>$$<br>J=\sum_{i,j}^{N}{(v^T_iv_j+b_i+b_j-log(X_{i,j}))^2}<br>$$<br>然后基于出现频率越高的词对儿权重应该越大的原则，在代价函数中添加权重项，于是代价函数进一步完善：<br>$$<br>J=\sum_{i,j}^{N}{f(X_{i,j})(v^T_iv_j+b_i+b_j-log(X_{i,j}))^2}<br>$$</p>
<h4 id="8-WordEmbedding"><a href="#8-WordEmbedding" class="headerlink" title="8.WordEmbedding"></a>8.WordEmbedding</h4><p>如果将<strong>word</strong>看作文本的最小单元，可以将<strong>Word Embedding</strong>理解为一种<strong>映射</strong>，其过程是：<strong>将文本空间中的某个word，通过一定的方法，映射或者说嵌入（embedding）到另一个数值向量空间</strong>（之所以称之为<strong>embedding</strong>，是因为这种表示方法往往伴随着一种<strong>降维</strong>的意思。</p>
<p><strong>WordEmbedding的输入：</strong></p>
<p><strong>Word Embedding</strong>的输入是原始文本中的一组不重叠的词汇，假设有句子：<strong>apple on a apple tree</strong>。那么为了便于处理，我们可以将这些词汇放置到一个<strong>dictionary</strong>里，例如：<strong>[“apple”, “on”, “a”, “tree”]</strong>，这个<strong>dictionary</strong>就可以看作是<strong>Word Embedding</strong>的一个输入。</p>
<p><strong>WordEmbedding的输出：</strong></p>
<p><strong>Word Embedding</strong>的输出就是每个<strong>word</strong>的向量表示。</p>
<p><strong>WordEmbedding的类型：</strong></p>
<ul>
<li>基于频率的<strong>Word Embedding</strong>（Frequency based embedding）</li>
<li>基于预测的<strong>Word Embedding</strong>（Prediction based embedding）</li>
</ul>
<h5 id="基于频率的Word-Embedding"><a href="#基于频率的Word-Embedding" class="headerlink" title="基于频率的Word Embedding"></a>基于频率的Word Embedding</h5><h5 id="8-1TF-IDF"><a href="#8-1TF-IDF" class="headerlink" title="8.1TF-IDF"></a>8.1TF-IDF</h5><p>词频（TF） = 某个词在文章中的出现次数</p>
<p>逆文档频率（IDF） = log（语料库的文档总数/包含该词的文档总数+1）</p>
<h5 id="8-2共现矩阵-lt-gt-7-4-glove模型"><a href="#8-2共现矩阵-lt-gt-7-4-glove模型" class="headerlink" title="8.2共现矩阵  &lt;&gt; 7.4 glove模型"></a>8.2共现矩阵  &lt;&gt; 7.4 glove模型</h5><p>glove模型，定义一个比如大小为5的共现窗口</p>
<h5 id="基于预测的Word-Embedding"><a href="#基于预测的Word-Embedding" class="headerlink" title="基于预测的Word Embedding"></a>基于预测的Word Embedding</h5><p>词的表示中如果蕴含了上下文信息，那么将会更加接近自然语言的本质；并且，由于相似的词有相似的表示方法，甚至可以进行一些运算，例如：<strong>人类-男人=女人</strong>。但是，上述讨论中，有一个很大的缺陷，那就是词的向量表示维度过大，一个词要用大量其余的词来表示，为后续运算带来了很大的麻烦。因此，我们需要找到一种更好的表示方法，这种方法需要满足如下两点要求：</p>
<p><strong>1.携带上下文信息</strong></p>
<p><strong>2.词的表示是稠密的</strong></p>
<p>方法：CBOW、Skip-Gram</p>
<h4 id="9-tf-idf"><a href="#9-tf-idf" class="headerlink" title="9.tf-idf"></a>9.tf-idf</h4><h5 id="tf"><a href="#tf" class="headerlink" title="tf:"></a>tf:</h5><p><strong>词频（TF） = 某个词在文章中的出现次数</strong></p>
<p>文章有长短之分，为了便于不同文章的比较,做”词频”标准化。</p>
<p>　　词频（TF） = 某个词在文章中的出现次数 / 文章总词数</p>
<p>或者 词频（TF） = 某个词在文章中的出现次数 / 拥有最高词频的词的次数</p>
<h5 id="Idf"><a href="#Idf" class="headerlink" title="Idf:"></a>Idf:</h5><p><strong>逆文档频率（IDF） = log（语料库的文档总数/包含该词的文档总数+1）</strong></p>
<p>TF-IDF = 词频（TF) * 逆文档频率（IDF）</p>
<h5 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h5><p>TF-IDF算法的优点是简单快速，结果比较符合实际情况。</p>
<h5 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h5><p>单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。</p>
<p>（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重）</p>
<h4 id="10-SVM"><a href="#10-SVM" class="headerlink" title="10.SVM"></a>10.SVM</h4><h5 id="SVM原理"><a href="#SVM原理" class="headerlink" title="SVM原理"></a>SVM原理</h5><p>基于训练样本D 在二维空间中找到一个符合这样条件的超平面来分开二类样本。这个超平面离两类样本都足够远，也就是使得“间隔”最大。即最终确定的参数w和b,使得r最大。即要：</p>
<p>min (1/2)||w||^2</p>
<p>实际中，对某个实际问题函数来寻找一个合适的空间进行映射是非常困难的，幸运的是，在计算中发现，我们需要的只是两个向量在新的映射空间中的内积结果，而映射函数到底是怎么样的其实并不需要知道。这就需要引入了核函数的概念。<strong>核函数是这样的一种函数：</strong>仍然以二维空间为例，假设对于变量x和y，将其映射到新空间的映射函数为φ，则在新空间中，二者分别对应φ(x)和φ(y)，他们的内积则为&lt;φ(x),φ(y)&gt;。我们令函数Kernel(x,y)=&lt;φ(x),φ(y)&gt;=k(x,y)，<strong>可以看出，函数Kernel(x,y)是一个关于x和y的函数！而与φ无关！</strong>这是一个多么好的性质！我们再也不用管φ具体是什么映射关系了，只需要最后计算Kernel(x,y)就可以得到他们在高维空间中的内积，这样就可以直接带入之前的支持向量机中计算！</p>
<p>RBF是最常用的核函数，RBF核函数可以将维度扩展到无穷维的空间，RBF对应的是泰勒级数展开，在泰勒级数中，一个函数可以分解为无穷多个项的加和，其中，每一个项可以看做是对应的一个维度，这样，原函数就可以看做是映射到了无穷维的空间中. 通过计算间隔和松弛变量等的最大化，可以对问题进行求解。</p>
<h5 id="逻辑回归和SVM区别-1"><a href="#逻辑回归和SVM区别-1" class="headerlink" title="逻辑回归和SVM区别"></a>逻辑回归和SVM区别</h5><p>相同点:</p>
<ol>
<li>都是分类算法</li>
<li>都是监督学习算法</li>
<li>都是判别模型</li>
<li>都能通过核函数方法针对非线性情况分类</li>
<li>目标都是找一个分类超平面</li>
<li>都能减少离群点的影响</li>
</ol>
<p>不同点:</p>
<ol>
<li><strong>损失函数不同，逻辑回归是对数损失函数，svm是hinge loss，自带L2正则项</strong></li>
<li><strong>逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。</strong></li>
<li>逻辑回归对概率建模，svm对分类超平面建模</li>
<li>逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有</li>
<li>逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响</li>
<li>逻辑回归是统计方法，svm是几何方法</li>
</ol>
<h5 id="SVM的参数"><a href="#SVM的参数" class="headerlink" title="SVM的参数"></a>SVM的参数</h5><p>SVM模型有两个非常重要的参数C与gamma。</p>
<p>1.C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差</p>
<p>2.gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，σ越小（σ^2是正太分布的方差，描述数据的离散程度），数据分布越集中，曲线越瘦高，<strong>那么会造成只作用于支持向量样本附近</strong>，<strong>对于未知样本分类效果很差</strong>，就会存在训练准确率很高而测试准确率不高的可能，也就是过训练。 而若gamma太小，会造成平滑效应太大，无法在训练集上得到比较好的结果，更不用说测试集准确率。</p>
<h4 id="11-LexRank-、TextRank"><a href="#11-LexRank-、TextRank" class="headerlink" title="11.LexRank 、TextRank"></a>11.LexRank 、TextRank</h4><h5 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank:"></a>PageRank:</h5><p>对于某个互联网网页A来说，该网页PageRank的计算基于下面两个基本如果：<br>     数量如果：在Web图模型中，<strong>如果一个页面节点接收到的其它网页指向的入链数量越多</strong>，那么这个页面越重要。<br>     质量如果：指向页面A的入链质量不同，<strong>质量高的页面会通过链接向其它页面传递很多其它的权重</strong>。所以越是质量高的页面指向页面A，则页面A越重要。</p>
<p>PageRank的计算充分利用了两个如果：数量如果和质量如果。过程例如以下：<br>      <strong>1）在初始阶段：</strong>网页通过链接关系构建起Web图，每一个页面设置同样的PageRank值，通过若干轮的计算，会得到每一个页面所获得的终于PageRank值。随着每一轮的计算进行，网页当前的PageRank值会不断得到更新。</p>
<p>​      <strong>2）在一轮中更新页面PageRank得分的计算方法：</strong>在一轮更新页面PageRank得分的计算中，<strong>每一个页面将其当前的PageRank值平均分配到本页面包括的出链上</strong>，这样每一个链接即获得了对应的权值。<strong>而每一个页面将全部指向本页面的入链所传入的权值求和，就可以得到新的PageRank得分</strong>。当每一个页面都获得了更新后的PageRank值，就完毕了一轮PageRank计算。 </p>
<p><strong>Case1.网页都有出入链</strong></p>
<p><img src="https://img-blog.csdn.net/20180517202555969?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>此种情况下的网页A的PR值计算公式为：</p>
<p><img src="https://img-blog.csdn.net/20180517203053419?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p><strong>Case2存在没有出链的网页</strong>:</p>
<p><img src="https://img-blog.csdn.net/20180517204233919?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>网页C是没有出链。因为C没有出链，所以对A,B,D网页没有PR值的贡献。PageRank算法的策略：从数学上考虑，为了满足Markov链，设定C对A,B,C,D都有出链（也对他自己也出链~）。你也可以理解为：没有出链的网页，我们强制让他对所有的网页都有出链，即让他对所有网页都有PR值贡献。</p>
<p>此种情况PR(A)的计算公式：</p>
<p><img src="https://img-blog.csdn.net/20180517204803765?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p><strong>Case3存在只对自己有出链的网页：</strong></p>
<p><img src="https://img-blog.csdn.net/2018051721101642?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>C是只对自己出链的网页。</p>
<p>此时访问C时，不会傻乎乎的停留在C页面，一直点击C-Link循环进入C，即C网页只对自己的网页PR值有贡献。正常的做法是，进入C后，存在这种情况：在地址输入栏输入A/B/C/D的URL地址，然后跳转到A/B/C/D进行浏览，这就是PageRank算法解决这种情况的策略：设定存在一定概率为α，用户在地址栏输入A/B/C/D地址，然后从C跳转到A/B/C/D进行浏览。</p>
<p>此时PR(A)的计算公式为：</p>
<p><img src="https://img-blog.csdn.net/20180517211424276?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">，一般取值α=0.85</p>
<p><strong>一般情况下，一个网页的PR值计算公式为：</strong></p>
<p><img src="https://img-blog.csdn.net/20180517211736459?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI0MjE4NTI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>注：Mpi是有出链到pi的所有网页集合，L(pj)是有网页pj的出链总数，N是网页总数，α一般取值为0.85（阻尼系数）</p>
<p>所有网页PR值同时计算需要迭代计算：一直迭代计算，停止直到下面2情况之一发生：<strong><u>每个网页的PR值前后误差dleta_pr小于自定义误差阈值，或者迭代次数超过了自定义的迭代次数阈值</u></strong></p>
<h5 id="TextRank"><a href="#TextRank" class="headerlink" title="TextRank:"></a>TextRank:</h5><p>基于TextRank的自动文摘属于自动摘录，通过选取文本中重要度较高的句子形成文摘，其主要步骤如下：</p>
<p>（1）预处理：将输入的文本或文本集的内容分割成句子得<img src="https://img.mukewang.com/5b3f329300018c2001690021.jpg" alt="https://img3.mukewang.com/5b3f329300018c2001690021.jpg">，构建图G =（V,E），其中V为句子集，对句子进行分词、去除停止词，得<img src="https://img1.mukewang.com/5b3f32a00001d9ab01800021.jpg" alt="https://img.mukewang.com/5b3f32a00001d9ab01800021.jpg">，其中<img src="https://img1.mukewang.com/5b3f32a90001906800570021.jpg" alt="https://img1.mukewang.com/5b3f32a90001906800570021.jpg">是保留后的候选关键词。</p>
<p>（2）句子相似度计算：构建图G中的边集E，基于句子间的内容覆盖率，给定两个句子<img src="https://img4.mukewang.com/5b3f32b3000101fc00550021.jpg" alt="https://img4.mukewang.com/5b3f32b3000101fc00550021.jpg">，采用如下公式进行计算：<br><img src="https://img1.mukewang.com/5b3f32db00019d8302930055.jpg" alt="https://img3.mukewang.com/5b3f32db00019d8302930055.jpg"><br>　　若两个句子之间的相似度大于给定的阈值0.01，就认为这两个句子语义相关并将它们连接起来，即边的权值；<img src="https://img2.mukewang.com/5b3f32ee00012ce201720022.jpg" alt="https://img4.mukewang.com/5b3f32ee00012ce201720022.jpg"></p>
<p>（3）句子权重计算：根据公式，迭代传播权重计算各句子的得分；</p>
<p><img src="https://img-blog.csdnimg.cn/2018110410374392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FpYW45OQ==,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>和pageRank的公式相比，基本上就是把原来对应边的部分添加了权重，边的数量和改成了权重和，很好理解。</p>
<p>（4）抽取文摘句：将（3）得到的句子得分进行倒序排序，抽取重要度最高的T个句子作为候选文摘句。</p>
<p>（5）形成文摘：根据字数或句子数要求，从候选文摘句中抽取句子组成文摘。</p>
<p>使用 TextRank 算法计算图中各节点的得分时，同样需要给图中的节点指定任意的初值，通常都设为1。然后递归计算直到收敛，即图中任意一点的误差率小于给定的极限值时就可以达到收敛，一般该极限值取 0.0001。</p>
<p>其它计算相似度的方法有：基于编辑距离，基于语义词典，余弦相似度等。</p>
<h4 id="12-VSM向量空间模型"><a href="#12-VSM向量空间模型" class="headerlink" title="12.VSM向量空间模型"></a>12.VSM向量空间模型</h4><p><img src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Vector_space_model.jpg/250px-Vector_space_model.jpg" alt="img"></p>
<p>我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。<br>于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。<br>Document = {term1, term2, …… ,term N}<br>Document Vector = {weight1, weight2, …… ,weight N}<br>同样我们把查询语句看作一个简单的文档，也用向量来表示。<br>Query = {term1, term 2, …… , term N}<br>Query Vector = {weight1, weight2, …… , weight N}</p>
<p>我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。</p>
<p>我们认为两个向量之间的夹角越小，相关性越大。<br>所以我们计算夹角的余弦值作为相关性的打分，夹角越小，余弦值越大，打分越高，相关性越大。</p>
<p>我们只要比较下图中的α,θ的余弦值的大小，余弦值越大，相似度越高。公式如下：</p>
<p><img src="https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D394/sign=20b5db49b7a1cd1101b674298d13c8b0/ac4bd11373f0820282c6ae4646fbfbedab641b76.jpg" alt="img"></p>
<p>这里的 <img src="https://gss1.bdstatic.com/-vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D38/sign=5ba753a7306d55fbc1c6702e6d22952c/f2deb48f8c5494ee8d2f306020f5e0fe99257ebd.jpg" alt="img"> 分别代表向量A和B的各分量。</p>
<h4 id="14-关联规则"><a href="#14-关联规则" class="headerlink" title="14.关联规则"></a>14.关联规则</h4><p>如何来度量一个规则是否够好？有两个量，<strong>置信度(Confidence)和支持度(Support)</strong>。假设有如下表的购买记录。</p>
<p><strong>置信度表示了这条规则有多大程度上值得可信</strong>。设条件的项的集合为A,结果的集合为B。<strong>置信度计算在A中，同时也含有B的概率。即Confidence(A==&gt;B)=P(B|A)。</strong>例 如计算”如果Orange则Coke”的置信度。由于在含有Orange的4条交易中，仅有2条交易含有Coke.其置信度为0.5。</p>
<p><strong>支持度计算在所有的交易集中，既有A又有B的概率。</strong>例如在5条记录中，既有Orange又有Coke的记录有2条。则此条规则的支持度为2/5=0.4。现在这条规则可表述为，如果一个顾客购买了Orange,则有50%的可能购买Coke。而这样的情况（即买了Orange会再买Coke）会有40%的可能发生。</p>
<h5 id="算法原理和过程"><a href="#算法原理和过程" class="headerlink" title="算法原理和过程"></a>算法原理和过程</h5><p>1.找出所有的频繁项目集，并计算其支持度。这些项集出现的频繁性至少和预定义的最小支持度一 样。</p>
<p>2.然后由频集产生强关联规则，这些规则必须满足最小支持度和最小可信度。</p>
<p>3.然后使用第1步找到的频繁项目集产生期望的规则，产生只包含集合的项的所有规则，其中 每一条规则的右部只有一项。</p>
<h4 id="15-决策树"><a href="#15-决策树" class="headerlink" title="15.决策树"></a>15.决策树</h4><p>首先，在了解树模型之前，自然想到树模型和线性模型有什么区别呢？其中最重要的是，树形模型是一个一个特征进行处理，之前线性模型是所有特征给予权重相加得到一个新的值。决策树与逻辑回归的分类区别也在于此，<strong>逻辑回归是将所有特征变换为概率后，通过大于某一概率阈值的划分为一类，小于某一概率阈值的为另一类；而决策树是对每一个特征做一个划分。</strong>另外逻辑回归只能找到线性分割（输入特征x与logit之间是线性的，除非对x进行多维映射），而决策树可以找到非线性分割。</p>
<p><strong>而树形模型更加接近人的思维方式，可以产生可视化的分类规则，产生的模型具有可解释性（可以抽取规则）。</strong>树模型拟合出来的函数其实是分区间的阶梯函数。</p>
<p><strong><u>决策树学习：采用自顶向下的递归的方法，基本思想是以信息熵为度量构造一棵熵值下降最快的树，到叶子节点处熵值为0（叶节点中的实例都属于一类）。</u></strong></p>
<p><strong>决策树的生成：</strong></p>
<p>决策树思想，实际上就是寻找最纯净的划分方法<strong>，这个最纯净在数学上叫纯度，纯度通俗点理解就是目标变量要分得足够开（y=1的和y=0的混到一起就会不纯）。另一种理解是分类误差率的一种衡量。实际决策树算法往往用到的是，纯度的另一面也即不纯度，下面是不纯度的公式。不纯度的选取有多种方法，每种方法也就形成了不同的决策树方法，比如</strong>ID3算法<strong>使用信息增益作为不纯度；</strong>C4.5算法<strong>使用信息增益率作为不纯度；</strong>CART算法使用<strong>基尼系数</strong>作为不纯度。</p>
<p><strong>决策树要达到寻找最纯净划分的目标要干两件事，建树和剪枝</strong></p>
<p><strong>建树：</strong></p>
<p><strong>（1）如何按次序选择属性</strong>（最小熵，最大增益，最大增益率，最小基尼系数）</p>
<p>也就是首先树根上以及树节点是哪个变量呢？这些变量是从最重要到次重要依次排序的，那怎么衡量这些变量的重要性呢？　<strong>ID3算法</strong>用的是信息增益，<strong>C4.5**</strong>算法<strong>用信息增益率；</strong>CART<strong>**算法</strong>使用基尼系数。决策树方法是会把每个特征都试一遍，然后选取那个，能够使分类分的最好的特征，也就是说将A属性作为父节点，产生的纯度增益（GainA）要大于B属性作为父节点，则A作为优先选取的属性。</p>
<p><strong>2）</strong> <strong>如何分裂训练数据（对每个属性选择最优的分割点）</strong></p>
<p>如何分裂数据也即分裂准则是什么？依然是通过不纯度来分裂数据的，通过比较划分前后的不纯度值，来确定如何分裂。</p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554384143952.png" alt="1554384143952"></p>
<p><img src="C:\Users\norkey\AppData\Roaming\Typora\typora-user-images\1554384301056.png" alt="1554384301056"></p>
<p>信息熵表示不确定性</p>
<p>信息熵越小，表示不确定性越低，即稳定性越高，因为右侧有一类占了70%，这个类别影响力更大</p>
<p>CART分类树在很多书籍和资料中介绍比较多，但是再次强调GDBT中使用的是回归树。作为对比，先说分类树，我们知道CART是二叉树，CART分类树在每次分枝时，是穷举每一个feature的每一个阈值，根据GINI系数找到使不纯性降低最大的的feature以及其阀值，然后按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝，每个分支包含符合分支条件的样本。用同样方法继续分枝直到该分支下的所有样本都属于统一类别，或达到预设的终止条件，若最终叶子节点中的类别不唯一，则以多数人的类别作为该叶子节点的性别。回归树总体流程也是类似，不过在每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但<strong>衡量最好的标准不再是GINI系数，而是最小化均方差–即（每个人的年龄-预测年龄）^2 的总和 / N</strong>，或者说是每个人的预测误差平方和 除以 N。这很好理解，被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最靠谱的分枝依据。分枝直到每个叶子节点上人的年龄都唯一（这太难了）或者达到预设的终止条件（如叶子个数上限），若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p>
<h4 id="16-K-Means"><a href="#16-K-Means" class="headerlink" title="16.K-Means"></a>16.K-Means</h4><p><strong>核心思想：</strong>由用户指定k个初始质心（initial centroids)，以作为聚类的类别（cluster），重复迭代直至算法收敛。</p>
<p><strong>算法流程</strong>：</p>
<p>选取k个初始质心（作为初始cluster）；<br>repeat：<br>对每个样本点，计算得到距其最近的质心，将其类别标为该质心所对应的cluster；<br>重新计算k个cluser对应的质心；<br>until 质心不再发生变化</p>
<p><strong>缺点</strong>：</p>
<p>k-Means是局部最优的，容易受到初始质心的影响。</p>
<h4 id="17-语言模型-lt-gt-7-Word2Vec-CBOW-lt-gt-N-gram-lt-gt-朴素贝叶斯"><a href="#17-语言模型-lt-gt-7-Word2Vec-CBOW-lt-gt-N-gram-lt-gt-朴素贝叶斯" class="headerlink" title="17.语言模型 &lt;&gt; 7.Word2Vec(CBOW) &lt;&gt; N-gram &lt;&gt; 朴素贝叶斯"></a>17.语言模型 &lt;&gt; 7.Word2Vec(CBOW) &lt;&gt; N-gram &lt;&gt; 朴素贝叶斯</h4><p>简单的说，<strong>语言模型</strong> （Language Model） 是用来计算一个句子出现概率的模型，假设句子 <img src="http://latex.codecogs.com/gif.latex?W%3D%28w_1%2Cw_2%2C...%2Cw_n%29" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> ,其中 <img src="http://latex.codecogs.com/gif.latex?w_i" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> 代表句子中的第 <img src="http://latex.codecogs.com/gif.latex?i" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> 个词语，则语句 W 以该顺序出现的概率可以表示为：p(S)=p(w1,w2,w3,w4,w5,…,wn) = p(w1)p(w2|w1)p(w3|w1,w2)…p(wn|w1,w2,…,wn-1)//链规则。   那么，如何计算p(wi|w1,w2,…,wi-1)呢？最简单、直接的方法是直接计数做除法，如下：</p>
<p>p(wi|w1,w2,…,wi-1) = p(w1,w2,…,wi-1,wi) / p(w1,w2,…,wi-1)  但是，这里面临两个重要的问题：数据稀疏严重；参数空间过大，无法实用。</p>
<h5 id="1-朴素贝叶斯"><a href="#1-朴素贝叶斯" class="headerlink" title="1.朴素贝叶斯"></a>1.朴素贝叶斯</h5><p>贝叶斯公式：<img src="https://images2015.cnblogs.com/blog/716934/201509/716934-20150901131811606-1633545801.jpg" alt="img"></p>
<p>判断一条微信朋友圈是不是广告。前置条件是：我们已经拥有了一个平日广大用户的朋友圈内容库，这些朋友圈当中，如果真的是在做广告的，会被“热心网友”打上“广告”的标签，我们要做的是把所有内容分成一个一个词，每个词对应一个维度，构建一个高维度空间 (别担心，这里未出现向量计算)。当出现一条新的朋友圈new_post，我们也将其分词，然后投放到朋友圈词库空间里。</p>
<p>这里的X表示多个特征（词）x1,x2,x3…组成的特征向量。</p>
<p>P(ad|x)表示：已知朋友圈内容而这条朋友圈是广告的概率。</p>
<p>利用贝叶斯公式，进行转换：</p>
<p>P(ad|X) = p(X|ad) p(ad) / p(X)</p>
<p>P(not-ad | X) = p(X|not-ad)p(not-ad) / p(X)</p>
<p>比较上面两个概率的大小，如果p(ad|X) &gt; p(not-ad|X),则这条朋友圈被划分为广告，反之则不是广告。</p>
<p>朴素贝叶斯假设：<strong>如果认为每个词都是独立的特征，那么朋友圈内容向量可以展开为分词(x1,x2,x3…xn)，</strong>因此有了下面的公式推导：</p>
<p>　　P(ad|X)*p(x) = p(X|ad)p(ad) = p(x1, x2, x3, x4…xn | ad) p(ad),   p(x)对于所有类别为常数</p>
<p><strong>假设所有词相互条件独立</strong>，则进一步拆分：</p>
<p>　　P(ad|X)*p(x) = p(x1|ad)p(x2|ad)p(x3|ad)…p(xn|ad) p(ad)</p>
<p>至此，P(xi|ad)很容易求解，P(ad)为词库中广告朋友圈占所有朋友圈（训练集）的概率。虽然现实中，一条朋友圈内容中，相互之间的词不会是相对独立的，因为我们的自然语言是讲究上下文的,所以由朴素贝叶斯引出n-garm</p>
<h5 id="2-n-gram"><a href="#2-n-gram" class="headerlink" title="2.n-gram"></a>2.n-gram</h5><p>N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）。 </p>
<p><img src="https://img-blog.csdn.net/20180507155519645?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NvbmdiaW54dQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>
<p>N-gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有 Bi-gram (N=2) 和 Tri-gram (N=3)，一般已经够用了。例如在上面这句话里，我可以分解的 Bi-gram 和 Tri-gram ：</p>
<p>Bi-gram : {I, love}, {love, deep}, {deep, learning} </p>
<p>Tri-gram : {I, love, deep}, {love, deep, learning}</p>
<p><strong>N-gram中概率计算:</strong> </p>
<p>假设我们有一个由n个词组成的句子S=(w1,w2,⋯,wn) 如何衡量它的概率呢？让我们假设，每一个单词wi都要依赖于从第一个单词w1到它之前一个单词wi−1的影响： </p>
<p>p(S)=p(w1w2⋯wn)=p(w1)p(w2|w1)⋯p(wn|wn−1⋯w2w1)不过这个衡量方法有两个缺陷：</p>
<ul>
<li><strong>参数空间过大</strong>，概率 p(wn|wn−1⋯w2w1)的参数有O(n) 个。</li>
<li><strong>数据稀疏严重</strong>，词同时出现的情况可能没有，组合阶数高时尤其明显。</li>
</ul>
<p><strong>为了解决第一个问题(参数空间过大)</strong>，我们引入<strong>马尔科夫假设（Markov Assumption）</strong>：<strong>一个词的出现仅与它之前的若干个词有关</strong>。 </p>
<p>如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为 Bi-gram： p(S)=p(w1w2⋯wn)=p(w1)p(w2|w1)⋯p(wn|wn−1)<br>如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为 Tri-gram： p(S)=p(w1w2⋯wn)=p(w1)p(w2|w1)⋯p(wn|wn−1wn−2)</p>
<p>N-gram的 N可以取很高，然而现实中一般 bi-gram 和 tri-gram 就够用了。</p>
<p>那么，如何计算其中的每一项条件概率 p(wn|wn−1⋯w2w1)呢？答案是极大似然估计（Maximum Likelihood Estimation，MLE），说人话就是数频数：<br>p(Wn|Wn−1)=C(Wn−1Wn) / C(Wn−1)</p>
<p>p(Wn|Wn−1Wn−2)=C(Wn−2Wn−1Wn)C(Wn−1Wn)<br>p(Wn|Wn−1⋯W2W1)=C(W1W2⋯Wn)C(W1W2⋯Wn−1)</p>
<p><strong>为了解决第二个问题（数据稀疏严重）</strong>，因此，我们要进行<strong>数据平滑（data Smoothing）</strong>，数据平滑的目的有两个：一个是使所有的N-gram概率之和为1，使所有的n-gram概率都不为0。它的本质，是重新分配整个概率空间，使已经出现过的n-gram的概率降低，补充给未曾出现过的n-gram。方法有拉普拉斯平滑，即强制让所有的n-gram至少出现一次，只需要在分子和分母上分别做加法即可。</p>
<h5 id="3-神经网络语言模型—CBOW是在此基础上进行改进的"><a href="#3-神经网络语言模型—CBOW是在此基础上进行改进的" class="headerlink" title="3.神经网络语言模型—CBOW是在此基础上进行改进的"></a>3.神经网络语言模型—CBOW是在此基础上进行改进的</h5><p><img src="https://images2015.cnblogs.com/blog/743682/201606/743682-20160604170443024-2120174326.png" alt="img"></p>
<p>来看 NNLM（神经网络语言模型） 构造 ,首先，引入了<strong>词向量</strong>的概念，即将词表中的词语 <img src="http://latex.codecogs.com/gif.latex?w" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> 表示为一个固定长度为 M向量的形式  <img src="http://latex.codecogs.com/gif.latex?C%28w%29" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> ，（m 为人工定义的词向量的长度），这样整个词表可以用 一个 <img src="http://latex.codecogs.com/gif.latex?m%20%5Ctimes%20%7CV%7C" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> 的矩阵表示。现在找到词 wt的上下文 context(wt)，这里 Bengio 设定的<strong>上下文 context(wt)是词wt 的前 <img src="http://latex.codecogs.com/gif.latex?n-1" alt="This is the rendered form of the equation. You can not edit this directly. Right click will give you the option to save the image, and in most browsers you can drag the image onto your desktop or another program."> 个词语。</strong></p>
<p><strong>神经网络的输入层：</strong>把这 n-1个词语的词向量首尾相接的拼起来，形成维度为 (n−1)m**的向量来当做神经网络的输入，所以 NNLM 输入层的大小已经确定为 (n−1)m </p>
<p><strong>隐层：</strong>隐层的规模就是人工指定了</p>
<p><strong>神经网络的输出层：</strong>输出层的大小为  |V|(词典的大小)，因为共有 |V|个词语，所以输出层维度为 |V| ， wt在词表 V中的下标对应的维度就是映射 wt的得分,而 softmax 正好可以把该得分归一化为概率。</p>
<h5 id="4-CBOW-Word2Vec"><a href="#4-CBOW-Word2Vec" class="headerlink" title="4.CBOW (Word2Vec)"></a>4.CBOW (Word2Vec)</h5><p>对比原始神经网络语言模型使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。输入是上下两个窗口。</p>
<h4 id="18-马尔可夫链"><a href="#18-马尔可夫链" class="headerlink" title="18.马尔可夫链"></a>18.马尔可夫链</h4><h4 id="19-向量空间模型"><a href="#19-向量空间模型" class="headerlink" title="19.向量空间模型"></a>19.向量空间模型</h4><h4 id="20-RNN"><a href="#20-RNN" class="headerlink" title="20.RNN"></a>20.RNN</h4><h5 id="1-为什么具有记忆功能？"><a href="#1-为什么具有记忆功能？" class="headerlink" title="1.为什么具有记忆功能？"></a>1.为什么具有记忆功能？</h5><p>因为此神经元结点上一时刻隐层的状态参与到了这个时刻的计算过程中。</p>
<p>在RNN中，神经元的输出可以在下一个时间戳直接作用到自身，<strong>即第i层神经元在m时刻的输入，除了（i-1）层神经元在该时刻的输出外，还包括其自身在（m-1）时刻的输出</strong>！表示成图就是这样的：</p>
<p><img src="https://images2015.cnblogs.com/blog/1108812/201702/1108812-20170223164229007-1606932794.png" alt="img"></p>
<p>我们可以看到在隐含层节点之间增加了互连。为了分析方便，我们常将RNN在时间上进行展开，得到如图6所示的结构</p>
<p><img src="https://images2015.cnblogs.com/blog/1108812/201702/1108812-20170223164433570-1697102042.png" alt="img"></p>
<p>（引出LSTM的背景知识）</p>
<p>RNN可以看成一个在时间上传递的神经网络，它的深度是时间的长度！正如我们上面所说，“梯度消失”现象又要出现了，只不过这次发生在时间轴上。对于t时刻来说，它产生的梯度在时间轴上向历史传播几层之后就消失了，根本就无法影响太遥远的过去。因此，之前说“所有历史”共同作用只是理想的情况，在实际中，这种影响也就只能维持若干个时间戳。<strong>为了解决时间上的梯度消失，机器学习领域发展出了长短时记忆单元。</strong>LSTM</p>
<h5 id="2-为什么LSTM记的时间长"><a href="#2-为什么LSTM记的时间长" class="headerlink" title="2.为什么LSTM记的时间长"></a>2.为什么LSTM记的时间长</h5><p>因为在rnn原始隐藏层的一个h状态上增加增加了一个状态c（单元状态），让它来保存长期的状态，并通过三个门控制长期c。这种结构使得误差向上一个状态传递时几乎没有衰减，所以权值调整的时候，对于很长时间之前的状态带来的影响和结尾状态带来的影响可以同时发挥作用，最后训练出来的模型就具有较长时间范围内的记忆功能。</p>
<p>具体解释：</p>
<p>其实，<strong>长短时记忆网络</strong>的思路比较简单。原始RNN的隐藏层只有一个状态，即h，它对于短期的输入非常敏感。那么，假如我们再增加一个状态，即c，让它来保存长期的状态，那么问题不就解决了么？如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20170904215807453" alt="img"></p>
<p>新增加的状态c，称为<strong>单元状态(cell state)</strong>。我们把上图按照时间维度展开：</p>
<p><img src="https://img-blog.csdn.net/20170904215819060" alt="img"></p>
<p><img src="https://img-blog.csdn.net/20170904215826539" alt="img"></p>
<p>LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。三个开关的作用如下图所示：</p>
<p><img src="https://img-blog.csdn.net/20170904215838680?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY2hlbmZlbmdnYW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>
<p>接下来，我们要描述一下，输出h和单元状态c的具体计算方法。</p>
<p>前面描述的开关是怎样在算法中实现的呢？这就用到了门（gate）的概念。门实际上就是一层全连接层，它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，是偏置项，那么门可以表示为：<img src="https://img-blog.csdn.net/20170904215849396" alt="img"></p>
<p><img src="https://ws1.sinaimg.cn/large/005BVyzmly1fotnxtqjlmj305i06q3yd.jpg" alt="img"></p>
<p>门 可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层 和一个逐点相乘的操作来实现的。sigmoid 层输出（是一个向量）的每个元素都是一个在 0 和 1 之间的实数，表示让对应信息通过的权重（或者占比）。比如， 0 表示“不让任何信息通过”， 1 表示“让所有信息通过”。</p>
<p>LSTM通过三个这样的本结构来实现信息的保护和控制。这三个门分别输入门、遗忘门和输出门。</p>
<p><img src="https://img-blog.csdn.net/20170904215859449" alt="img"></p>
<p><img src="https://img-blog.csdn.net/20170904215908067" alt="img"></p>
<p><strong>遗忘门：</strong></p>
<p><strong>最左边的遗忘门，遗忘们决定有多少重Ct-1 传播到Ct。</strong></p>
<p><img src="https://img-blog.csdn.net/20170904215918744" alt="img"></p>
<p>在我们 LSTM 中的第一步是决定我们会从细胞状态中丢弃什么信息。这个决定通过一个称为忘记门层完成。该门会读取ht−1 和xt ,输出一个在 0到 1之间的数值给每个在细胞状态ct-1中的数字。1 表示“完全保留”，0 表示“完全舍弃”。其中ht−1表示的是上一个cell的输出，xt 表示的是当前细胞的输入。σ 表示sigmod函数。</p>
<p>让我们回到语言模型的例子中来基于已经看到的预测下一个词。在这个问题中，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语</p>
<p><strong>输入门</strong>：</p>
<p><strong>输入门决定当前输入有多少保存到Ct</strong></p>
<p><img src="https://img-blog.csdn.net/20170904215926971" alt="img"></p>
<p>下一步是决定让多少新的信息加入到 cell 状态 中来。实现这个需要包括两个 步骤：首先，一个叫做“input gate layer ”的 sigmoid 层决定哪些信息需要更新；一个 tanh 层生成一个向量，也就是备选的用来更新的内容，即Ct(~)</p>
<p>在下一步，我们把这两部分联合起来，对 cell 的状态进行一个更新。</p>
<p><strong>新的保存信息</strong>： </p>
<p>所以最终的保存信息为：符号*表示<strong>按元素乘</strong><img src="https://img-blog.csdn.net/20170904215934392" alt="img"></p>
<p>现在是更新旧细胞状态的时间了，Ct−1 更新为Ct 。前面的步骤已经决定了将会做什么，我们现在就是实际去完成。我们把旧状态与ft相乘，丢弃掉我们确定需要丢弃的信息。接着加上it∗C˜t 。这就是新的候选值，根据我们决定更新每个状态的程度进行变化。在语言模型的例子中，这就是我们实际根据前面确定的目标，丢弃旧代词的性别信息并添加新的信息的地方。</p>
<p><strong>输出</strong>：</p>
<p><img src="https://img-blog.csdn.net/20170904215943994" alt="img"></p>
<p>最终，我们需要确定输出什么值。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本。首先，我们运行一个 sigmoid 层来确定细胞状态的哪个部分将输出出去。接着，我们把细胞状态通过 tanh 进行处理（得到一个在 -1 到 1 之间的值）并将它和 sigmoid 门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<p>在语言模型的例子中，因为他就看到了一个 代词，可能需要输出与一个 动词 相关的信息。例如，可能输出是否代词是单数还是负数，这样如果是动词的话，我们也知道动词需要进行的词形变化。</p>
<h5 id="3-RNN特点"><a href="#3-RNN特点" class="headerlink" title="3.RNN特点"></a>3.RNN特点</h5><p>RNN（Recurrent Neural Network）<u>是一类<strong>用于处理序列数据的神经网络</strong>【1】</u>。首先我们要明确什么是序列数据，摘取百度百科词条：时间序列数据是指在不同时间点上收集到的数据，这类数据反映了某一事物、现象等随时间的变化状态或程度。这是时间序列数据的定义，当然这里也可以不是时间，比如文字序列，<strong>但总归序列数据有一个特点——后面的数据跟前面的数据有关系。</strong></p>
<p>我们从基础的神经网络中知道，神经网络包含输入层、隐层、输出层，通过激活函数控制输出，层与层之间通过权值连接。激活函数是事先确定好的，那么神经网络模型通过训练“学“到的东西就蕴含在“权值“中。 基础的神经网络只在层与层之间建立了权连接，<u><strong>RNN最大的不同之处就是在层之间的神经元之间也建立的权连接。【2】</strong></u>只要知道上一时刻的隐藏状态ht−1与当前时刻的输入xt，就可以计算当前时刻的隐藏状态ht。</p>
<p><u><strong>权值共享【3】</strong></u>在RNN中U、V、W的参数都是共享的，也就是只需要关注每一步都在做相同的事情，只是输入不同，这样来降低参数个数和计算量。</p>
<p><img src="https://img-blog.csdn.net/20170724164552739?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveHdkMTgyODA4MjAwNTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="img"></p>
<p><strong><u>LSTM在RNN基础上解决了其时间序列上梯度消失的问题【4】</u></strong></p>
<h4 id="21-HMM"><a href="#21-HMM" class="headerlink" title="21.HMM"></a>21.HMM</h4><h4 id="22-CRF"><a href="#22-CRF" class="headerlink" title="22.CRF"></a>22.CRF</h4>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Internship/" rel="tag"># Internship</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/09/Leetcode-4-dp/" rel="next" title="Leetcode-4-dp">
                <i class="fa fa-chevron-left"></i> Leetcode-4-dp
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/29519546.png" alt="Junru Lu">
            
              <p class="site-author-name" itemprop="name">Junru Lu</p>
              <p class="site-description motion-element" itemprop="description">Keep moving</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/LuJunru" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:lj1230@nyu.edu" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.linkedin.com/in/%E4%BF%8A%E5%A6%82-%E9%99%86-29333916b/?locale=en_US" target="_blank" title="Linkin">
                      
                        <i class="fa fa-fw fa-linkedin"></i>Linkin</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="http://cusp.nyu.edu/profiles/junru-lu/" target="_blank" title="NYU">
                      
                        <i class="fa fa-fw fa-university"></i>NYU</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-L1-、L2损失函数"><span class="nav-number">1.</span> <span class="nav-text">1.L1 、L2损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#常用损失函数"><span class="nav-number">1.1.</span> <span class="nav-text">常用损失函数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-L1、L2正则"><span class="nav-number">2.</span> <span class="nav-text">2.L1、L2正则</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#L1、L2具体过程："><span class="nav-number">2.0.1.</span> <span class="nav-text">L1、L2具体过程：</span></a></li></ol></li></ol><li class="nav-item nav-level-4"><a class="nav-link" href="#3-逻辑回归"><span class="nav-number">3.</span> <span class="nav-text">3.逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#逻辑回归原理"><span class="nav-number">3.1.</span> <span class="nav-text">逻辑回归原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#逻辑回归和SVM区别"><span class="nav-number">3.2.</span> <span class="nav-text">逻辑回归和SVM区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#逻辑回归与最大熵"><span class="nav-number">3.3.</span> <span class="nav-text">逻辑回归与最大熵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#逻辑回归与线性回归"><span class="nav-number">3.4.</span> <span class="nav-text">逻辑回归与线性回归</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-GDBT和RF区别-GDBT和Xgboost区别"><span class="nav-number">4.</span> <span class="nav-text">4.GDBT和RF区别   GDBT和Xgboost区别</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#随机森林RF和GBDT的区别"><span class="nav-number">4.1.</span> <span class="nav-text">随机森林RF和GBDT的区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Xgboost和GBDT的区别"><span class="nav-number">4.2.</span> <span class="nav-text">Xgboost和GBDT的区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#随机森林RF"><span class="nav-number">4.3.</span> <span class="nav-text">随机森林RF</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GBDT"><span class="nav-number">4.4.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Adaboost"><span class="nav-number">4.5.</span> <span class="nav-text">Adaboost</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Xgboost"><span class="nav-number">4.6.</span> <span class="nav-text">Xgboost</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Stacking"><span class="nav-number">4.7.</span> <span class="nav-text">Stacking</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-CNN、DNN"><span class="nav-number">5.</span> <span class="nav-text">5.CNN、DNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#卷积运算"><span class="nav-number">5.1.</span> <span class="nav-text">卷积运算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#softmax函数"><span class="nav-number">5.2.</span> <span class="nav-text">softmax函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#常用的激活函数及其作用："><span class="nav-number">5.3.</span> <span class="nav-text">常用的激活函数及其作用：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#神经网络："><span class="nav-number">5.4.</span> <span class="nav-text">神经网络：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#对卡在局部极小值的处理方法："><span class="nav-number">5.5.</span> <span class="nav-text">对卡在局部极小值的处理方法：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#浅层VS深层："><span class="nav-number">5.6.</span> <span class="nav-text">浅层VS深层：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#防止过拟合："><span class="nav-number">5.7.</span> <span class="nav-text">防止过拟合：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CNN卷积神经网络的层级结构"><span class="nav-number">5.8.</span> <span class="nav-text">CNN卷积神经网络的层级结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CNN卷积神经网络的优点"><span class="nav-number">5.9.</span> <span class="nav-text">CNN卷积神经网络的优点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CNN与DNN的区别"><span class="nav-number">5.10.</span> <span class="nav-text">CNN与DNN的区别</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-LDA主题模型"><span class="nav-number">6.</span> <span class="nav-text">6.LDA主题模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-Word2Vec-17-语言模型-glove词向量"><span class="nav-number">7.</span> <span class="nav-text">7.Word2Vec   17.语言模型 || glove词向量</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-原始CBOW"><span class="nav-number">7.1.</span> <span class="nav-text">1.原始CBOW</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-改进CBOW"><span class="nav-number">7.2.</span> <span class="nav-text">2.改进CBOW</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-改进Skip-Gram"><span class="nav-number">7.3.</span> <span class="nav-text">3.改进Skip-Gram</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-glove模型"><span class="nav-number">7.4.</span> <span class="nav-text">4.glove模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-WordEmbedding"><span class="nav-number">8.</span> <span class="nav-text">8.WordEmbedding</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#基于频率的Word-Embedding"><span class="nav-number">8.1.</span> <span class="nav-text">基于频率的Word Embedding</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-1TF-IDF"><span class="nav-number">8.2.</span> <span class="nav-text">8.1TF-IDF</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-2共现矩阵-lt-gt-7-4-glove模型"><span class="nav-number">8.3.</span> <span class="nav-text">8.2共现矩阵  &lt;&gt; 7.4 glove模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#基于预测的Word-Embedding"><span class="nav-number">8.4.</span> <span class="nav-text">基于预测的Word Embedding</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-tf-idf"><span class="nav-number">9.</span> <span class="nav-text">9.tf-idf</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#tf"><span class="nav-number">9.1.</span> <span class="nav-text">tf:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Idf"><span class="nav-number">9.2.</span> <span class="nav-text">Idf:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#优点："><span class="nav-number">9.3.</span> <span class="nav-text">优点：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#缺点："><span class="nav-number">9.4.</span> <span class="nav-text">缺点：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-SVM"><span class="nav-number">10.</span> <span class="nav-text">10.SVM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#SVM原理"><span class="nav-number">10.1.</span> <span class="nav-text">SVM原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#逻辑回归和SVM区别-1"><span class="nav-number">10.2.</span> <span class="nav-text">逻辑回归和SVM区别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SVM的参数"><span class="nav-number">10.3.</span> <span class="nav-text">SVM的参数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-LexRank-、TextRank"><span class="nav-number">11.</span> <span class="nav-text">11.LexRank 、TextRank</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#PageRank"><span class="nav-number">11.1.</span> <span class="nav-text">PageRank:</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TextRank"><span class="nav-number">11.2.</span> <span class="nav-text">TextRank:</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-VSM向量空间模型"><span class="nav-number">12.</span> <span class="nav-text">12.VSM向量空间模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-关联规则"><span class="nav-number">13.</span> <span class="nav-text">14.关联规则</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#算法原理和过程"><span class="nav-number">13.1.</span> <span class="nav-text">算法原理和过程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-决策树"><span class="nav-number">14.</span> <span class="nav-text">15.决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-K-Means"><span class="nav-number">15.</span> <span class="nav-text">16.K-Means</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-语言模型-lt-gt-7-Word2Vec-CBOW-lt-gt-N-gram-lt-gt-朴素贝叶斯"><span class="nav-number">16.</span> <span class="nav-text">17.语言模型 &lt;&gt; 7.Word2Vec(CBOW) &lt;&gt; N-gram &lt;&gt; 朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-朴素贝叶斯"><span class="nav-number">16.1.</span> <span class="nav-text">1.朴素贝叶斯</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-n-gram"><span class="nav-number">16.2.</span> <span class="nav-text">2.n-gram</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-神经网络语言模型—CBOW是在此基础上进行改进的"><span class="nav-number">16.3.</span> <span class="nav-text">3.神经网络语言模型—CBOW是在此基础上进行改进的</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-CBOW-Word2Vec"><span class="nav-number">16.4.</span> <span class="nav-text">4.CBOW (Word2Vec)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-马尔可夫链"><span class="nav-number">17.</span> <span class="nav-text">18.马尔可夫链</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#19-向量空间模型"><span class="nav-number">18.</span> <span class="nav-text">19.向量空间模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#20-RNN"><span class="nav-number">19.</span> <span class="nav-text">20.RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-为什么具有记忆功能？"><span class="nav-number">19.1.</span> <span class="nav-text">1.为什么具有记忆功能？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-为什么LSTM记的时间长"><span class="nav-number">19.2.</span> <span class="nav-text">2.为什么LSTM记的时间长</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-RNN特点"><span class="nav-number">19.3.</span> <span class="nav-text">3.RNN特点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#21-HMM"><span class="nav-number">20.</span> <span class="nav-text">21.HMM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#22-CRF"><span class="nav-number">21.</span> <span class="nav-text">22.CRF</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Junru Lu</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'TESN0aruYWfEKaMV36cdA8qo-gzGzoHsz',
        appKey: '73zKQnJQHrfIPjWV2puu1YS3',
        placeholder: '(￣∇￣)来啊，快活啊!',
        avatar:'mm',
        guest_info:guest,
        pageSize:'10' || 10,
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("TESN0aruYWfEKaMV36cdA8qo-gzGzoHsz", "73zKQnJQHrfIPjWV2puu1YS3");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

</body>
</html>
